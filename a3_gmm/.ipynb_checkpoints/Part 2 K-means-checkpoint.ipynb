{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ruocx\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import helper as hlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "#data = np.load('data2D.npy')\n",
    "data = np.load('data100D.npy')\n",
    "[num_pts, dim] = np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid = True\n",
    "\n",
    "# Splits the dataset into training and validation set:\n",
    "if is_valid:\n",
    "  valid_batch = int(num_pts / 3.0)\n",
    "  np.random.seed(45689)\n",
    "  rnd_idx = np.arange(num_pts)\n",
    "  np.random.shuffle(rnd_idx)\n",
    "  val_data = data[rnd_idx[:valid_batch]]\n",
    "  data = data[rnd_idx[valid_batch:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.convert_to_tensor(data, dtype=tf.float64)\n",
    "MU = tf.Variable(tf.random_normal(np.array([K, dim]), dtype=X.dtype))\n",
    "X = tf.expand_dims(X, 0)\n",
    "MU = tf.expand_dims(MU, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance function for K-means\n",
    "def distanceFunc(X, MU):\n",
    "    # Inputs\n",
    "    # X: is an NxD matrix (N observations and D dimensions)\n",
    "    # MU: is an KxD matrix (K means and D dimensions)\n",
    "    # Outputs\n",
    "    # pair_dist: is the squared pairwise distance matrix (NxK)\n",
    "    # TODO\n",
    "    \n",
    "    pair_dist = tf.transpose(tf.reduce_sum(tf.square(X - MU), axis=2))\n",
    "    return pair_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(X, MU):\n",
    "    dist = distanceFunc(X, MU) #get distances from data points to cluster means\n",
    "    error = tf.reduce_min(dist, axis=1) #get smallest cluster-point distances\n",
    "    loss = tf.reduce_sum(error) #calculate error\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_assignments(X, MU):\n",
    "    dist = distanceFunc(X, MU)\n",
    "    cluster = tf.argmin(dist, axis=1)\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_function(X, MU)\n",
    "dist = distanceFunc(X, MU)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= 0.1, beta1=0.9, beta2=0.99, epsilon=1e-5).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Total training loss:  1109862.926897976\n",
      "Average training loss: 166.47111547892246\n",
      "Average validation loss: 149.48062177023618\n",
      "Epoch:  2\n",
      "Total training loss:  1002838.7003079114\n",
      "Average training loss: 150.4182841319801\n",
      "Average validation loss: 134.5284908645837\n",
      "Epoch:  3\n",
      "Total training loss:  902771.3925369619\n",
      "Average training loss: 135.4089384336226\n",
      "Average validation loss: 120.88537266365053\n",
      "Epoch:  4\n",
      "Total training loss:  811387.3407120749\n",
      "Average training loss: 121.70201600601094\n",
      "Average validation loss: 108.73580472005726\n",
      "Epoch:  5\n",
      "Total training loss:  730075.2186835317\n",
      "Average training loss: 109.50580751215415\n",
      "Average validation loss: 98.03405356031084\n",
      "Epoch:  6\n",
      "Total training loss:  658405.7272627228\n",
      "Average training loss: 98.75592129334375\n",
      "Average validation loss: 88.64640733970566\n",
      "Epoch:  7\n",
      "Total training loss:  595499.9168534249\n",
      "Average training loss: 89.32052150193863\n",
      "Average validation loss: 80.3812060369705\n",
      "Epoch:  8\n",
      "Total training loss:  540113.7890096959\n",
      "Average training loss: 81.01301770056935\n",
      "Average validation loss: 72.95574531110726\n",
      "Epoch:  9\n",
      "Total training loss:  490244.8272449294\n",
      "Average training loss: 73.53304743436769\n",
      "Average validation loss: 66.18093149679093\n",
      "Epoch:  10\n",
      "Total training loss:  444680.51988533663\n",
      "Average training loss: 66.69874304564821\n",
      "Average validation loss: 60.11376588259069\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as training_loop:\n",
    "  tf.initializers.global_variables().run()\n",
    "\n",
    "  training_loss = []\n",
    "  validation_loss = []\n",
    "  for epoch in range(150):\n",
    "    new_MU, train_loss, new_dist, _ = training_loop.run([MU, loss, dist, optimizer])\n",
    "\n",
    "    if is_valid:\n",
    "      v_loss = loss_function(val_data,new_MU)\n",
    "      v_dist = distanceFunc(val_data, new_MU)\n",
    "      with tf.Session() as get_loss:\n",
    "        tf.initializers.global_variables().run()\n",
    "        valid_loss, valid_dist = get_loss.run([v_loss,v_dist])\n",
    "        validation_loss.append(valid_loss/len(val_data))\n",
    "      \n",
    "    training_loss.append(train_loss/len(data))\n",
    "\n",
    "    print(\"Epoch: \", epoch+1)\n",
    "    print(\"Total training loss: \", train_loss)\n",
    "    print(\"Average training loss:\", train_loss / len(data))\n",
    "    print(\"Average validation loss:\", valid_loss / len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loss Plot:\n",
    "\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.xlim(0, len(training_loss))\n",
    "plt.ylabel('Average Loss')\n",
    "plt.ylim(0, max(training_loss) + 1)\n",
    "plt.title('Training Data Loss of K-means Clustering w/ ' + str(K) + ' Cluster Center(s) for dim = ' + str(dim))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the all the distributions:\n",
    "pred = np.argmin(new_dist, axis = 1) # The predictions of the model.\n",
    "\n",
    "combined_data = np.concatenate((data, pred.reshape((len(pred),1))), axis =1)\n",
    "\n",
    "# Getting accuracies of the 3 clusters:\n",
    "cluster_distrib_percentage = []\n",
    "final_data = []\n",
    "for center in range(K):\n",
    "  distrib_percentage = (pred==center).sum() / data.shape[0]\n",
    "  cluster_distrib_percentage.append(distrib_percentage)\n",
    "  d = combined_data[combined_data[:,2] == center]\n",
    "  final_data.append(d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(K):\n",
    "  print(\"Percentage of data in Cluster \" + str(i + 1) + \": \", cluster_distrib_percentage[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Loss Plot:\n",
    "\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.xlim(0, len(training_loss))\n",
    "plt.ylabel('Average Loss')\n",
    "plt.ylim(0, max(training_loss) + 1)\n",
    "plt.title('Training and Validation Loss of K-means Clustering w/ '+str(K)+' Cluster Center(s) for dim = ' + str(dim))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Validation Error of the model\n",
    "\n",
    "v_loss = loss_function(val_data,new_MU)\n",
    "with tf.Session() as get_loss:\n",
    "  tf.initializers.global_variables().run()\n",
    "  valid_loss = get_loss.run(v_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss #Total Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss/len(val_data) #Avg Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the all the distributions:\n",
    "v_pred = np.argmin(valid_dist, axis = 1) # The predictions of the model.\n",
    "\n",
    "v_combined_data = np.concatenate((val_data, v_pred.reshape((len(v_pred),1))), axis =1)\n",
    "\n",
    "# Getting accuracies of the 3 clusters:\n",
    "v_cluster_distrib_percentage = []\n",
    "v_final_data = []\n",
    "for center in range(K):\n",
    "  v_distrib_percentage = (v_pred==center).sum() / val_data.shape[0]\n",
    "  v_cluster_distrib_percentage.append(v_distrib_percentage)\n",
    "  v_d = v_combined_data[v_combined_data[:,2] == center]\n",
    "  v_final_data.append(v_d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(K):\n",
    "  print(\"Percentage of vdata in Cluster \" + str(i + 1) + \": \", v_cluster_distrib_percentage[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

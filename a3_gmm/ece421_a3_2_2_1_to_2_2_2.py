# -*- coding: utf-8 -*-
"""ECE421 A3 2.2.1-2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Avg1QPBx_vVdKdnHW1DfHpUgSkhFEq13
"""

from warnings import simplefilter
simplefilter(action='ignore', category=FutureWarning)

# Importing modules:

import tensorflow.compat.v1 as tf 
tf.disable_v2_behavior()
import numpy as np
import matplotlib.pyplot as plt
import helper as hlp

# Loading data from data2D.npy:

data = np.load('data2D.npy')
[num_pts, dim] = np.shape(data)

# Splits the dataset into training and validation set:

is_valid =  True
if is_valid:
  valid_batch = int(num_pts / 3.0)
  np.random.seed(45689)
  rnd_idx = np.arange(num_pts)
  np.random.shuffle(rnd_idx)
  val_data = data[rnd_idx[:valid_batch]]
  data = data[rnd_idx[valid_batch:]]

# Setting the # of clusters:

K = 5

# Convert the data and cluster centers to tensors:

t_data = tf.convert_to_tensor(data, dtype=tf.float64)
X = tf.expand_dims(t_data, 0)
MU = tf.Variable(tf.random_normal(np.array([K, dim]), stddev= 0.05, dtype=X.dtype))
MU = tf.expand_dims(MU, 1)

# Distance function for GMM:

def distanceFunc(X, MU):
    # Inputs
    # X: is an NxD matrix (N observations and D dimensions)
    # MU: is an KxD matrix (K means and D dimensions)
    # Outputs
    # pair_dist: is the squared pairwise distance matrix (NxK)
    # TODO
    pair_dist = tf.transpose(tf.reduce_sum(tf.square(X - MU), axis=2))
    return pair_dist

# Gets the Gaussian PDF:

def log_GaussPDF(X, mu, sigma):
    # Inputs
    # X: N X D
    # mu: K X D
    # sigma: K X 1

    # Outputs:
    # log Gaussian PDF N X K

    # TODO
    dim = tf.cast(data.shape[1], "float64")
    dist = distanceFunc(X, mu)
    sigma = tf.transpose(sigma)
    gauss_coeff = tf.log(2 * np.pi * sigma)
    pdf = -(1/2)*dim*gauss_coeff - (dist/(2*sigma))
    return pdf

# Gets the log of the posterior values:

def log_posterior(log_PDF, log_pi):
    # Input
    # log_PDF: log Gaussian PDF N X K
    # log_pi: K X 1

    # Outputs
    # log_post: N X K

    # TODO
    log_pi = tf.transpose(log_pi)
    prob = log_pi + log_PDF
    prob_sum = hlp.reduce_logsumexp(prob, keep_dims=True)
    posterior = prob - prob_sum
    return posterior

# Loss function of the GMM:

def loss_function(log_PDF, log_pi):
    loss = -tf.reduce_sum(hlp.reduce_logsumexp(log_PDF + log_pi, 1, keep_dims=True), axis =0)
    return loss

phi = tf.Variable(tf.random_normal(np.array([K, 1]), stddev= 0.05, dtype=X.dtype)) # The phi value.
sigma = tf.exp(phi) # The variance of the model.
psi = tf.Variable(tf.random_normal(np.array([K, 1]), stddev = 0.05, dtype=X.dtype)) # The psi value
logpi = tf.squeeze(hlp.logsoftmax(psi)) # The log of the pi parameter.

logpdf = log_GaussPDF(X, MU, sigma) 
prediction = tf.argmax(tf.nn.softmax(log_posterior(logpdf, logpi)),1)
loss = loss_function(logpdf, logpi)

optimizer = tf.train.AdamOptimizer(learning_rate=0.1, beta1=0.9, beta2=0.99, epsilon=1e-5).minimize(loss) # Adam optimizing function.

with tf.Session() as training_loop:
  tf.initializers.global_variables().run()

  prev_loss = float('inf')
  training_loss = [] # Stores the average training losses.
  validation_loss = [] # Stores the average validation losses.
  len_data = t_data.get_shape().as_list()[0]
  for epoch in range(250):
    new_MU, train_loss, new_pred, new_phi, new_psi, _ = training_loop.run([MU, loss, prediction, phi, psi, optimizer]) # Updates hyperparameters.

    if is_valid:
      v_sigma = tf.exp(new_phi) # Updated validation variance.
      v_logpi = tf.squeeze(hlp.logsoftmax(new_psi)) # Updated log of pi parameter for validation.
      v_logpdf = log_GaussPDF(val_data, new_MU, v_sigma) # Updated Gaussian PDF.
      v_loss = loss_function(v_logpdf, v_logpi) # Validation loss.
      v_prediction = tf.argmax(tf.nn.softmax(log_posterior(v_logpdf, v_logpi)),1)
      valid_loss, v_pred = training_loop.run([v_loss, v_prediction]) 
      validation_loss.append(valid_loss[0] / len(val_data))

    training_loss.append(train_loss[0] / len_data)
				
    print("Epoch: ", epoch + 1)
    print("Total training loss: ", train_loss[0])
    print("Average training loss:", train_loss[0] / len_data)
    print("Average validation loss:", valid_loss[0] / len(val_data))

# Training Loss Plot:

plt.plot(training_loss, label='Training Loss')
plt.xlabel('Number of Epochs')
plt.xlim(0, len(training_loss))
plt.ylabel('Average Loss')
plt.ylim(0, max(training_loss) + 1)
plt.title('Training Loss of GMM Clustering w/ '+str(K)+' Cluster Center(s)')
plt.legend()
plt.show()

# Combining the data with their corresponding predictions:
combined_data = np.concatenate((data, new_pred.reshape((len(new_pred),1))), axis =1)

# Getting distributions of the K clusters:
cluster_distrib_percentage = []
final_data = []
for center in range(K):
  distrib_percentage = (new_pred==center).sum() / data.shape[0] # Getting the percentage of each cluster.
  cluster_distrib_percentage.append(distrib_percentage)
  d = combined_data[combined_data[:,2] == center] # Getting the unique data for each of the clusters.
  final_data.append(d)

for i in range(K):
  print("Percentage of data in Cluster " + str(i + 1) + ": ", cluster_distrib_percentage[i])

# Scatter Plot of Clustering Distribution for training data:

for i in range(K):  
  plt.scatter(final_data[i][:,0], final_data[i][:,1], label = 'Cluster ' + str(i+1))
plt.plot(new_MU[:,0,0], new_MU[:,0,1], 'kx', markersize=15)
plt.xlabel('x coord data')
plt.ylabel('y coord data')
plt.title('GMM Clustering Training Prediction Distributions w/ '+str(K)+' Cluster Center(s)')
plt.legend()
plt.show()

# Training and Validation Loss Plot:

plt.plot(training_loss, label='Training Loss')
plt.plot(validation_loss, label='Validation Loss')
plt.xlabel('Number of Epochs')
plt.xlim(0, len(training_loss))
plt.ylabel('Average Loss')
plt.ylim(0, max(training_loss) + 1)
plt.title('Training and Validation Loss of GMM Clustering w/ '+str(K)+' Cluster Center(s)')
plt.legend()
plt.show()

with tf.Session() as get_loss:
  tf.initializers.global_variables().run()
  valid_loss = get_loss.run(v_loss)

valid_loss[0] # Total Validation Loss

valid_loss[0]/len(val_data) # Avg Validation Loss

v_combined_data = np.concatenate((val_data, v_pred.reshape((len(v_pred),1))), axis =1)

# Getting validation accuracies of the K clusters:
v_cluster_distrib_percentage = []
v_final_data = []
for center in range(K):
  v_distrib_percentage = (v_pred==center).sum() / val_data.shape[0] # Getting the percentage of each cluster.
  v_cluster_distrib_percentage.append(v_distrib_percentage)
  v_d = v_combined_data[v_combined_data[:,2] == center] # Getting the unique data for each of the clusters.
  v_final_data.append(v_d)

for i in range(K):
  print("Percentage of vdata in Cluster " + str(i + 1) + ": ", v_cluster_distrib_percentage[i])

# Scatter Plot of Clustering Distribution for validation data:

for i in range(K):  
  plt.scatter(v_final_data[i][:,0], v_final_data[i][:,1], label = 'Cluster ' + str(i+1))
plt.plot(new_MU[:,0,0], new_MU[:,0,1], 'kx', markersize=15)
plt.xlabel('x coord data')
plt.ylabel('y coord data')
plt.title('GMM Clustering Validation Prediction Distributions w/ '+str(K)+' Cluster Center(s)')
plt.legend()
plt.show()


{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE421 Assignment 1 Comparison",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTHjYIbDoKxa",
        "colab_type": "code",
        "outputId": "7ae1eced-e7bf-4515-8f61-f33896614ce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJdvxZ4eoR8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadData():\n",
        "    with np.load('notMNIST.npz') as data :\n",
        "        Data, Target = data ['images'], data['labels']\n",
        "        posClass = 2\n",
        "        negClass = 9\n",
        "        dataIndx = (Target==posClass) + (Target==negClass)\n",
        "        Data = Data[dataIndx]/255.\n",
        "        Target = Target[dataIndx].reshape(-1, 1)\n",
        "        Target[Target==posClass] = 1\n",
        "        Target[Target==negClass] = 0\n",
        "        np.random.seed(421)\n",
        "        randIndx = np.arange(len(Data))\n",
        "        np.random.shuffle(randIndx)\n",
        "        Data, Target = Data[randIndx], Target[randIndx]\n",
        "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
        "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
        "        testData, testTarget = Data[3600:], Target[3600:]\n",
        "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn4_De4RoXag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
        "trainData = trainData.reshape(3500,784)\n",
        "validData = validData.reshape(100,784)\n",
        "testData = testData.reshape(145,784)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn2yy45rokrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossEntropyLoss(W, b, x, y, reg):\n",
        "    # Your implementation here\n",
        "    x = np.insert(x, 0, 1, axis=1)\n",
        "    W_b = np.insert(W, 0, b)\n",
        "    y = y.reshape(y.shape[0])\n",
        "    z = np.dot(x,W_b)\n",
        "    sigmoid = 1/(1 + np.exp(-z))\n",
        "    CELoss = np.linalg.norm(np.dot(-y, np.log(sigmoid)) - np.dot(1 - y, np.log(1-sigmoid)))/len(x) + (reg/2)*np.linalg.norm(W)**2\n",
        "    return CELoss\n",
        "\n",
        "def gradCE(W, b, x, y, reg):\n",
        "    # Your implementation here\n",
        "    x = np.insert(x, 0, 1, axis=1)\n",
        "    W_b = np.insert(W, 0, b)\n",
        "    y = y.reshape(y.shape[0])\n",
        "    z = np.dot(x,W_b)\n",
        "    sigmoid = 1 / (1 + np.exp(-z))\n",
        "    error = sigmoid - y\n",
        "    dL_dwb = np.dot(np.transpose(x),error)/len(x) \n",
        "    return dL_dwb[1:], dL_dwb[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dddLNWdmoo3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(W, b, x, y):\n",
        "   x = np.insert(x, 0, 1, axis=1)\n",
        "   W_b = np.insert(W, 0, b)\n",
        "   y = y.reshape(y.shape[0])\n",
        "   pred = np.dot(x,W_b)\n",
        "   pred = np.where(pred >= 0.5, 1, 0)\n",
        "   accuracy = np.sum(pred == y)/len(pred)\n",
        "   return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GugNGAvoucd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_descent(W, b, x, y, v_x, v_y, test_x, test_y, alpha, epochs, reg, error_tol, lossType = 'MSE'):\n",
        "    # Your implementation here\n",
        "    training_loss = []\n",
        "    training_accuracy = []\n",
        "    validation_loss = []\n",
        "    validation_accuracy = []\n",
        "    test_loss = []\n",
        "    test_accuracy = []\n",
        "    for i in range(epochs):\n",
        "        if lossType == 'MSE':\n",
        "            dl_dw, dl_db = gradMSE(W, b, x, y, reg) # The gradient based on loss for each image\n",
        "        elif lossType == 'CE':\n",
        "            dl_dw, dl_db = gradCE(W, b, x, y, reg) # The gradient based on loss for each image\n",
        "        W_new = W - alpha*(dl_dw) # Updates weights\n",
        "        b -= alpha*(dl_db) # Updates bias\n",
        "        \n",
        "        if (np.linalg.norm(W_new - W) < error_tol):\n",
        "            return [W, b, training_loss, validation_loss, test_loss, training_accuracy, validation_accuracy, test_accuracy]\n",
        "        \n",
        "        W = W_new\n",
        "        # Training Loss:\n",
        "        t_accuracy = accuracy(W,b,x,y)\n",
        "        if lossType == 'MSE':\n",
        "            t_loss = MSE(W, b, x, y, reg)\n",
        "        elif lossType == 'CE':\n",
        "            t_loss = crossEntropyLoss(W, b, x, y, reg)\n",
        "        print(\"Epoch: %d, Training Loss: %0.2f, Training Accuracy: %0.2f\" % (i, t_loss, t_accuracy))\n",
        "        training_loss += [t_loss]\n",
        "        training_accuracy += [t_accuracy]\n",
        "\n",
        "        # Validation Loss:\n",
        "        v_accuracy = accuracy(W,b,v_x,v_y)\n",
        "        if lossType == 'MSE':\n",
        "            v_loss = MSE(W, b, x, y, reg)\n",
        "        elif lossType == 'CE':\n",
        "            v_loss = crossEntropyLoss(W, b, x, y, reg)\n",
        "        print(\"Epoch: %d, Validation Loss: %0.2f, Validation Accuracy: %0.2f\" % (i, v_loss, v_accuracy))\n",
        "        validation_loss += [v_loss]\n",
        "        validation_accuracy += [v_accuracy]\n",
        "\n",
        "        # Testing Loss:\n",
        "        test_acc = accuracy(W,b,test_x,test_y)\n",
        "        if lossType == 'MSE':\n",
        "            te_loss = MSE(W, b, x, y, reg)\n",
        "        elif lossType == 'CE':\n",
        "            te_loss = crossEntropyLoss(W, b, x, y, reg)\n",
        "        print(\"Epoch: %d, Testing Loss: %0.2f, Testing Accuracy: %0.2f\" % (i, te_loss, test_acc))\n",
        "        test_loss += [te_loss]\n",
        "        test_accuracy += [test_acc]\n",
        "      \n",
        "    return [W, b, training_loss, validation_loss, test_loss, training_accuracy, validation_accuracy, test_accuracy]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4f8cxX1owmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_i = np.random.normal(0,0,28*28)\n",
        "b_i = 0\n",
        "l_r = [0.001]\n",
        "epochs = 700\n",
        "reg = 0.5\n",
        "error_tol = 10**-7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMKyF52Bqym3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = np.empty([len(l_r), 28*28])\n",
        "bias = np.empty([len(l_r), 1])\n",
        "training_error = np.empty([len(l_r), epochs])\n",
        "training_accuracy = np.empty([len(l_r), epochs])\n",
        "validation_error = np.empty([len(l_r), epochs])\n",
        "validation_accuracy = np.empty([len(l_r), epochs])\n",
        "testing_error = np.empty([len(l_r), epochs])\n",
        "testing_accuracy = np.empty([len(l_r), epochs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDJglAgZqy9G",
        "colab_type": "code",
        "outputId": "18ae351e-25ac-44fc-a2d0-af449c8feec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "W = W_i\n",
        "b = b_i\n",
        "r = 0\n",
        "[weight[r], bias[r], training_error[r], validation_error[r], testing_error[r], training_accuracy[r], validation_accuracy[r], testing_accuracy[r]] = grad_descent(W, b, trainData, trainTarget, validData, validTarget, testData, testTarget, 0.001, epochs, reg, error_tol,lossType = 'CE')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Training Loss: 87.31, Training Accuracy: 0.50\n",
            "Epoch: 0, Validation Loss: 87.31, Validation Accuracy: 0.49\n",
            "Epoch: 0, Testing Loss: 87.31, Testing Accuracy: 0.55\n",
            "Epoch: 1, Training Loss: 86.27, Training Accuracy: 0.50\n",
            "Epoch: 1, Validation Loss: 86.27, Validation Accuracy: 0.49\n",
            "Epoch: 1, Testing Loss: 86.27, Testing Accuracy: 0.55\n",
            "Epoch: 2, Training Loss: 85.25, Training Accuracy: 0.50\n",
            "Epoch: 2, Validation Loss: 85.25, Validation Accuracy: 0.49\n",
            "Epoch: 2, Testing Loss: 85.25, Testing Accuracy: 0.55\n",
            "Epoch: 3, Training Loss: 84.25, Training Accuracy: 0.50\n",
            "Epoch: 3, Validation Loss: 84.25, Validation Accuracy: 0.49\n",
            "Epoch: 3, Testing Loss: 84.25, Testing Accuracy: 0.55\n",
            "Epoch: 4, Training Loss: 83.27, Training Accuracy: 0.50\n",
            "Epoch: 4, Validation Loss: 83.27, Validation Accuracy: 0.49\n",
            "Epoch: 4, Testing Loss: 83.27, Testing Accuracy: 0.55\n",
            "Epoch: 5, Training Loss: 82.32, Training Accuracy: 0.50\n",
            "Epoch: 5, Validation Loss: 82.32, Validation Accuracy: 0.49\n",
            "Epoch: 5, Testing Loss: 82.32, Testing Accuracy: 0.55\n",
            "Epoch: 6, Training Loss: 81.39, Training Accuracy: 0.50\n",
            "Epoch: 6, Validation Loss: 81.39, Validation Accuracy: 0.49\n",
            "Epoch: 6, Testing Loss: 81.39, Testing Accuracy: 0.55\n",
            "Epoch: 7, Training Loss: 80.48, Training Accuracy: 0.50\n",
            "Epoch: 7, Validation Loss: 80.48, Validation Accuracy: 0.49\n",
            "Epoch: 7, Testing Loss: 80.48, Testing Accuracy: 0.55\n",
            "Epoch: 8, Training Loss: 79.59, Training Accuracy: 0.50\n",
            "Epoch: 8, Validation Loss: 79.59, Validation Accuracy: 0.49\n",
            "Epoch: 8, Testing Loss: 79.59, Testing Accuracy: 0.55\n",
            "Epoch: 9, Training Loss: 78.72, Training Accuracy: 0.50\n",
            "Epoch: 9, Validation Loss: 78.72, Validation Accuracy: 0.49\n",
            "Epoch: 9, Testing Loss: 78.72, Testing Accuracy: 0.55\n",
            "Epoch: 10, Training Loss: 77.87, Training Accuracy: 0.50\n",
            "Epoch: 10, Validation Loss: 77.87, Validation Accuracy: 0.49\n",
            "Epoch: 10, Testing Loss: 77.87, Testing Accuracy: 0.55\n",
            "Epoch: 11, Training Loss: 77.04, Training Accuracy: 0.50\n",
            "Epoch: 11, Validation Loss: 77.04, Validation Accuracy: 0.49\n",
            "Epoch: 11, Testing Loss: 77.04, Testing Accuracy: 0.55\n",
            "Epoch: 12, Training Loss: 76.23, Training Accuracy: 0.50\n",
            "Epoch: 12, Validation Loss: 76.23, Validation Accuracy: 0.49\n",
            "Epoch: 12, Testing Loss: 76.23, Testing Accuracy: 0.55\n",
            "Epoch: 13, Training Loss: 75.43, Training Accuracy: 0.50\n",
            "Epoch: 13, Validation Loss: 75.43, Validation Accuracy: 0.49\n",
            "Epoch: 13, Testing Loss: 75.43, Testing Accuracy: 0.55\n",
            "Epoch: 14, Training Loss: 74.65, Training Accuracy: 0.50\n",
            "Epoch: 14, Validation Loss: 74.65, Validation Accuracy: 0.49\n",
            "Epoch: 14, Testing Loss: 74.65, Testing Accuracy: 0.55\n",
            "Epoch: 15, Training Loss: 73.89, Training Accuracy: 0.50\n",
            "Epoch: 15, Validation Loss: 73.89, Validation Accuracy: 0.49\n",
            "Epoch: 15, Testing Loss: 73.89, Testing Accuracy: 0.55\n",
            "Epoch: 16, Training Loss: 73.14, Training Accuracy: 0.50\n",
            "Epoch: 16, Validation Loss: 73.14, Validation Accuracy: 0.49\n",
            "Epoch: 16, Testing Loss: 73.14, Testing Accuracy: 0.55\n",
            "Epoch: 17, Training Loss: 72.41, Training Accuracy: 0.50\n",
            "Epoch: 17, Validation Loss: 72.41, Validation Accuracy: 0.49\n",
            "Epoch: 17, Testing Loss: 72.41, Testing Accuracy: 0.55\n",
            "Epoch: 18, Training Loss: 71.70, Training Accuracy: 0.50\n",
            "Epoch: 18, Validation Loss: 71.70, Validation Accuracy: 0.49\n",
            "Epoch: 18, Testing Loss: 71.70, Testing Accuracy: 0.55\n",
            "Epoch: 19, Training Loss: 71.00, Training Accuracy: 0.50\n",
            "Epoch: 19, Validation Loss: 71.00, Validation Accuracy: 0.50\n",
            "Epoch: 19, Testing Loss: 71.00, Testing Accuracy: 0.55\n",
            "Epoch: 20, Training Loss: 70.31, Training Accuracy: 0.52\n",
            "Epoch: 20, Validation Loss: 70.31, Validation Accuracy: 0.51\n",
            "Epoch: 20, Testing Loss: 70.31, Testing Accuracy: 0.56\n",
            "Epoch: 21, Training Loss: 69.64, Training Accuracy: 0.54\n",
            "Epoch: 21, Validation Loss: 69.64, Validation Accuracy: 0.53\n",
            "Epoch: 21, Testing Loss: 69.64, Testing Accuracy: 0.59\n",
            "Epoch: 22, Training Loss: 68.98, Training Accuracy: 0.56\n",
            "Epoch: 22, Validation Loss: 68.98, Validation Accuracy: 0.55\n",
            "Epoch: 22, Testing Loss: 68.98, Testing Accuracy: 0.60\n",
            "Epoch: 23, Training Loss: 68.34, Training Accuracy: 0.58\n",
            "Epoch: 23, Validation Loss: 68.34, Validation Accuracy: 0.58\n",
            "Epoch: 23, Testing Loss: 68.34, Testing Accuracy: 0.61\n",
            "Epoch: 24, Training Loss: 67.71, Training Accuracy: 0.60\n",
            "Epoch: 24, Validation Loss: 67.71, Validation Accuracy: 0.58\n",
            "Epoch: 24, Testing Loss: 67.71, Testing Accuracy: 0.63\n",
            "Epoch: 25, Training Loss: 67.09, Training Accuracy: 0.62\n",
            "Epoch: 25, Validation Loss: 67.09, Validation Accuracy: 0.59\n",
            "Epoch: 25, Testing Loss: 67.09, Testing Accuracy: 0.64\n",
            "Epoch: 26, Training Loss: 66.48, Training Accuracy: 0.64\n",
            "Epoch: 26, Validation Loss: 66.48, Validation Accuracy: 0.59\n",
            "Epoch: 26, Testing Loss: 66.48, Testing Accuracy: 0.65\n",
            "Epoch: 27, Training Loss: 65.89, Training Accuracy: 0.67\n",
            "Epoch: 27, Validation Loss: 65.89, Validation Accuracy: 0.60\n",
            "Epoch: 27, Testing Loss: 65.89, Testing Accuracy: 0.68\n",
            "Epoch: 28, Training Loss: 65.31, Training Accuracy: 0.68\n",
            "Epoch: 28, Validation Loss: 65.31, Validation Accuracy: 0.61\n",
            "Epoch: 28, Testing Loss: 65.31, Testing Accuracy: 0.72\n",
            "Epoch: 29, Training Loss: 64.73, Training Accuracy: 0.70\n",
            "Epoch: 29, Validation Loss: 64.73, Validation Accuracy: 0.61\n",
            "Epoch: 29, Testing Loss: 64.73, Testing Accuracy: 0.76\n",
            "Epoch: 30, Training Loss: 64.17, Training Accuracy: 0.72\n",
            "Epoch: 30, Validation Loss: 64.17, Validation Accuracy: 0.64\n",
            "Epoch: 30, Testing Loss: 64.17, Testing Accuracy: 0.77\n",
            "Epoch: 31, Training Loss: 63.62, Training Accuracy: 0.73\n",
            "Epoch: 31, Validation Loss: 63.62, Validation Accuracy: 0.65\n",
            "Epoch: 31, Testing Loss: 63.62, Testing Accuracy: 0.79\n",
            "Epoch: 32, Training Loss: 63.08, Training Accuracy: 0.74\n",
            "Epoch: 32, Validation Loss: 63.08, Validation Accuracy: 0.69\n",
            "Epoch: 32, Testing Loss: 63.08, Testing Accuracy: 0.80\n",
            "Epoch: 33, Training Loss: 62.55, Training Accuracy: 0.76\n",
            "Epoch: 33, Validation Loss: 62.55, Validation Accuracy: 0.69\n",
            "Epoch: 33, Testing Loss: 62.55, Testing Accuracy: 0.81\n",
            "Epoch: 34, Training Loss: 62.03, Training Accuracy: 0.77\n",
            "Epoch: 34, Validation Loss: 62.03, Validation Accuracy: 0.69\n",
            "Epoch: 34, Testing Loss: 62.03, Testing Accuracy: 0.81\n",
            "Epoch: 35, Training Loss: 61.52, Training Accuracy: 0.78\n",
            "Epoch: 35, Validation Loss: 61.52, Validation Accuracy: 0.69\n",
            "Epoch: 35, Testing Loss: 61.52, Testing Accuracy: 0.82\n",
            "Epoch: 36, Training Loss: 61.02, Training Accuracy: 0.79\n",
            "Epoch: 36, Validation Loss: 61.02, Validation Accuracy: 0.74\n",
            "Epoch: 36, Testing Loss: 61.02, Testing Accuracy: 0.83\n",
            "Epoch: 37, Training Loss: 60.53, Training Accuracy: 0.80\n",
            "Epoch: 37, Validation Loss: 60.53, Validation Accuracy: 0.75\n",
            "Epoch: 37, Testing Loss: 60.53, Testing Accuracy: 0.83\n",
            "Epoch: 38, Training Loss: 60.05, Training Accuracy: 0.80\n",
            "Epoch: 38, Validation Loss: 60.05, Validation Accuracy: 0.76\n",
            "Epoch: 38, Testing Loss: 60.05, Testing Accuracy: 0.83\n",
            "Epoch: 39, Training Loss: 59.57, Training Accuracy: 0.81\n",
            "Epoch: 39, Validation Loss: 59.57, Validation Accuracy: 0.77\n",
            "Epoch: 39, Testing Loss: 59.57, Testing Accuracy: 0.84\n",
            "Epoch: 40, Training Loss: 59.11, Training Accuracy: 0.82\n",
            "Epoch: 40, Validation Loss: 59.11, Validation Accuracy: 0.78\n",
            "Epoch: 40, Testing Loss: 59.11, Testing Accuracy: 0.84\n",
            "Epoch: 41, Training Loss: 58.65, Training Accuracy: 0.83\n",
            "Epoch: 41, Validation Loss: 58.65, Validation Accuracy: 0.80\n",
            "Epoch: 41, Testing Loss: 58.65, Testing Accuracy: 0.86\n",
            "Epoch: 42, Training Loss: 58.20, Training Accuracy: 0.83\n",
            "Epoch: 42, Validation Loss: 58.20, Validation Accuracy: 0.80\n",
            "Epoch: 42, Testing Loss: 58.20, Testing Accuracy: 0.86\n",
            "Epoch: 43, Training Loss: 57.76, Training Accuracy: 0.84\n",
            "Epoch: 43, Validation Loss: 57.76, Validation Accuracy: 0.81\n",
            "Epoch: 43, Testing Loss: 57.76, Testing Accuracy: 0.87\n",
            "Epoch: 44, Training Loss: 57.32, Training Accuracy: 0.84\n",
            "Epoch: 44, Validation Loss: 57.32, Validation Accuracy: 0.81\n",
            "Epoch: 44, Testing Loss: 57.32, Testing Accuracy: 0.87\n",
            "Epoch: 45, Training Loss: 56.90, Training Accuracy: 0.85\n",
            "Epoch: 45, Validation Loss: 56.90, Validation Accuracy: 0.82\n",
            "Epoch: 45, Testing Loss: 56.90, Testing Accuracy: 0.87\n",
            "Epoch: 46, Training Loss: 56.48, Training Accuracy: 0.85\n",
            "Epoch: 46, Validation Loss: 56.48, Validation Accuracy: 0.83\n",
            "Epoch: 46, Testing Loss: 56.48, Testing Accuracy: 0.87\n",
            "Epoch: 47, Training Loss: 56.06, Training Accuracy: 0.85\n",
            "Epoch: 47, Validation Loss: 56.06, Validation Accuracy: 0.83\n",
            "Epoch: 47, Testing Loss: 56.06, Testing Accuracy: 0.88\n",
            "Epoch: 48, Training Loss: 55.66, Training Accuracy: 0.86\n",
            "Epoch: 48, Validation Loss: 55.66, Validation Accuracy: 0.83\n",
            "Epoch: 48, Testing Loss: 55.66, Testing Accuracy: 0.88\n",
            "Epoch: 49, Training Loss: 55.26, Training Accuracy: 0.86\n",
            "Epoch: 49, Validation Loss: 55.26, Validation Accuracy: 0.83\n",
            "Epoch: 49, Testing Loss: 55.26, Testing Accuracy: 0.88\n",
            "Epoch: 50, Training Loss: 54.87, Training Accuracy: 0.86\n",
            "Epoch: 50, Validation Loss: 54.87, Validation Accuracy: 0.84\n",
            "Epoch: 50, Testing Loss: 54.87, Testing Accuracy: 0.88\n",
            "Epoch: 51, Training Loss: 54.48, Training Accuracy: 0.87\n",
            "Epoch: 51, Validation Loss: 54.48, Validation Accuracy: 0.86\n",
            "Epoch: 51, Testing Loss: 54.48, Testing Accuracy: 0.88\n",
            "Epoch: 52, Training Loss: 54.10, Training Accuracy: 0.87\n",
            "Epoch: 52, Validation Loss: 54.10, Validation Accuracy: 0.86\n",
            "Epoch: 52, Testing Loss: 54.10, Testing Accuracy: 0.88\n",
            "Epoch: 53, Training Loss: 53.73, Training Accuracy: 0.87\n",
            "Epoch: 53, Validation Loss: 53.73, Validation Accuracy: 0.86\n",
            "Epoch: 53, Testing Loss: 53.73, Testing Accuracy: 0.90\n",
            "Epoch: 54, Training Loss: 53.36, Training Accuracy: 0.88\n",
            "Epoch: 54, Validation Loss: 53.36, Validation Accuracy: 0.87\n",
            "Epoch: 54, Testing Loss: 53.36, Testing Accuracy: 0.90\n",
            "Epoch: 55, Training Loss: 53.00, Training Accuracy: 0.88\n",
            "Epoch: 55, Validation Loss: 53.00, Validation Accuracy: 0.87\n",
            "Epoch: 55, Testing Loss: 53.00, Testing Accuracy: 0.90\n",
            "Epoch: 56, Training Loss: 52.64, Training Accuracy: 0.88\n",
            "Epoch: 56, Validation Loss: 52.64, Validation Accuracy: 0.87\n",
            "Epoch: 56, Testing Loss: 52.64, Testing Accuracy: 0.91\n",
            "Epoch: 57, Training Loss: 52.29, Training Accuracy: 0.88\n",
            "Epoch: 57, Validation Loss: 52.29, Validation Accuracy: 0.88\n",
            "Epoch: 57, Testing Loss: 52.29, Testing Accuracy: 0.91\n",
            "Epoch: 58, Training Loss: 51.95, Training Accuracy: 0.88\n",
            "Epoch: 58, Validation Loss: 51.95, Validation Accuracy: 0.88\n",
            "Epoch: 58, Testing Loss: 51.95, Testing Accuracy: 0.91\n",
            "Epoch: 59, Training Loss: 51.61, Training Accuracy: 0.89\n",
            "Epoch: 59, Validation Loss: 51.61, Validation Accuracy: 0.88\n",
            "Epoch: 59, Testing Loss: 51.61, Testing Accuracy: 0.91\n",
            "Epoch: 60, Training Loss: 51.28, Training Accuracy: 0.89\n",
            "Epoch: 60, Validation Loss: 51.28, Validation Accuracy: 0.88\n",
            "Epoch: 60, Testing Loss: 51.28, Testing Accuracy: 0.92\n",
            "Epoch: 61, Training Loss: 50.95, Training Accuracy: 0.89\n",
            "Epoch: 61, Validation Loss: 50.95, Validation Accuracy: 0.89\n",
            "Epoch: 61, Testing Loss: 50.95, Testing Accuracy: 0.92\n",
            "Epoch: 62, Training Loss: 50.63, Training Accuracy: 0.89\n",
            "Epoch: 62, Validation Loss: 50.63, Validation Accuracy: 0.89\n",
            "Epoch: 62, Testing Loss: 50.63, Testing Accuracy: 0.92\n",
            "Epoch: 63, Training Loss: 50.31, Training Accuracy: 0.89\n",
            "Epoch: 63, Validation Loss: 50.31, Validation Accuracy: 0.89\n",
            "Epoch: 63, Testing Loss: 50.31, Testing Accuracy: 0.92\n",
            "Epoch: 64, Training Loss: 49.99, Training Accuracy: 0.90\n",
            "Epoch: 64, Validation Loss: 49.99, Validation Accuracy: 0.89\n",
            "Epoch: 64, Testing Loss: 49.99, Testing Accuracy: 0.92\n",
            "Epoch: 65, Training Loss: 49.68, Training Accuracy: 0.90\n",
            "Epoch: 65, Validation Loss: 49.68, Validation Accuracy: 0.90\n",
            "Epoch: 65, Testing Loss: 49.68, Testing Accuracy: 0.92\n",
            "Epoch: 66, Training Loss: 49.38, Training Accuracy: 0.90\n",
            "Epoch: 66, Validation Loss: 49.38, Validation Accuracy: 0.90\n",
            "Epoch: 66, Testing Loss: 49.38, Testing Accuracy: 0.92\n",
            "Epoch: 67, Training Loss: 49.08, Training Accuracy: 0.90\n",
            "Epoch: 67, Validation Loss: 49.08, Validation Accuracy: 0.90\n",
            "Epoch: 67, Testing Loss: 49.08, Testing Accuracy: 0.92\n",
            "Epoch: 68, Training Loss: 48.78, Training Accuracy: 0.90\n",
            "Epoch: 68, Validation Loss: 48.78, Validation Accuracy: 0.90\n",
            "Epoch: 68, Testing Loss: 48.78, Testing Accuracy: 0.92\n",
            "Epoch: 69, Training Loss: 48.49, Training Accuracy: 0.90\n",
            "Epoch: 69, Validation Loss: 48.49, Validation Accuracy: 0.90\n",
            "Epoch: 69, Testing Loss: 48.49, Testing Accuracy: 0.92\n",
            "Epoch: 70, Training Loss: 48.20, Training Accuracy: 0.90\n",
            "Epoch: 70, Validation Loss: 48.20, Validation Accuracy: 0.90\n",
            "Epoch: 70, Testing Loss: 48.20, Testing Accuracy: 0.92\n",
            "Epoch: 71, Training Loss: 47.92, Training Accuracy: 0.91\n",
            "Epoch: 71, Validation Loss: 47.92, Validation Accuracy: 0.90\n",
            "Epoch: 71, Testing Loss: 47.92, Testing Accuracy: 0.92\n",
            "Epoch: 72, Training Loss: 47.64, Training Accuracy: 0.91\n",
            "Epoch: 72, Validation Loss: 47.64, Validation Accuracy: 0.90\n",
            "Epoch: 72, Testing Loss: 47.64, Testing Accuracy: 0.92\n",
            "Epoch: 73, Training Loss: 47.37, Training Accuracy: 0.91\n",
            "Epoch: 73, Validation Loss: 47.37, Validation Accuracy: 0.90\n",
            "Epoch: 73, Testing Loss: 47.37, Testing Accuracy: 0.92\n",
            "Epoch: 74, Training Loss: 47.09, Training Accuracy: 0.91\n",
            "Epoch: 74, Validation Loss: 47.09, Validation Accuracy: 0.90\n",
            "Epoch: 74, Testing Loss: 47.09, Testing Accuracy: 0.92\n",
            "Epoch: 75, Training Loss: 46.83, Training Accuracy: 0.91\n",
            "Epoch: 75, Validation Loss: 46.83, Validation Accuracy: 0.90\n",
            "Epoch: 75, Testing Loss: 46.83, Testing Accuracy: 0.92\n",
            "Epoch: 76, Training Loss: 46.56, Training Accuracy: 0.91\n",
            "Epoch: 76, Validation Loss: 46.56, Validation Accuracy: 0.90\n",
            "Epoch: 76, Testing Loss: 46.56, Testing Accuracy: 0.92\n",
            "Epoch: 77, Training Loss: 46.30, Training Accuracy: 0.91\n",
            "Epoch: 77, Validation Loss: 46.30, Validation Accuracy: 0.90\n",
            "Epoch: 77, Testing Loss: 46.30, Testing Accuracy: 0.92\n",
            "Epoch: 78, Training Loss: 46.05, Training Accuracy: 0.91\n",
            "Epoch: 78, Validation Loss: 46.05, Validation Accuracy: 0.90\n",
            "Epoch: 78, Testing Loss: 46.05, Testing Accuracy: 0.92\n",
            "Epoch: 79, Training Loss: 45.79, Training Accuracy: 0.91\n",
            "Epoch: 79, Validation Loss: 45.79, Validation Accuracy: 0.91\n",
            "Epoch: 79, Testing Loss: 45.79, Testing Accuracy: 0.92\n",
            "Epoch: 80, Training Loss: 45.54, Training Accuracy: 0.91\n",
            "Epoch: 80, Validation Loss: 45.54, Validation Accuracy: 0.91\n",
            "Epoch: 80, Testing Loss: 45.54, Testing Accuracy: 0.92\n",
            "Epoch: 81, Training Loss: 45.30, Training Accuracy: 0.91\n",
            "Epoch: 81, Validation Loss: 45.30, Validation Accuracy: 0.91\n",
            "Epoch: 81, Testing Loss: 45.30, Testing Accuracy: 0.92\n",
            "Epoch: 82, Training Loss: 45.05, Training Accuracy: 0.91\n",
            "Epoch: 82, Validation Loss: 45.05, Validation Accuracy: 0.91\n",
            "Epoch: 82, Testing Loss: 45.05, Testing Accuracy: 0.92\n",
            "Epoch: 83, Training Loss: 44.82, Training Accuracy: 0.91\n",
            "Epoch: 83, Validation Loss: 44.82, Validation Accuracy: 0.91\n",
            "Epoch: 83, Testing Loss: 44.82, Testing Accuracy: 0.92\n",
            "Epoch: 84, Training Loss: 44.58, Training Accuracy: 0.92\n",
            "Epoch: 84, Validation Loss: 44.58, Validation Accuracy: 0.91\n",
            "Epoch: 84, Testing Loss: 44.58, Testing Accuracy: 0.92\n",
            "Epoch: 85, Training Loss: 44.35, Training Accuracy: 0.92\n",
            "Epoch: 85, Validation Loss: 44.35, Validation Accuracy: 0.91\n",
            "Epoch: 85, Testing Loss: 44.35, Testing Accuracy: 0.93\n",
            "Epoch: 86, Training Loss: 44.12, Training Accuracy: 0.92\n",
            "Epoch: 86, Validation Loss: 44.12, Validation Accuracy: 0.91\n",
            "Epoch: 86, Testing Loss: 44.12, Testing Accuracy: 0.93\n",
            "Epoch: 87, Training Loss: 43.89, Training Accuracy: 0.92\n",
            "Epoch: 87, Validation Loss: 43.89, Validation Accuracy: 0.91\n",
            "Epoch: 87, Testing Loss: 43.89, Testing Accuracy: 0.94\n",
            "Epoch: 88, Training Loss: 43.66, Training Accuracy: 0.92\n",
            "Epoch: 88, Validation Loss: 43.66, Validation Accuracy: 0.91\n",
            "Epoch: 88, Testing Loss: 43.66, Testing Accuracy: 0.94\n",
            "Epoch: 89, Training Loss: 43.44, Training Accuracy: 0.92\n",
            "Epoch: 89, Validation Loss: 43.44, Validation Accuracy: 0.91\n",
            "Epoch: 89, Testing Loss: 43.44, Testing Accuracy: 0.94\n",
            "Epoch: 90, Training Loss: 43.22, Training Accuracy: 0.92\n",
            "Epoch: 90, Validation Loss: 43.22, Validation Accuracy: 0.91\n",
            "Epoch: 90, Testing Loss: 43.22, Testing Accuracy: 0.94\n",
            "Epoch: 91, Training Loss: 43.01, Training Accuracy: 0.92\n",
            "Epoch: 91, Validation Loss: 43.01, Validation Accuracy: 0.91\n",
            "Epoch: 91, Testing Loss: 43.01, Testing Accuracy: 0.94\n",
            "Epoch: 92, Training Loss: 42.79, Training Accuracy: 0.92\n",
            "Epoch: 92, Validation Loss: 42.79, Validation Accuracy: 0.91\n",
            "Epoch: 92, Testing Loss: 42.79, Testing Accuracy: 0.94\n",
            "Epoch: 93, Training Loss: 42.58, Training Accuracy: 0.92\n",
            "Epoch: 93, Validation Loss: 42.58, Validation Accuracy: 0.92\n",
            "Epoch: 93, Testing Loss: 42.58, Testing Accuracy: 0.94\n",
            "Epoch: 94, Training Loss: 42.38, Training Accuracy: 0.92\n",
            "Epoch: 94, Validation Loss: 42.38, Validation Accuracy: 0.92\n",
            "Epoch: 94, Testing Loss: 42.38, Testing Accuracy: 0.94\n",
            "Epoch: 95, Training Loss: 42.17, Training Accuracy: 0.92\n",
            "Epoch: 95, Validation Loss: 42.17, Validation Accuracy: 0.92\n",
            "Epoch: 95, Testing Loss: 42.17, Testing Accuracy: 0.94\n",
            "Epoch: 96, Training Loss: 41.97, Training Accuracy: 0.92\n",
            "Epoch: 96, Validation Loss: 41.97, Validation Accuracy: 0.92\n",
            "Epoch: 96, Testing Loss: 41.97, Testing Accuracy: 0.94\n",
            "Epoch: 97, Training Loss: 41.77, Training Accuracy: 0.92\n",
            "Epoch: 97, Validation Loss: 41.77, Validation Accuracy: 0.92\n",
            "Epoch: 97, Testing Loss: 41.77, Testing Accuracy: 0.94\n",
            "Epoch: 98, Training Loss: 41.57, Training Accuracy: 0.92\n",
            "Epoch: 98, Validation Loss: 41.57, Validation Accuracy: 0.92\n",
            "Epoch: 98, Testing Loss: 41.57, Testing Accuracy: 0.94\n",
            "Epoch: 99, Training Loss: 41.38, Training Accuracy: 0.93\n",
            "Epoch: 99, Validation Loss: 41.38, Validation Accuracy: 0.92\n",
            "Epoch: 99, Testing Loss: 41.38, Testing Accuracy: 0.94\n",
            "Epoch: 100, Training Loss: 41.18, Training Accuracy: 0.93\n",
            "Epoch: 100, Validation Loss: 41.18, Validation Accuracy: 0.92\n",
            "Epoch: 100, Testing Loss: 41.18, Testing Accuracy: 0.94\n",
            "Epoch: 101, Training Loss: 40.99, Training Accuracy: 0.93\n",
            "Epoch: 101, Validation Loss: 40.99, Validation Accuracy: 0.92\n",
            "Epoch: 101, Testing Loss: 40.99, Testing Accuracy: 0.94\n",
            "Epoch: 102, Training Loss: 40.80, Training Accuracy: 0.93\n",
            "Epoch: 102, Validation Loss: 40.80, Validation Accuracy: 0.92\n",
            "Epoch: 102, Testing Loss: 40.80, Testing Accuracy: 0.94\n",
            "Epoch: 103, Training Loss: 40.62, Training Accuracy: 0.93\n",
            "Epoch: 103, Validation Loss: 40.62, Validation Accuracy: 0.92\n",
            "Epoch: 103, Testing Loss: 40.62, Testing Accuracy: 0.94\n",
            "Epoch: 104, Training Loss: 40.43, Training Accuracy: 0.93\n",
            "Epoch: 104, Validation Loss: 40.43, Validation Accuracy: 0.92\n",
            "Epoch: 104, Testing Loss: 40.43, Testing Accuracy: 0.94\n",
            "Epoch: 105, Training Loss: 40.25, Training Accuracy: 0.93\n",
            "Epoch: 105, Validation Loss: 40.25, Validation Accuracy: 0.92\n",
            "Epoch: 105, Testing Loss: 40.25, Testing Accuracy: 0.94\n",
            "Epoch: 106, Training Loss: 40.07, Training Accuracy: 0.93\n",
            "Epoch: 106, Validation Loss: 40.07, Validation Accuracy: 0.92\n",
            "Epoch: 106, Testing Loss: 40.07, Testing Accuracy: 0.94\n",
            "Epoch: 107, Training Loss: 39.89, Training Accuracy: 0.93\n",
            "Epoch: 107, Validation Loss: 39.89, Validation Accuracy: 0.92\n",
            "Epoch: 107, Testing Loss: 39.89, Testing Accuracy: 0.93\n",
            "Epoch: 108, Training Loss: 39.72, Training Accuracy: 0.93\n",
            "Epoch: 108, Validation Loss: 39.72, Validation Accuracy: 0.93\n",
            "Epoch: 108, Testing Loss: 39.72, Testing Accuracy: 0.93\n",
            "Epoch: 109, Training Loss: 39.55, Training Accuracy: 0.93\n",
            "Epoch: 109, Validation Loss: 39.55, Validation Accuracy: 0.93\n",
            "Epoch: 109, Testing Loss: 39.55, Testing Accuracy: 0.93\n",
            "Epoch: 110, Training Loss: 39.37, Training Accuracy: 0.93\n",
            "Epoch: 110, Validation Loss: 39.37, Validation Accuracy: 0.94\n",
            "Epoch: 110, Testing Loss: 39.37, Testing Accuracy: 0.93\n",
            "Epoch: 111, Training Loss: 39.20, Training Accuracy: 0.93\n",
            "Epoch: 111, Validation Loss: 39.20, Validation Accuracy: 0.94\n",
            "Epoch: 111, Testing Loss: 39.20, Testing Accuracy: 0.93\n",
            "Epoch: 112, Training Loss: 39.04, Training Accuracy: 0.93\n",
            "Epoch: 112, Validation Loss: 39.04, Validation Accuracy: 0.95\n",
            "Epoch: 112, Testing Loss: 39.04, Testing Accuracy: 0.93\n",
            "Epoch: 113, Training Loss: 38.87, Training Accuracy: 0.93\n",
            "Epoch: 113, Validation Loss: 38.87, Validation Accuracy: 0.95\n",
            "Epoch: 113, Testing Loss: 38.87, Testing Accuracy: 0.93\n",
            "Epoch: 114, Training Loss: 38.71, Training Accuracy: 0.93\n",
            "Epoch: 114, Validation Loss: 38.71, Validation Accuracy: 0.95\n",
            "Epoch: 114, Testing Loss: 38.71, Testing Accuracy: 0.93\n",
            "Epoch: 115, Training Loss: 38.55, Training Accuracy: 0.93\n",
            "Epoch: 115, Validation Loss: 38.55, Validation Accuracy: 0.95\n",
            "Epoch: 115, Testing Loss: 38.55, Testing Accuracy: 0.93\n",
            "Epoch: 116, Training Loss: 38.39, Training Accuracy: 0.93\n",
            "Epoch: 116, Validation Loss: 38.39, Validation Accuracy: 0.95\n",
            "Epoch: 116, Testing Loss: 38.39, Testing Accuracy: 0.93\n",
            "Epoch: 117, Training Loss: 38.23, Training Accuracy: 0.93\n",
            "Epoch: 117, Validation Loss: 38.23, Validation Accuracy: 0.95\n",
            "Epoch: 117, Testing Loss: 38.23, Testing Accuracy: 0.93\n",
            "Epoch: 118, Training Loss: 38.07, Training Accuracy: 0.93\n",
            "Epoch: 118, Validation Loss: 38.07, Validation Accuracy: 0.95\n",
            "Epoch: 118, Testing Loss: 38.07, Testing Accuracy: 0.93\n",
            "Epoch: 119, Training Loss: 37.92, Training Accuracy: 0.93\n",
            "Epoch: 119, Validation Loss: 37.92, Validation Accuracy: 0.95\n",
            "Epoch: 119, Testing Loss: 37.92, Testing Accuracy: 0.93\n",
            "Epoch: 120, Training Loss: 37.76, Training Accuracy: 0.93\n",
            "Epoch: 120, Validation Loss: 37.76, Validation Accuracy: 0.95\n",
            "Epoch: 120, Testing Loss: 37.76, Testing Accuracy: 0.93\n",
            "Epoch: 121, Training Loss: 37.61, Training Accuracy: 0.93\n",
            "Epoch: 121, Validation Loss: 37.61, Validation Accuracy: 0.95\n",
            "Epoch: 121, Testing Loss: 37.61, Testing Accuracy: 0.93\n",
            "Epoch: 122, Training Loss: 37.46, Training Accuracy: 0.93\n",
            "Epoch: 122, Validation Loss: 37.46, Validation Accuracy: 0.95\n",
            "Epoch: 122, Testing Loss: 37.46, Testing Accuracy: 0.93\n",
            "Epoch: 123, Training Loss: 37.31, Training Accuracy: 0.93\n",
            "Epoch: 123, Validation Loss: 37.31, Validation Accuracy: 0.95\n",
            "Epoch: 123, Testing Loss: 37.31, Testing Accuracy: 0.93\n",
            "Epoch: 124, Training Loss: 37.17, Training Accuracy: 0.93\n",
            "Epoch: 124, Validation Loss: 37.17, Validation Accuracy: 0.95\n",
            "Epoch: 124, Testing Loss: 37.17, Testing Accuracy: 0.93\n",
            "Epoch: 125, Training Loss: 37.02, Training Accuracy: 0.93\n",
            "Epoch: 125, Validation Loss: 37.02, Validation Accuracy: 0.95\n",
            "Epoch: 125, Testing Loss: 37.02, Testing Accuracy: 0.93\n",
            "Epoch: 126, Training Loss: 36.88, Training Accuracy: 0.93\n",
            "Epoch: 126, Validation Loss: 36.88, Validation Accuracy: 0.95\n",
            "Epoch: 126, Testing Loss: 36.88, Testing Accuracy: 0.93\n",
            "Epoch: 127, Training Loss: 36.73, Training Accuracy: 0.93\n",
            "Epoch: 127, Validation Loss: 36.73, Validation Accuracy: 0.95\n",
            "Epoch: 127, Testing Loss: 36.73, Testing Accuracy: 0.93\n",
            "Epoch: 128, Training Loss: 36.59, Training Accuracy: 0.93\n",
            "Epoch: 128, Validation Loss: 36.59, Validation Accuracy: 0.95\n",
            "Epoch: 128, Testing Loss: 36.59, Testing Accuracy: 0.93\n",
            "Epoch: 129, Training Loss: 36.45, Training Accuracy: 0.93\n",
            "Epoch: 129, Validation Loss: 36.45, Validation Accuracy: 0.95\n",
            "Epoch: 129, Testing Loss: 36.45, Testing Accuracy: 0.93\n",
            "Epoch: 130, Training Loss: 36.32, Training Accuracy: 0.93\n",
            "Epoch: 130, Validation Loss: 36.32, Validation Accuracy: 0.95\n",
            "Epoch: 130, Testing Loss: 36.32, Testing Accuracy: 0.93\n",
            "Epoch: 131, Training Loss: 36.18, Training Accuracy: 0.94\n",
            "Epoch: 131, Validation Loss: 36.18, Validation Accuracy: 0.95\n",
            "Epoch: 131, Testing Loss: 36.18, Testing Accuracy: 0.93\n",
            "Epoch: 132, Training Loss: 36.04, Training Accuracy: 0.94\n",
            "Epoch: 132, Validation Loss: 36.04, Validation Accuracy: 0.95\n",
            "Epoch: 132, Testing Loss: 36.04, Testing Accuracy: 0.93\n",
            "Epoch: 133, Training Loss: 35.91, Training Accuracy: 0.94\n",
            "Epoch: 133, Validation Loss: 35.91, Validation Accuracy: 0.95\n",
            "Epoch: 133, Testing Loss: 35.91, Testing Accuracy: 0.93\n",
            "Epoch: 134, Training Loss: 35.78, Training Accuracy: 0.94\n",
            "Epoch: 134, Validation Loss: 35.78, Validation Accuracy: 0.95\n",
            "Epoch: 134, Testing Loss: 35.78, Testing Accuracy: 0.93\n",
            "Epoch: 135, Training Loss: 35.65, Training Accuracy: 0.94\n",
            "Epoch: 135, Validation Loss: 35.65, Validation Accuracy: 0.95\n",
            "Epoch: 135, Testing Loss: 35.65, Testing Accuracy: 0.93\n",
            "Epoch: 136, Training Loss: 35.52, Training Accuracy: 0.94\n",
            "Epoch: 136, Validation Loss: 35.52, Validation Accuracy: 0.95\n",
            "Epoch: 136, Testing Loss: 35.52, Testing Accuracy: 0.93\n",
            "Epoch: 137, Training Loss: 35.39, Training Accuracy: 0.94\n",
            "Epoch: 137, Validation Loss: 35.39, Validation Accuracy: 0.95\n",
            "Epoch: 137, Testing Loss: 35.39, Testing Accuracy: 0.93\n",
            "Epoch: 138, Training Loss: 35.26, Training Accuracy: 0.94\n",
            "Epoch: 138, Validation Loss: 35.26, Validation Accuracy: 0.95\n",
            "Epoch: 138, Testing Loss: 35.26, Testing Accuracy: 0.93\n",
            "Epoch: 139, Training Loss: 35.14, Training Accuracy: 0.94\n",
            "Epoch: 139, Validation Loss: 35.14, Validation Accuracy: 0.95\n",
            "Epoch: 139, Testing Loss: 35.14, Testing Accuracy: 0.94\n",
            "Epoch: 140, Training Loss: 35.01, Training Accuracy: 0.94\n",
            "Epoch: 140, Validation Loss: 35.01, Validation Accuracy: 0.95\n",
            "Epoch: 140, Testing Loss: 35.01, Testing Accuracy: 0.94\n",
            "Epoch: 141, Training Loss: 34.89, Training Accuracy: 0.94\n",
            "Epoch: 141, Validation Loss: 34.89, Validation Accuracy: 0.95\n",
            "Epoch: 141, Testing Loss: 34.89, Testing Accuracy: 0.94\n",
            "Epoch: 142, Training Loss: 34.77, Training Accuracy: 0.94\n",
            "Epoch: 142, Validation Loss: 34.77, Validation Accuracy: 0.95\n",
            "Epoch: 142, Testing Loss: 34.77, Testing Accuracy: 0.94\n",
            "Epoch: 143, Training Loss: 34.65, Training Accuracy: 0.94\n",
            "Epoch: 143, Validation Loss: 34.65, Validation Accuracy: 0.95\n",
            "Epoch: 143, Testing Loss: 34.65, Testing Accuracy: 0.94\n",
            "Epoch: 144, Training Loss: 34.53, Training Accuracy: 0.94\n",
            "Epoch: 144, Validation Loss: 34.53, Validation Accuracy: 0.95\n",
            "Epoch: 144, Testing Loss: 34.53, Testing Accuracy: 0.94\n",
            "Epoch: 145, Training Loss: 34.41, Training Accuracy: 0.94\n",
            "Epoch: 145, Validation Loss: 34.41, Validation Accuracy: 0.95\n",
            "Epoch: 145, Testing Loss: 34.41, Testing Accuracy: 0.94\n",
            "Epoch: 146, Training Loss: 34.29, Training Accuracy: 0.94\n",
            "Epoch: 146, Validation Loss: 34.29, Validation Accuracy: 0.95\n",
            "Epoch: 146, Testing Loss: 34.29, Testing Accuracy: 0.94\n",
            "Epoch: 147, Training Loss: 34.17, Training Accuracy: 0.94\n",
            "Epoch: 147, Validation Loss: 34.17, Validation Accuracy: 0.95\n",
            "Epoch: 147, Testing Loss: 34.17, Testing Accuracy: 0.94\n",
            "Epoch: 148, Training Loss: 34.06, Training Accuracy: 0.94\n",
            "Epoch: 148, Validation Loss: 34.06, Validation Accuracy: 0.95\n",
            "Epoch: 148, Testing Loss: 34.06, Testing Accuracy: 0.94\n",
            "Epoch: 149, Training Loss: 33.94, Training Accuracy: 0.94\n",
            "Epoch: 149, Validation Loss: 33.94, Validation Accuracy: 0.95\n",
            "Epoch: 149, Testing Loss: 33.94, Testing Accuracy: 0.94\n",
            "Epoch: 150, Training Loss: 33.83, Training Accuracy: 0.94\n",
            "Epoch: 150, Validation Loss: 33.83, Validation Accuracy: 0.95\n",
            "Epoch: 150, Testing Loss: 33.83, Testing Accuracy: 0.94\n",
            "Epoch: 151, Training Loss: 33.72, Training Accuracy: 0.94\n",
            "Epoch: 151, Validation Loss: 33.72, Validation Accuracy: 0.95\n",
            "Epoch: 151, Testing Loss: 33.72, Testing Accuracy: 0.94\n",
            "Epoch: 152, Training Loss: 33.61, Training Accuracy: 0.94\n",
            "Epoch: 152, Validation Loss: 33.61, Validation Accuracy: 0.95\n",
            "Epoch: 152, Testing Loss: 33.61, Testing Accuracy: 0.94\n",
            "Epoch: 153, Training Loss: 33.50, Training Accuracy: 0.94\n",
            "Epoch: 153, Validation Loss: 33.50, Validation Accuracy: 0.95\n",
            "Epoch: 153, Testing Loss: 33.50, Testing Accuracy: 0.94\n",
            "Epoch: 154, Training Loss: 33.39, Training Accuracy: 0.94\n",
            "Epoch: 154, Validation Loss: 33.39, Validation Accuracy: 0.95\n",
            "Epoch: 154, Testing Loss: 33.39, Testing Accuracy: 0.94\n",
            "Epoch: 155, Training Loss: 33.28, Training Accuracy: 0.94\n",
            "Epoch: 155, Validation Loss: 33.28, Validation Accuracy: 0.95\n",
            "Epoch: 155, Testing Loss: 33.28, Testing Accuracy: 0.94\n",
            "Epoch: 156, Training Loss: 33.17, Training Accuracy: 0.94\n",
            "Epoch: 156, Validation Loss: 33.17, Validation Accuracy: 0.95\n",
            "Epoch: 156, Testing Loss: 33.17, Testing Accuracy: 0.94\n",
            "Epoch: 157, Training Loss: 33.07, Training Accuracy: 0.94\n",
            "Epoch: 157, Validation Loss: 33.07, Validation Accuracy: 0.95\n",
            "Epoch: 157, Testing Loss: 33.07, Testing Accuracy: 0.95\n",
            "Epoch: 158, Training Loss: 32.96, Training Accuracy: 0.94\n",
            "Epoch: 158, Validation Loss: 32.96, Validation Accuracy: 0.95\n",
            "Epoch: 158, Testing Loss: 32.96, Testing Accuracy: 0.95\n",
            "Epoch: 159, Training Loss: 32.86, Training Accuracy: 0.94\n",
            "Epoch: 159, Validation Loss: 32.86, Validation Accuracy: 0.95\n",
            "Epoch: 159, Testing Loss: 32.86, Testing Accuracy: 0.95\n",
            "Epoch: 160, Training Loss: 32.76, Training Accuracy: 0.94\n",
            "Epoch: 160, Validation Loss: 32.76, Validation Accuracy: 0.95\n",
            "Epoch: 160, Testing Loss: 32.76, Testing Accuracy: 0.95\n",
            "Epoch: 161, Training Loss: 32.65, Training Accuracy: 0.94\n",
            "Epoch: 161, Validation Loss: 32.65, Validation Accuracy: 0.95\n",
            "Epoch: 161, Testing Loss: 32.65, Testing Accuracy: 0.95\n",
            "Epoch: 162, Training Loss: 32.55, Training Accuracy: 0.94\n",
            "Epoch: 162, Validation Loss: 32.55, Validation Accuracy: 0.95\n",
            "Epoch: 162, Testing Loss: 32.55, Testing Accuracy: 0.95\n",
            "Epoch: 163, Training Loss: 32.45, Training Accuracy: 0.94\n",
            "Epoch: 163, Validation Loss: 32.45, Validation Accuracy: 0.95\n",
            "Epoch: 163, Testing Loss: 32.45, Testing Accuracy: 0.95\n",
            "Epoch: 164, Training Loss: 32.35, Training Accuracy: 0.94\n",
            "Epoch: 164, Validation Loss: 32.35, Validation Accuracy: 0.95\n",
            "Epoch: 164, Testing Loss: 32.35, Testing Accuracy: 0.95\n",
            "Epoch: 165, Training Loss: 32.25, Training Accuracy: 0.94\n",
            "Epoch: 165, Validation Loss: 32.25, Validation Accuracy: 0.96\n",
            "Epoch: 165, Testing Loss: 32.25, Testing Accuracy: 0.95\n",
            "Epoch: 166, Training Loss: 32.16, Training Accuracy: 0.94\n",
            "Epoch: 166, Validation Loss: 32.16, Validation Accuracy: 0.96\n",
            "Epoch: 166, Testing Loss: 32.16, Testing Accuracy: 0.95\n",
            "Epoch: 167, Training Loss: 32.06, Training Accuracy: 0.94\n",
            "Epoch: 167, Validation Loss: 32.06, Validation Accuracy: 0.96\n",
            "Epoch: 167, Testing Loss: 32.06, Testing Accuracy: 0.95\n",
            "Epoch: 168, Training Loss: 31.96, Training Accuracy: 0.94\n",
            "Epoch: 168, Validation Loss: 31.96, Validation Accuracy: 0.96\n",
            "Epoch: 168, Testing Loss: 31.96, Testing Accuracy: 0.95\n",
            "Epoch: 169, Training Loss: 31.87, Training Accuracy: 0.94\n",
            "Epoch: 169, Validation Loss: 31.87, Validation Accuracy: 0.96\n",
            "Epoch: 169, Testing Loss: 31.87, Testing Accuracy: 0.95\n",
            "Epoch: 170, Training Loss: 31.77, Training Accuracy: 0.94\n",
            "Epoch: 170, Validation Loss: 31.77, Validation Accuracy: 0.96\n",
            "Epoch: 170, Testing Loss: 31.77, Testing Accuracy: 0.95\n",
            "Epoch: 171, Training Loss: 31.68, Training Accuracy: 0.94\n",
            "Epoch: 171, Validation Loss: 31.68, Validation Accuracy: 0.96\n",
            "Epoch: 171, Testing Loss: 31.68, Testing Accuracy: 0.95\n",
            "Epoch: 172, Training Loss: 31.59, Training Accuracy: 0.94\n",
            "Epoch: 172, Validation Loss: 31.59, Validation Accuracy: 0.96\n",
            "Epoch: 172, Testing Loss: 31.59, Testing Accuracy: 0.95\n",
            "Epoch: 173, Training Loss: 31.49, Training Accuracy: 0.94\n",
            "Epoch: 173, Validation Loss: 31.49, Validation Accuracy: 0.96\n",
            "Epoch: 173, Testing Loss: 31.49, Testing Accuracy: 0.96\n",
            "Epoch: 174, Training Loss: 31.40, Training Accuracy: 0.94\n",
            "Epoch: 174, Validation Loss: 31.40, Validation Accuracy: 0.96\n",
            "Epoch: 174, Testing Loss: 31.40, Testing Accuracy: 0.96\n",
            "Epoch: 175, Training Loss: 31.31, Training Accuracy: 0.94\n",
            "Epoch: 175, Validation Loss: 31.31, Validation Accuracy: 0.96\n",
            "Epoch: 175, Testing Loss: 31.31, Testing Accuracy: 0.96\n",
            "Epoch: 176, Training Loss: 31.22, Training Accuracy: 0.94\n",
            "Epoch: 176, Validation Loss: 31.22, Validation Accuracy: 0.96\n",
            "Epoch: 176, Testing Loss: 31.22, Testing Accuracy: 0.96\n",
            "Epoch: 177, Training Loss: 31.13, Training Accuracy: 0.94\n",
            "Epoch: 177, Validation Loss: 31.13, Validation Accuracy: 0.96\n",
            "Epoch: 177, Testing Loss: 31.13, Testing Accuracy: 0.96\n",
            "Epoch: 178, Training Loss: 31.05, Training Accuracy: 0.94\n",
            "Epoch: 178, Validation Loss: 31.05, Validation Accuracy: 0.96\n",
            "Epoch: 178, Testing Loss: 31.05, Testing Accuracy: 0.96\n",
            "Epoch: 179, Training Loss: 30.96, Training Accuracy: 0.94\n",
            "Epoch: 179, Validation Loss: 30.96, Validation Accuracy: 0.96\n",
            "Epoch: 179, Testing Loss: 30.96, Testing Accuracy: 0.96\n",
            "Epoch: 180, Training Loss: 30.87, Training Accuracy: 0.94\n",
            "Epoch: 180, Validation Loss: 30.87, Validation Accuracy: 0.96\n",
            "Epoch: 180, Testing Loss: 30.87, Testing Accuracy: 0.96\n",
            "Epoch: 181, Training Loss: 30.78, Training Accuracy: 0.94\n",
            "Epoch: 181, Validation Loss: 30.78, Validation Accuracy: 0.96\n",
            "Epoch: 181, Testing Loss: 30.78, Testing Accuracy: 0.96\n",
            "Epoch: 182, Training Loss: 30.70, Training Accuracy: 0.94\n",
            "Epoch: 182, Validation Loss: 30.70, Validation Accuracy: 0.96\n",
            "Epoch: 182, Testing Loss: 30.70, Testing Accuracy: 0.96\n",
            "Epoch: 183, Training Loss: 30.61, Training Accuracy: 0.95\n",
            "Epoch: 183, Validation Loss: 30.61, Validation Accuracy: 0.96\n",
            "Epoch: 183, Testing Loss: 30.61, Testing Accuracy: 0.96\n",
            "Epoch: 184, Training Loss: 30.53, Training Accuracy: 0.95\n",
            "Epoch: 184, Validation Loss: 30.53, Validation Accuracy: 0.96\n",
            "Epoch: 184, Testing Loss: 30.53, Testing Accuracy: 0.96\n",
            "Epoch: 185, Training Loss: 30.45, Training Accuracy: 0.95\n",
            "Epoch: 185, Validation Loss: 30.45, Validation Accuracy: 0.96\n",
            "Epoch: 185, Testing Loss: 30.45, Testing Accuracy: 0.96\n",
            "Epoch: 186, Training Loss: 30.36, Training Accuracy: 0.95\n",
            "Epoch: 186, Validation Loss: 30.36, Validation Accuracy: 0.96\n",
            "Epoch: 186, Testing Loss: 30.36, Testing Accuracy: 0.96\n",
            "Epoch: 187, Training Loss: 30.28, Training Accuracy: 0.95\n",
            "Epoch: 187, Validation Loss: 30.28, Validation Accuracy: 0.96\n",
            "Epoch: 187, Testing Loss: 30.28, Testing Accuracy: 0.96\n",
            "Epoch: 188, Training Loss: 30.20, Training Accuracy: 0.95\n",
            "Epoch: 188, Validation Loss: 30.20, Validation Accuracy: 0.96\n",
            "Epoch: 188, Testing Loss: 30.20, Testing Accuracy: 0.96\n",
            "Epoch: 189, Training Loss: 30.12, Training Accuracy: 0.95\n",
            "Epoch: 189, Validation Loss: 30.12, Validation Accuracy: 0.96\n",
            "Epoch: 189, Testing Loss: 30.12, Testing Accuracy: 0.96\n",
            "Epoch: 190, Training Loss: 30.04, Training Accuracy: 0.95\n",
            "Epoch: 190, Validation Loss: 30.04, Validation Accuracy: 0.96\n",
            "Epoch: 190, Testing Loss: 30.04, Testing Accuracy: 0.96\n",
            "Epoch: 191, Training Loss: 29.96, Training Accuracy: 0.95\n",
            "Epoch: 191, Validation Loss: 29.96, Validation Accuracy: 0.96\n",
            "Epoch: 191, Testing Loss: 29.96, Testing Accuracy: 0.96\n",
            "Epoch: 192, Training Loss: 29.88, Training Accuracy: 0.95\n",
            "Epoch: 192, Validation Loss: 29.88, Validation Accuracy: 0.96\n",
            "Epoch: 192, Testing Loss: 29.88, Testing Accuracy: 0.96\n",
            "Epoch: 193, Training Loss: 29.80, Training Accuracy: 0.95\n",
            "Epoch: 193, Validation Loss: 29.80, Validation Accuracy: 0.96\n",
            "Epoch: 193, Testing Loss: 29.80, Testing Accuracy: 0.96\n",
            "Epoch: 194, Training Loss: 29.73, Training Accuracy: 0.95\n",
            "Epoch: 194, Validation Loss: 29.73, Validation Accuracy: 0.96\n",
            "Epoch: 194, Testing Loss: 29.73, Testing Accuracy: 0.96\n",
            "Epoch: 195, Training Loss: 29.65, Training Accuracy: 0.95\n",
            "Epoch: 195, Validation Loss: 29.65, Validation Accuracy: 0.96\n",
            "Epoch: 195, Testing Loss: 29.65, Testing Accuracy: 0.96\n",
            "Epoch: 196, Training Loss: 29.57, Training Accuracy: 0.95\n",
            "Epoch: 196, Validation Loss: 29.57, Validation Accuracy: 0.96\n",
            "Epoch: 196, Testing Loss: 29.57, Testing Accuracy: 0.96\n",
            "Epoch: 197, Training Loss: 29.50, Training Accuracy: 0.95\n",
            "Epoch: 197, Validation Loss: 29.50, Validation Accuracy: 0.96\n",
            "Epoch: 197, Testing Loss: 29.50, Testing Accuracy: 0.96\n",
            "Epoch: 198, Training Loss: 29.42, Training Accuracy: 0.95\n",
            "Epoch: 198, Validation Loss: 29.42, Validation Accuracy: 0.96\n",
            "Epoch: 198, Testing Loss: 29.42, Testing Accuracy: 0.96\n",
            "Epoch: 199, Training Loss: 29.35, Training Accuracy: 0.95\n",
            "Epoch: 199, Validation Loss: 29.35, Validation Accuracy: 0.96\n",
            "Epoch: 199, Testing Loss: 29.35, Testing Accuracy: 0.96\n",
            "Epoch: 200, Training Loss: 29.27, Training Accuracy: 0.95\n",
            "Epoch: 200, Validation Loss: 29.27, Validation Accuracy: 0.96\n",
            "Epoch: 200, Testing Loss: 29.27, Testing Accuracy: 0.96\n",
            "Epoch: 201, Training Loss: 29.20, Training Accuracy: 0.95\n",
            "Epoch: 201, Validation Loss: 29.20, Validation Accuracy: 0.96\n",
            "Epoch: 201, Testing Loss: 29.20, Testing Accuracy: 0.96\n",
            "Epoch: 202, Training Loss: 29.13, Training Accuracy: 0.95\n",
            "Epoch: 202, Validation Loss: 29.13, Validation Accuracy: 0.96\n",
            "Epoch: 202, Testing Loss: 29.13, Testing Accuracy: 0.96\n",
            "Epoch: 203, Training Loss: 29.05, Training Accuracy: 0.95\n",
            "Epoch: 203, Validation Loss: 29.05, Validation Accuracy: 0.96\n",
            "Epoch: 203, Testing Loss: 29.05, Testing Accuracy: 0.96\n",
            "Epoch: 204, Training Loss: 28.98, Training Accuracy: 0.95\n",
            "Epoch: 204, Validation Loss: 28.98, Validation Accuracy: 0.96\n",
            "Epoch: 204, Testing Loss: 28.98, Testing Accuracy: 0.96\n",
            "Epoch: 205, Training Loss: 28.91, Training Accuracy: 0.95\n",
            "Epoch: 205, Validation Loss: 28.91, Validation Accuracy: 0.96\n",
            "Epoch: 205, Testing Loss: 28.91, Testing Accuracy: 0.96\n",
            "Epoch: 206, Training Loss: 28.84, Training Accuracy: 0.95\n",
            "Epoch: 206, Validation Loss: 28.84, Validation Accuracy: 0.96\n",
            "Epoch: 206, Testing Loss: 28.84, Testing Accuracy: 0.96\n",
            "Epoch: 207, Training Loss: 28.77, Training Accuracy: 0.95\n",
            "Epoch: 207, Validation Loss: 28.77, Validation Accuracy: 0.96\n",
            "Epoch: 207, Testing Loss: 28.77, Testing Accuracy: 0.96\n",
            "Epoch: 208, Training Loss: 28.70, Training Accuracy: 0.95\n",
            "Epoch: 208, Validation Loss: 28.70, Validation Accuracy: 0.96\n",
            "Epoch: 208, Testing Loss: 28.70, Testing Accuracy: 0.96\n",
            "Epoch: 209, Training Loss: 28.63, Training Accuracy: 0.95\n",
            "Epoch: 209, Validation Loss: 28.63, Validation Accuracy: 0.96\n",
            "Epoch: 209, Testing Loss: 28.63, Testing Accuracy: 0.96\n",
            "Epoch: 210, Training Loss: 28.56, Training Accuracy: 0.95\n",
            "Epoch: 210, Validation Loss: 28.56, Validation Accuracy: 0.96\n",
            "Epoch: 210, Testing Loss: 28.56, Testing Accuracy: 0.96\n",
            "Epoch: 211, Training Loss: 28.49, Training Accuracy: 0.95\n",
            "Epoch: 211, Validation Loss: 28.49, Validation Accuracy: 0.96\n",
            "Epoch: 211, Testing Loss: 28.49, Testing Accuracy: 0.96\n",
            "Epoch: 212, Training Loss: 28.43, Training Accuracy: 0.95\n",
            "Epoch: 212, Validation Loss: 28.43, Validation Accuracy: 0.96\n",
            "Epoch: 212, Testing Loss: 28.43, Testing Accuracy: 0.96\n",
            "Epoch: 213, Training Loss: 28.36, Training Accuracy: 0.95\n",
            "Epoch: 213, Validation Loss: 28.36, Validation Accuracy: 0.96\n",
            "Epoch: 213, Testing Loss: 28.36, Testing Accuracy: 0.96\n",
            "Epoch: 214, Training Loss: 28.29, Training Accuracy: 0.95\n",
            "Epoch: 214, Validation Loss: 28.29, Validation Accuracy: 0.96\n",
            "Epoch: 214, Testing Loss: 28.29, Testing Accuracy: 0.96\n",
            "Epoch: 215, Training Loss: 28.23, Training Accuracy: 0.95\n",
            "Epoch: 215, Validation Loss: 28.23, Validation Accuracy: 0.96\n",
            "Epoch: 215, Testing Loss: 28.23, Testing Accuracy: 0.96\n",
            "Epoch: 216, Training Loss: 28.16, Training Accuracy: 0.95\n",
            "Epoch: 216, Validation Loss: 28.16, Validation Accuracy: 0.96\n",
            "Epoch: 216, Testing Loss: 28.16, Testing Accuracy: 0.96\n",
            "Epoch: 217, Training Loss: 28.09, Training Accuracy: 0.95\n",
            "Epoch: 217, Validation Loss: 28.09, Validation Accuracy: 0.96\n",
            "Epoch: 217, Testing Loss: 28.09, Testing Accuracy: 0.96\n",
            "Epoch: 218, Training Loss: 28.03, Training Accuracy: 0.95\n",
            "Epoch: 218, Validation Loss: 28.03, Validation Accuracy: 0.96\n",
            "Epoch: 218, Testing Loss: 28.03, Testing Accuracy: 0.96\n",
            "Epoch: 219, Training Loss: 27.96, Training Accuracy: 0.95\n",
            "Epoch: 219, Validation Loss: 27.96, Validation Accuracy: 0.96\n",
            "Epoch: 219, Testing Loss: 27.96, Testing Accuracy: 0.96\n",
            "Epoch: 220, Training Loss: 27.90, Training Accuracy: 0.95\n",
            "Epoch: 220, Validation Loss: 27.90, Validation Accuracy: 0.96\n",
            "Epoch: 220, Testing Loss: 27.90, Testing Accuracy: 0.96\n",
            "Epoch: 221, Training Loss: 27.84, Training Accuracy: 0.95\n",
            "Epoch: 221, Validation Loss: 27.84, Validation Accuracy: 0.96\n",
            "Epoch: 221, Testing Loss: 27.84, Testing Accuracy: 0.96\n",
            "Epoch: 222, Training Loss: 27.77, Training Accuracy: 0.95\n",
            "Epoch: 222, Validation Loss: 27.77, Validation Accuracy: 0.96\n",
            "Epoch: 222, Testing Loss: 27.77, Testing Accuracy: 0.96\n",
            "Epoch: 223, Training Loss: 27.71, Training Accuracy: 0.95\n",
            "Epoch: 223, Validation Loss: 27.71, Validation Accuracy: 0.96\n",
            "Epoch: 223, Testing Loss: 27.71, Testing Accuracy: 0.96\n",
            "Epoch: 224, Training Loss: 27.65, Training Accuracy: 0.95\n",
            "Epoch: 224, Validation Loss: 27.65, Validation Accuracy: 0.96\n",
            "Epoch: 224, Testing Loss: 27.65, Testing Accuracy: 0.96\n",
            "Epoch: 225, Training Loss: 27.59, Training Accuracy: 0.95\n",
            "Epoch: 225, Validation Loss: 27.59, Validation Accuracy: 0.96\n",
            "Epoch: 225, Testing Loss: 27.59, Testing Accuracy: 0.96\n",
            "Epoch: 226, Training Loss: 27.53, Training Accuracy: 0.95\n",
            "Epoch: 226, Validation Loss: 27.53, Validation Accuracy: 0.96\n",
            "Epoch: 226, Testing Loss: 27.53, Testing Accuracy: 0.96\n",
            "Epoch: 227, Training Loss: 27.47, Training Accuracy: 0.95\n",
            "Epoch: 227, Validation Loss: 27.47, Validation Accuracy: 0.96\n",
            "Epoch: 227, Testing Loss: 27.47, Testing Accuracy: 0.96\n",
            "Epoch: 228, Training Loss: 27.40, Training Accuracy: 0.95\n",
            "Epoch: 228, Validation Loss: 27.40, Validation Accuracy: 0.96\n",
            "Epoch: 228, Testing Loss: 27.40, Testing Accuracy: 0.96\n",
            "Epoch: 229, Training Loss: 27.34, Training Accuracy: 0.95\n",
            "Epoch: 229, Validation Loss: 27.34, Validation Accuracy: 0.96\n",
            "Epoch: 229, Testing Loss: 27.34, Testing Accuracy: 0.96\n",
            "Epoch: 230, Training Loss: 27.28, Training Accuracy: 0.95\n",
            "Epoch: 230, Validation Loss: 27.28, Validation Accuracy: 0.96\n",
            "Epoch: 230, Testing Loss: 27.28, Testing Accuracy: 0.96\n",
            "Epoch: 231, Training Loss: 27.23, Training Accuracy: 0.95\n",
            "Epoch: 231, Validation Loss: 27.23, Validation Accuracy: 0.96\n",
            "Epoch: 231, Testing Loss: 27.23, Testing Accuracy: 0.96\n",
            "Epoch: 232, Training Loss: 27.17, Training Accuracy: 0.95\n",
            "Epoch: 232, Validation Loss: 27.17, Validation Accuracy: 0.96\n",
            "Epoch: 232, Testing Loss: 27.17, Testing Accuracy: 0.96\n",
            "Epoch: 233, Training Loss: 27.11, Training Accuracy: 0.95\n",
            "Epoch: 233, Validation Loss: 27.11, Validation Accuracy: 0.96\n",
            "Epoch: 233, Testing Loss: 27.11, Testing Accuracy: 0.96\n",
            "Epoch: 234, Training Loss: 27.05, Training Accuracy: 0.95\n",
            "Epoch: 234, Validation Loss: 27.05, Validation Accuracy: 0.96\n",
            "Epoch: 234, Testing Loss: 27.05, Testing Accuracy: 0.96\n",
            "Epoch: 235, Training Loss: 26.99, Training Accuracy: 0.95\n",
            "Epoch: 235, Validation Loss: 26.99, Validation Accuracy: 0.96\n",
            "Epoch: 235, Testing Loss: 26.99, Testing Accuracy: 0.96\n",
            "Epoch: 236, Training Loss: 26.94, Training Accuracy: 0.95\n",
            "Epoch: 236, Validation Loss: 26.94, Validation Accuracy: 0.96\n",
            "Epoch: 236, Testing Loss: 26.94, Testing Accuracy: 0.96\n",
            "Epoch: 237, Training Loss: 26.88, Training Accuracy: 0.95\n",
            "Epoch: 237, Validation Loss: 26.88, Validation Accuracy: 0.96\n",
            "Epoch: 237, Testing Loss: 26.88, Testing Accuracy: 0.96\n",
            "Epoch: 238, Training Loss: 26.82, Training Accuracy: 0.95\n",
            "Epoch: 238, Validation Loss: 26.82, Validation Accuracy: 0.96\n",
            "Epoch: 238, Testing Loss: 26.82, Testing Accuracy: 0.96\n",
            "Epoch: 239, Training Loss: 26.77, Training Accuracy: 0.95\n",
            "Epoch: 239, Validation Loss: 26.77, Validation Accuracy: 0.96\n",
            "Epoch: 239, Testing Loss: 26.77, Testing Accuracy: 0.96\n",
            "Epoch: 240, Training Loss: 26.71, Training Accuracy: 0.95\n",
            "Epoch: 240, Validation Loss: 26.71, Validation Accuracy: 0.96\n",
            "Epoch: 240, Testing Loss: 26.71, Testing Accuracy: 0.96\n",
            "Epoch: 241, Training Loss: 26.65, Training Accuracy: 0.95\n",
            "Epoch: 241, Validation Loss: 26.65, Validation Accuracy: 0.96\n",
            "Epoch: 241, Testing Loss: 26.65, Testing Accuracy: 0.96\n",
            "Epoch: 242, Training Loss: 26.60, Training Accuracy: 0.95\n",
            "Epoch: 242, Validation Loss: 26.60, Validation Accuracy: 0.96\n",
            "Epoch: 242, Testing Loss: 26.60, Testing Accuracy: 0.96\n",
            "Epoch: 243, Training Loss: 26.54, Training Accuracy: 0.95\n",
            "Epoch: 243, Validation Loss: 26.54, Validation Accuracy: 0.96\n",
            "Epoch: 243, Testing Loss: 26.54, Testing Accuracy: 0.96\n",
            "Epoch: 244, Training Loss: 26.49, Training Accuracy: 0.95\n",
            "Epoch: 244, Validation Loss: 26.49, Validation Accuracy: 0.96\n",
            "Epoch: 244, Testing Loss: 26.49, Testing Accuracy: 0.96\n",
            "Epoch: 245, Training Loss: 26.44, Training Accuracy: 0.95\n",
            "Epoch: 245, Validation Loss: 26.44, Validation Accuracy: 0.96\n",
            "Epoch: 245, Testing Loss: 26.44, Testing Accuracy: 0.96\n",
            "Epoch: 246, Training Loss: 26.38, Training Accuracy: 0.95\n",
            "Epoch: 246, Validation Loss: 26.38, Validation Accuracy: 0.96\n",
            "Epoch: 246, Testing Loss: 26.38, Testing Accuracy: 0.96\n",
            "Epoch: 247, Training Loss: 26.33, Training Accuracy: 0.95\n",
            "Epoch: 247, Validation Loss: 26.33, Validation Accuracy: 0.96\n",
            "Epoch: 247, Testing Loss: 26.33, Testing Accuracy: 0.96\n",
            "Epoch: 248, Training Loss: 26.28, Training Accuracy: 0.95\n",
            "Epoch: 248, Validation Loss: 26.28, Validation Accuracy: 0.96\n",
            "Epoch: 248, Testing Loss: 26.28, Testing Accuracy: 0.96\n",
            "Epoch: 249, Training Loss: 26.22, Training Accuracy: 0.95\n",
            "Epoch: 249, Validation Loss: 26.22, Validation Accuracy: 0.96\n",
            "Epoch: 249, Testing Loss: 26.22, Testing Accuracy: 0.96\n",
            "Epoch: 250, Training Loss: 26.17, Training Accuracy: 0.95\n",
            "Epoch: 250, Validation Loss: 26.17, Validation Accuracy: 0.96\n",
            "Epoch: 250, Testing Loss: 26.17, Testing Accuracy: 0.95\n",
            "Epoch: 251, Training Loss: 26.12, Training Accuracy: 0.95\n",
            "Epoch: 251, Validation Loss: 26.12, Validation Accuracy: 0.96\n",
            "Epoch: 251, Testing Loss: 26.12, Testing Accuracy: 0.95\n",
            "Epoch: 252, Training Loss: 26.07, Training Accuracy: 0.95\n",
            "Epoch: 252, Validation Loss: 26.07, Validation Accuracy: 0.96\n",
            "Epoch: 252, Testing Loss: 26.07, Testing Accuracy: 0.95\n",
            "Epoch: 253, Training Loss: 26.02, Training Accuracy: 0.95\n",
            "Epoch: 253, Validation Loss: 26.02, Validation Accuracy: 0.96\n",
            "Epoch: 253, Testing Loss: 26.02, Testing Accuracy: 0.95\n",
            "Epoch: 254, Training Loss: 25.96, Training Accuracy: 0.95\n",
            "Epoch: 254, Validation Loss: 25.96, Validation Accuracy: 0.96\n",
            "Epoch: 254, Testing Loss: 25.96, Testing Accuracy: 0.95\n",
            "Epoch: 255, Training Loss: 25.91, Training Accuracy: 0.95\n",
            "Epoch: 255, Validation Loss: 25.91, Validation Accuracy: 0.96\n",
            "Epoch: 255, Testing Loss: 25.91, Testing Accuracy: 0.95\n",
            "Epoch: 256, Training Loss: 25.86, Training Accuracy: 0.95\n",
            "Epoch: 256, Validation Loss: 25.86, Validation Accuracy: 0.96\n",
            "Epoch: 256, Testing Loss: 25.86, Testing Accuracy: 0.95\n",
            "Epoch: 257, Training Loss: 25.81, Training Accuracy: 0.95\n",
            "Epoch: 257, Validation Loss: 25.81, Validation Accuracy: 0.96\n",
            "Epoch: 257, Testing Loss: 25.81, Testing Accuracy: 0.95\n",
            "Epoch: 258, Training Loss: 25.76, Training Accuracy: 0.95\n",
            "Epoch: 258, Validation Loss: 25.76, Validation Accuracy: 0.96\n",
            "Epoch: 258, Testing Loss: 25.76, Testing Accuracy: 0.95\n",
            "Epoch: 259, Training Loss: 25.71, Training Accuracy: 0.95\n",
            "Epoch: 259, Validation Loss: 25.71, Validation Accuracy: 0.96\n",
            "Epoch: 259, Testing Loss: 25.71, Testing Accuracy: 0.95\n",
            "Epoch: 260, Training Loss: 25.67, Training Accuracy: 0.95\n",
            "Epoch: 260, Validation Loss: 25.67, Validation Accuracy: 0.96\n",
            "Epoch: 260, Testing Loss: 25.67, Testing Accuracy: 0.95\n",
            "Epoch: 261, Training Loss: 25.62, Training Accuracy: 0.95\n",
            "Epoch: 261, Validation Loss: 25.62, Validation Accuracy: 0.96\n",
            "Epoch: 261, Testing Loss: 25.62, Testing Accuracy: 0.95\n",
            "Epoch: 262, Training Loss: 25.57, Training Accuracy: 0.95\n",
            "Epoch: 262, Validation Loss: 25.57, Validation Accuracy: 0.96\n",
            "Epoch: 262, Testing Loss: 25.57, Testing Accuracy: 0.95\n",
            "Epoch: 263, Training Loss: 25.52, Training Accuracy: 0.95\n",
            "Epoch: 263, Validation Loss: 25.52, Validation Accuracy: 0.96\n",
            "Epoch: 263, Testing Loss: 25.52, Testing Accuracy: 0.95\n",
            "Epoch: 264, Training Loss: 25.47, Training Accuracy: 0.95\n",
            "Epoch: 264, Validation Loss: 25.47, Validation Accuracy: 0.96\n",
            "Epoch: 264, Testing Loss: 25.47, Testing Accuracy: 0.95\n",
            "Epoch: 265, Training Loss: 25.42, Training Accuracy: 0.95\n",
            "Epoch: 265, Validation Loss: 25.42, Validation Accuracy: 0.96\n",
            "Epoch: 265, Testing Loss: 25.42, Testing Accuracy: 0.95\n",
            "Epoch: 266, Training Loss: 25.38, Training Accuracy: 0.95\n",
            "Epoch: 266, Validation Loss: 25.38, Validation Accuracy: 0.96\n",
            "Epoch: 266, Testing Loss: 25.38, Testing Accuracy: 0.95\n",
            "Epoch: 267, Training Loss: 25.33, Training Accuracy: 0.95\n",
            "Epoch: 267, Validation Loss: 25.33, Validation Accuracy: 0.96\n",
            "Epoch: 267, Testing Loss: 25.33, Testing Accuracy: 0.95\n",
            "Epoch: 268, Training Loss: 25.28, Training Accuracy: 0.95\n",
            "Epoch: 268, Validation Loss: 25.28, Validation Accuracy: 0.96\n",
            "Epoch: 268, Testing Loss: 25.28, Testing Accuracy: 0.95\n",
            "Epoch: 269, Training Loss: 25.24, Training Accuracy: 0.95\n",
            "Epoch: 269, Validation Loss: 25.24, Validation Accuracy: 0.96\n",
            "Epoch: 269, Testing Loss: 25.24, Testing Accuracy: 0.95\n",
            "Epoch: 270, Training Loss: 25.19, Training Accuracy: 0.95\n",
            "Epoch: 270, Validation Loss: 25.19, Validation Accuracy: 0.96\n",
            "Epoch: 270, Testing Loss: 25.19, Testing Accuracy: 0.95\n",
            "Epoch: 271, Training Loss: 25.14, Training Accuracy: 0.95\n",
            "Epoch: 271, Validation Loss: 25.14, Validation Accuracy: 0.96\n",
            "Epoch: 271, Testing Loss: 25.14, Testing Accuracy: 0.95\n",
            "Epoch: 272, Training Loss: 25.10, Training Accuracy: 0.95\n",
            "Epoch: 272, Validation Loss: 25.10, Validation Accuracy: 0.96\n",
            "Epoch: 272, Testing Loss: 25.10, Testing Accuracy: 0.95\n",
            "Epoch: 273, Training Loss: 25.05, Training Accuracy: 0.95\n",
            "Epoch: 273, Validation Loss: 25.05, Validation Accuracy: 0.96\n",
            "Epoch: 273, Testing Loss: 25.05, Testing Accuracy: 0.95\n",
            "Epoch: 274, Training Loss: 25.01, Training Accuracy: 0.95\n",
            "Epoch: 274, Validation Loss: 25.01, Validation Accuracy: 0.96\n",
            "Epoch: 274, Testing Loss: 25.01, Testing Accuracy: 0.95\n",
            "Epoch: 275, Training Loss: 24.96, Training Accuracy: 0.95\n",
            "Epoch: 275, Validation Loss: 24.96, Validation Accuracy: 0.96\n",
            "Epoch: 275, Testing Loss: 24.96, Testing Accuracy: 0.95\n",
            "Epoch: 276, Training Loss: 24.92, Training Accuracy: 0.95\n",
            "Epoch: 276, Validation Loss: 24.92, Validation Accuracy: 0.96\n",
            "Epoch: 276, Testing Loss: 24.92, Testing Accuracy: 0.95\n",
            "Epoch: 277, Training Loss: 24.87, Training Accuracy: 0.95\n",
            "Epoch: 277, Validation Loss: 24.87, Validation Accuracy: 0.96\n",
            "Epoch: 277, Testing Loss: 24.87, Testing Accuracy: 0.95\n",
            "Epoch: 278, Training Loss: 24.83, Training Accuracy: 0.95\n",
            "Epoch: 278, Validation Loss: 24.83, Validation Accuracy: 0.96\n",
            "Epoch: 278, Testing Loss: 24.83, Testing Accuracy: 0.95\n",
            "Epoch: 279, Training Loss: 24.79, Training Accuracy: 0.95\n",
            "Epoch: 279, Validation Loss: 24.79, Validation Accuracy: 0.96\n",
            "Epoch: 279, Testing Loss: 24.79, Testing Accuracy: 0.95\n",
            "Epoch: 280, Training Loss: 24.74, Training Accuracy: 0.95\n",
            "Epoch: 280, Validation Loss: 24.74, Validation Accuracy: 0.96\n",
            "Epoch: 280, Testing Loss: 24.74, Testing Accuracy: 0.95\n",
            "Epoch: 281, Training Loss: 24.70, Training Accuracy: 0.95\n",
            "Epoch: 281, Validation Loss: 24.70, Validation Accuracy: 0.96\n",
            "Epoch: 281, Testing Loss: 24.70, Testing Accuracy: 0.95\n",
            "Epoch: 282, Training Loss: 24.65, Training Accuracy: 0.95\n",
            "Epoch: 282, Validation Loss: 24.65, Validation Accuracy: 0.96\n",
            "Epoch: 282, Testing Loss: 24.65, Testing Accuracy: 0.95\n",
            "Epoch: 283, Training Loss: 24.61, Training Accuracy: 0.95\n",
            "Epoch: 283, Validation Loss: 24.61, Validation Accuracy: 0.96\n",
            "Epoch: 283, Testing Loss: 24.61, Testing Accuracy: 0.95\n",
            "Epoch: 284, Training Loss: 24.57, Training Accuracy: 0.95\n",
            "Epoch: 284, Validation Loss: 24.57, Validation Accuracy: 0.96\n",
            "Epoch: 284, Testing Loss: 24.57, Testing Accuracy: 0.95\n",
            "Epoch: 285, Training Loss: 24.53, Training Accuracy: 0.95\n",
            "Epoch: 285, Validation Loss: 24.53, Validation Accuracy: 0.96\n",
            "Epoch: 285, Testing Loss: 24.53, Testing Accuracy: 0.95\n",
            "Epoch: 286, Training Loss: 24.48, Training Accuracy: 0.95\n",
            "Epoch: 286, Validation Loss: 24.48, Validation Accuracy: 0.96\n",
            "Epoch: 286, Testing Loss: 24.48, Testing Accuracy: 0.95\n",
            "Epoch: 287, Training Loss: 24.44, Training Accuracy: 0.95\n",
            "Epoch: 287, Validation Loss: 24.44, Validation Accuracy: 0.96\n",
            "Epoch: 287, Testing Loss: 24.44, Testing Accuracy: 0.95\n",
            "Epoch: 288, Training Loss: 24.40, Training Accuracy: 0.95\n",
            "Epoch: 288, Validation Loss: 24.40, Validation Accuracy: 0.96\n",
            "Epoch: 288, Testing Loss: 24.40, Testing Accuracy: 0.95\n",
            "Epoch: 289, Training Loss: 24.36, Training Accuracy: 0.95\n",
            "Epoch: 289, Validation Loss: 24.36, Validation Accuracy: 0.96\n",
            "Epoch: 289, Testing Loss: 24.36, Testing Accuracy: 0.95\n",
            "Epoch: 290, Training Loss: 24.32, Training Accuracy: 0.95\n",
            "Epoch: 290, Validation Loss: 24.32, Validation Accuracy: 0.96\n",
            "Epoch: 290, Testing Loss: 24.32, Testing Accuracy: 0.95\n",
            "Epoch: 291, Training Loss: 24.28, Training Accuracy: 0.95\n",
            "Epoch: 291, Validation Loss: 24.28, Validation Accuracy: 0.96\n",
            "Epoch: 291, Testing Loss: 24.28, Testing Accuracy: 0.95\n",
            "Epoch: 292, Training Loss: 24.24, Training Accuracy: 0.95\n",
            "Epoch: 292, Validation Loss: 24.24, Validation Accuracy: 0.96\n",
            "Epoch: 292, Testing Loss: 24.24, Testing Accuracy: 0.95\n",
            "Epoch: 293, Training Loss: 24.20, Training Accuracy: 0.95\n",
            "Epoch: 293, Validation Loss: 24.20, Validation Accuracy: 0.96\n",
            "Epoch: 293, Testing Loss: 24.20, Testing Accuracy: 0.95\n",
            "Epoch: 294, Training Loss: 24.16, Training Accuracy: 0.95\n",
            "Epoch: 294, Validation Loss: 24.16, Validation Accuracy: 0.96\n",
            "Epoch: 294, Testing Loss: 24.16, Testing Accuracy: 0.95\n",
            "Epoch: 295, Training Loss: 24.12, Training Accuracy: 0.95\n",
            "Epoch: 295, Validation Loss: 24.12, Validation Accuracy: 0.96\n",
            "Epoch: 295, Testing Loss: 24.12, Testing Accuracy: 0.95\n",
            "Epoch: 296, Training Loss: 24.08, Training Accuracy: 0.95\n",
            "Epoch: 296, Validation Loss: 24.08, Validation Accuracy: 0.96\n",
            "Epoch: 296, Testing Loss: 24.08, Testing Accuracy: 0.95\n",
            "Epoch: 297, Training Loss: 24.04, Training Accuracy: 0.95\n",
            "Epoch: 297, Validation Loss: 24.04, Validation Accuracy: 0.96\n",
            "Epoch: 297, Testing Loss: 24.04, Testing Accuracy: 0.95\n",
            "Epoch: 298, Training Loss: 24.00, Training Accuracy: 0.95\n",
            "Epoch: 298, Validation Loss: 24.00, Validation Accuracy: 0.96\n",
            "Epoch: 298, Testing Loss: 24.00, Testing Accuracy: 0.95\n",
            "Epoch: 299, Training Loss: 23.96, Training Accuracy: 0.95\n",
            "Epoch: 299, Validation Loss: 23.96, Validation Accuracy: 0.96\n",
            "Epoch: 299, Testing Loss: 23.96, Testing Accuracy: 0.95\n",
            "Epoch: 300, Training Loss: 23.92, Training Accuracy: 0.95\n",
            "Epoch: 300, Validation Loss: 23.92, Validation Accuracy: 0.96\n",
            "Epoch: 300, Testing Loss: 23.92, Testing Accuracy: 0.95\n",
            "Epoch: 301, Training Loss: 23.88, Training Accuracy: 0.95\n",
            "Epoch: 301, Validation Loss: 23.88, Validation Accuracy: 0.96\n",
            "Epoch: 301, Testing Loss: 23.88, Testing Accuracy: 0.95\n",
            "Epoch: 302, Training Loss: 23.84, Training Accuracy: 0.95\n",
            "Epoch: 302, Validation Loss: 23.84, Validation Accuracy: 0.96\n",
            "Epoch: 302, Testing Loss: 23.84, Testing Accuracy: 0.95\n",
            "Epoch: 303, Training Loss: 23.80, Training Accuracy: 0.95\n",
            "Epoch: 303, Validation Loss: 23.80, Validation Accuracy: 0.96\n",
            "Epoch: 303, Testing Loss: 23.80, Testing Accuracy: 0.95\n",
            "Epoch: 304, Training Loss: 23.76, Training Accuracy: 0.95\n",
            "Epoch: 304, Validation Loss: 23.76, Validation Accuracy: 0.96\n",
            "Epoch: 304, Testing Loss: 23.76, Testing Accuracy: 0.95\n",
            "Epoch: 305, Training Loss: 23.73, Training Accuracy: 0.95\n",
            "Epoch: 305, Validation Loss: 23.73, Validation Accuracy: 0.96\n",
            "Epoch: 305, Testing Loss: 23.73, Testing Accuracy: 0.95\n",
            "Epoch: 306, Training Loss: 23.69, Training Accuracy: 0.95\n",
            "Epoch: 306, Validation Loss: 23.69, Validation Accuracy: 0.96\n",
            "Epoch: 306, Testing Loss: 23.69, Testing Accuracy: 0.95\n",
            "Epoch: 307, Training Loss: 23.65, Training Accuracy: 0.95\n",
            "Epoch: 307, Validation Loss: 23.65, Validation Accuracy: 0.96\n",
            "Epoch: 307, Testing Loss: 23.65, Testing Accuracy: 0.95\n",
            "Epoch: 308, Training Loss: 23.61, Training Accuracy: 0.95\n",
            "Epoch: 308, Validation Loss: 23.61, Validation Accuracy: 0.96\n",
            "Epoch: 308, Testing Loss: 23.61, Testing Accuracy: 0.95\n",
            "Epoch: 309, Training Loss: 23.58, Training Accuracy: 0.95\n",
            "Epoch: 309, Validation Loss: 23.58, Validation Accuracy: 0.96\n",
            "Epoch: 309, Testing Loss: 23.58, Testing Accuracy: 0.95\n",
            "Epoch: 310, Training Loss: 23.54, Training Accuracy: 0.95\n",
            "Epoch: 310, Validation Loss: 23.54, Validation Accuracy: 0.96\n",
            "Epoch: 310, Testing Loss: 23.54, Testing Accuracy: 0.95\n",
            "Epoch: 311, Training Loss: 23.50, Training Accuracy: 0.95\n",
            "Epoch: 311, Validation Loss: 23.50, Validation Accuracy: 0.96\n",
            "Epoch: 311, Testing Loss: 23.50, Testing Accuracy: 0.95\n",
            "Epoch: 312, Training Loss: 23.47, Training Accuracy: 0.95\n",
            "Epoch: 312, Validation Loss: 23.47, Validation Accuracy: 0.96\n",
            "Epoch: 312, Testing Loss: 23.47, Testing Accuracy: 0.95\n",
            "Epoch: 313, Training Loss: 23.43, Training Accuracy: 0.95\n",
            "Epoch: 313, Validation Loss: 23.43, Validation Accuracy: 0.96\n",
            "Epoch: 313, Testing Loss: 23.43, Testing Accuracy: 0.95\n",
            "Epoch: 314, Training Loss: 23.39, Training Accuracy: 0.95\n",
            "Epoch: 314, Validation Loss: 23.39, Validation Accuracy: 0.96\n",
            "Epoch: 314, Testing Loss: 23.39, Testing Accuracy: 0.95\n",
            "Epoch: 315, Training Loss: 23.36, Training Accuracy: 0.95\n",
            "Epoch: 315, Validation Loss: 23.36, Validation Accuracy: 0.96\n",
            "Epoch: 315, Testing Loss: 23.36, Testing Accuracy: 0.95\n",
            "Epoch: 316, Training Loss: 23.32, Training Accuracy: 0.95\n",
            "Epoch: 316, Validation Loss: 23.32, Validation Accuracy: 0.96\n",
            "Epoch: 316, Testing Loss: 23.32, Testing Accuracy: 0.95\n",
            "Epoch: 317, Training Loss: 23.29, Training Accuracy: 0.95\n",
            "Epoch: 317, Validation Loss: 23.29, Validation Accuracy: 0.96\n",
            "Epoch: 317, Testing Loss: 23.29, Testing Accuracy: 0.95\n",
            "Epoch: 318, Training Loss: 23.25, Training Accuracy: 0.95\n",
            "Epoch: 318, Validation Loss: 23.25, Validation Accuracy: 0.96\n",
            "Epoch: 318, Testing Loss: 23.25, Testing Accuracy: 0.95\n",
            "Epoch: 319, Training Loss: 23.22, Training Accuracy: 0.95\n",
            "Epoch: 319, Validation Loss: 23.22, Validation Accuracy: 0.96\n",
            "Epoch: 319, Testing Loss: 23.22, Testing Accuracy: 0.95\n",
            "Epoch: 320, Training Loss: 23.18, Training Accuracy: 0.95\n",
            "Epoch: 320, Validation Loss: 23.18, Validation Accuracy: 0.96\n",
            "Epoch: 320, Testing Loss: 23.18, Testing Accuracy: 0.95\n",
            "Epoch: 321, Training Loss: 23.15, Training Accuracy: 0.95\n",
            "Epoch: 321, Validation Loss: 23.15, Validation Accuracy: 0.96\n",
            "Epoch: 321, Testing Loss: 23.15, Testing Accuracy: 0.95\n",
            "Epoch: 322, Training Loss: 23.11, Training Accuracy: 0.95\n",
            "Epoch: 322, Validation Loss: 23.11, Validation Accuracy: 0.96\n",
            "Epoch: 322, Testing Loss: 23.11, Testing Accuracy: 0.95\n",
            "Epoch: 323, Training Loss: 23.08, Training Accuracy: 0.95\n",
            "Epoch: 323, Validation Loss: 23.08, Validation Accuracy: 0.96\n",
            "Epoch: 323, Testing Loss: 23.08, Testing Accuracy: 0.95\n",
            "Epoch: 324, Training Loss: 23.04, Training Accuracy: 0.95\n",
            "Epoch: 324, Validation Loss: 23.04, Validation Accuracy: 0.96\n",
            "Epoch: 324, Testing Loss: 23.04, Testing Accuracy: 0.95\n",
            "Epoch: 325, Training Loss: 23.01, Training Accuracy: 0.95\n",
            "Epoch: 325, Validation Loss: 23.01, Validation Accuracy: 0.96\n",
            "Epoch: 325, Testing Loss: 23.01, Testing Accuracy: 0.95\n",
            "Epoch: 326, Training Loss: 22.97, Training Accuracy: 0.95\n",
            "Epoch: 326, Validation Loss: 22.97, Validation Accuracy: 0.96\n",
            "Epoch: 326, Testing Loss: 22.97, Testing Accuracy: 0.95\n",
            "Epoch: 327, Training Loss: 22.94, Training Accuracy: 0.95\n",
            "Epoch: 327, Validation Loss: 22.94, Validation Accuracy: 0.96\n",
            "Epoch: 327, Testing Loss: 22.94, Testing Accuracy: 0.95\n",
            "Epoch: 328, Training Loss: 22.91, Training Accuracy: 0.95\n",
            "Epoch: 328, Validation Loss: 22.91, Validation Accuracy: 0.96\n",
            "Epoch: 328, Testing Loss: 22.91, Testing Accuracy: 0.95\n",
            "Epoch: 329, Training Loss: 22.87, Training Accuracy: 0.95\n",
            "Epoch: 329, Validation Loss: 22.87, Validation Accuracy: 0.96\n",
            "Epoch: 329, Testing Loss: 22.87, Testing Accuracy: 0.95\n",
            "Epoch: 330, Training Loss: 22.84, Training Accuracy: 0.95\n",
            "Epoch: 330, Validation Loss: 22.84, Validation Accuracy: 0.96\n",
            "Epoch: 330, Testing Loss: 22.84, Testing Accuracy: 0.95\n",
            "Epoch: 331, Training Loss: 22.81, Training Accuracy: 0.95\n",
            "Epoch: 331, Validation Loss: 22.81, Validation Accuracy: 0.96\n",
            "Epoch: 331, Testing Loss: 22.81, Testing Accuracy: 0.95\n",
            "Epoch: 332, Training Loss: 22.77, Training Accuracy: 0.95\n",
            "Epoch: 332, Validation Loss: 22.77, Validation Accuracy: 0.96\n",
            "Epoch: 332, Testing Loss: 22.77, Testing Accuracy: 0.95\n",
            "Epoch: 333, Training Loss: 22.74, Training Accuracy: 0.95\n",
            "Epoch: 333, Validation Loss: 22.74, Validation Accuracy: 0.96\n",
            "Epoch: 333, Testing Loss: 22.74, Testing Accuracy: 0.95\n",
            "Epoch: 334, Training Loss: 22.71, Training Accuracy: 0.95\n",
            "Epoch: 334, Validation Loss: 22.71, Validation Accuracy: 0.96\n",
            "Epoch: 334, Testing Loss: 22.71, Testing Accuracy: 0.95\n",
            "Epoch: 335, Training Loss: 22.67, Training Accuracy: 0.95\n",
            "Epoch: 335, Validation Loss: 22.67, Validation Accuracy: 0.96\n",
            "Epoch: 335, Testing Loss: 22.67, Testing Accuracy: 0.95\n",
            "Epoch: 336, Training Loss: 22.64, Training Accuracy: 0.95\n",
            "Epoch: 336, Validation Loss: 22.64, Validation Accuracy: 0.96\n",
            "Epoch: 336, Testing Loss: 22.64, Testing Accuracy: 0.95\n",
            "Epoch: 337, Training Loss: 22.61, Training Accuracy: 0.95\n",
            "Epoch: 337, Validation Loss: 22.61, Validation Accuracy: 0.96\n",
            "Epoch: 337, Testing Loss: 22.61, Testing Accuracy: 0.95\n",
            "Epoch: 338, Training Loss: 22.58, Training Accuracy: 0.95\n",
            "Epoch: 338, Validation Loss: 22.58, Validation Accuracy: 0.96\n",
            "Epoch: 338, Testing Loss: 22.58, Testing Accuracy: 0.95\n",
            "Epoch: 339, Training Loss: 22.55, Training Accuracy: 0.95\n",
            "Epoch: 339, Validation Loss: 22.55, Validation Accuracy: 0.96\n",
            "Epoch: 339, Testing Loss: 22.55, Testing Accuracy: 0.95\n",
            "Epoch: 340, Training Loss: 22.51, Training Accuracy: 0.95\n",
            "Epoch: 340, Validation Loss: 22.51, Validation Accuracy: 0.96\n",
            "Epoch: 340, Testing Loss: 22.51, Testing Accuracy: 0.95\n",
            "Epoch: 341, Training Loss: 22.48, Training Accuracy: 0.95\n",
            "Epoch: 341, Validation Loss: 22.48, Validation Accuracy: 0.96\n",
            "Epoch: 341, Testing Loss: 22.48, Testing Accuracy: 0.95\n",
            "Epoch: 342, Training Loss: 22.45, Training Accuracy: 0.95\n",
            "Epoch: 342, Validation Loss: 22.45, Validation Accuracy: 0.96\n",
            "Epoch: 342, Testing Loss: 22.45, Testing Accuracy: 0.95\n",
            "Epoch: 343, Training Loss: 22.42, Training Accuracy: 0.95\n",
            "Epoch: 343, Validation Loss: 22.42, Validation Accuracy: 0.96\n",
            "Epoch: 343, Testing Loss: 22.42, Testing Accuracy: 0.95\n",
            "Epoch: 344, Training Loss: 22.39, Training Accuracy: 0.95\n",
            "Epoch: 344, Validation Loss: 22.39, Validation Accuracy: 0.96\n",
            "Epoch: 344, Testing Loss: 22.39, Testing Accuracy: 0.95\n",
            "Epoch: 345, Training Loss: 22.36, Training Accuracy: 0.95\n",
            "Epoch: 345, Validation Loss: 22.36, Validation Accuracy: 0.96\n",
            "Epoch: 345, Testing Loss: 22.36, Testing Accuracy: 0.95\n",
            "Epoch: 346, Training Loss: 22.33, Training Accuracy: 0.95\n",
            "Epoch: 346, Validation Loss: 22.33, Validation Accuracy: 0.96\n",
            "Epoch: 346, Testing Loss: 22.33, Testing Accuracy: 0.95\n",
            "Epoch: 347, Training Loss: 22.30, Training Accuracy: 0.95\n",
            "Epoch: 347, Validation Loss: 22.30, Validation Accuracy: 0.96\n",
            "Epoch: 347, Testing Loss: 22.30, Testing Accuracy: 0.95\n",
            "Epoch: 348, Training Loss: 22.26, Training Accuracy: 0.95\n",
            "Epoch: 348, Validation Loss: 22.26, Validation Accuracy: 0.96\n",
            "Epoch: 348, Testing Loss: 22.26, Testing Accuracy: 0.95\n",
            "Epoch: 349, Training Loss: 22.23, Training Accuracy: 0.95\n",
            "Epoch: 349, Validation Loss: 22.23, Validation Accuracy: 0.96\n",
            "Epoch: 349, Testing Loss: 22.23, Testing Accuracy: 0.95\n",
            "Epoch: 350, Training Loss: 22.20, Training Accuracy: 0.95\n",
            "Epoch: 350, Validation Loss: 22.20, Validation Accuracy: 0.96\n",
            "Epoch: 350, Testing Loss: 22.20, Testing Accuracy: 0.95\n",
            "Epoch: 351, Training Loss: 22.17, Training Accuracy: 0.95\n",
            "Epoch: 351, Validation Loss: 22.17, Validation Accuracy: 0.96\n",
            "Epoch: 351, Testing Loss: 22.17, Testing Accuracy: 0.95\n",
            "Epoch: 352, Training Loss: 22.14, Training Accuracy: 0.95\n",
            "Epoch: 352, Validation Loss: 22.14, Validation Accuracy: 0.96\n",
            "Epoch: 352, Testing Loss: 22.14, Testing Accuracy: 0.95\n",
            "Epoch: 353, Training Loss: 22.11, Training Accuracy: 0.95\n",
            "Epoch: 353, Validation Loss: 22.11, Validation Accuracy: 0.96\n",
            "Epoch: 353, Testing Loss: 22.11, Testing Accuracy: 0.95\n",
            "Epoch: 354, Training Loss: 22.08, Training Accuracy: 0.95\n",
            "Epoch: 354, Validation Loss: 22.08, Validation Accuracy: 0.96\n",
            "Epoch: 354, Testing Loss: 22.08, Testing Accuracy: 0.95\n",
            "Epoch: 355, Training Loss: 22.05, Training Accuracy: 0.95\n",
            "Epoch: 355, Validation Loss: 22.05, Validation Accuracy: 0.96\n",
            "Epoch: 355, Testing Loss: 22.05, Testing Accuracy: 0.95\n",
            "Epoch: 356, Training Loss: 22.03, Training Accuracy: 0.95\n",
            "Epoch: 356, Validation Loss: 22.03, Validation Accuracy: 0.96\n",
            "Epoch: 356, Testing Loss: 22.03, Testing Accuracy: 0.95\n",
            "Epoch: 357, Training Loss: 22.00, Training Accuracy: 0.95\n",
            "Epoch: 357, Validation Loss: 22.00, Validation Accuracy: 0.96\n",
            "Epoch: 357, Testing Loss: 22.00, Testing Accuracy: 0.95\n",
            "Epoch: 358, Training Loss: 21.97, Training Accuracy: 0.95\n",
            "Epoch: 358, Validation Loss: 21.97, Validation Accuracy: 0.96\n",
            "Epoch: 358, Testing Loss: 21.97, Testing Accuracy: 0.95\n",
            "Epoch: 359, Training Loss: 21.94, Training Accuracy: 0.95\n",
            "Epoch: 359, Validation Loss: 21.94, Validation Accuracy: 0.96\n",
            "Epoch: 359, Testing Loss: 21.94, Testing Accuracy: 0.95\n",
            "Epoch: 360, Training Loss: 21.91, Training Accuracy: 0.95\n",
            "Epoch: 360, Validation Loss: 21.91, Validation Accuracy: 0.96\n",
            "Epoch: 360, Testing Loss: 21.91, Testing Accuracy: 0.95\n",
            "Epoch: 361, Training Loss: 21.88, Training Accuracy: 0.95\n",
            "Epoch: 361, Validation Loss: 21.88, Validation Accuracy: 0.96\n",
            "Epoch: 361, Testing Loss: 21.88, Testing Accuracy: 0.95\n",
            "Epoch: 362, Training Loss: 21.85, Training Accuracy: 0.95\n",
            "Epoch: 362, Validation Loss: 21.85, Validation Accuracy: 0.96\n",
            "Epoch: 362, Testing Loss: 21.85, Testing Accuracy: 0.95\n",
            "Epoch: 363, Training Loss: 21.82, Training Accuracy: 0.95\n",
            "Epoch: 363, Validation Loss: 21.82, Validation Accuracy: 0.96\n",
            "Epoch: 363, Testing Loss: 21.82, Testing Accuracy: 0.95\n",
            "Epoch: 364, Training Loss: 21.79, Training Accuracy: 0.95\n",
            "Epoch: 364, Validation Loss: 21.79, Validation Accuracy: 0.96\n",
            "Epoch: 364, Testing Loss: 21.79, Testing Accuracy: 0.95\n",
            "Epoch: 365, Training Loss: 21.77, Training Accuracy: 0.96\n",
            "Epoch: 365, Validation Loss: 21.77, Validation Accuracy: 0.96\n",
            "Epoch: 365, Testing Loss: 21.77, Testing Accuracy: 0.95\n",
            "Epoch: 366, Training Loss: 21.74, Training Accuracy: 0.96\n",
            "Epoch: 366, Validation Loss: 21.74, Validation Accuracy: 0.96\n",
            "Epoch: 366, Testing Loss: 21.74, Testing Accuracy: 0.95\n",
            "Epoch: 367, Training Loss: 21.71, Training Accuracy: 0.96\n",
            "Epoch: 367, Validation Loss: 21.71, Validation Accuracy: 0.96\n",
            "Epoch: 367, Testing Loss: 21.71, Testing Accuracy: 0.95\n",
            "Epoch: 368, Training Loss: 21.68, Training Accuracy: 0.96\n",
            "Epoch: 368, Validation Loss: 21.68, Validation Accuracy: 0.96\n",
            "Epoch: 368, Testing Loss: 21.68, Testing Accuracy: 0.95\n",
            "Epoch: 369, Training Loss: 21.65, Training Accuracy: 0.96\n",
            "Epoch: 369, Validation Loss: 21.65, Validation Accuracy: 0.96\n",
            "Epoch: 369, Testing Loss: 21.65, Testing Accuracy: 0.95\n",
            "Epoch: 370, Training Loss: 21.63, Training Accuracy: 0.96\n",
            "Epoch: 370, Validation Loss: 21.63, Validation Accuracy: 0.96\n",
            "Epoch: 370, Testing Loss: 21.63, Testing Accuracy: 0.95\n",
            "Epoch: 371, Training Loss: 21.60, Training Accuracy: 0.96\n",
            "Epoch: 371, Validation Loss: 21.60, Validation Accuracy: 0.96\n",
            "Epoch: 371, Testing Loss: 21.60, Testing Accuracy: 0.95\n",
            "Epoch: 372, Training Loss: 21.57, Training Accuracy: 0.96\n",
            "Epoch: 372, Validation Loss: 21.57, Validation Accuracy: 0.96\n",
            "Epoch: 372, Testing Loss: 21.57, Testing Accuracy: 0.95\n",
            "Epoch: 373, Training Loss: 21.54, Training Accuracy: 0.96\n",
            "Epoch: 373, Validation Loss: 21.54, Validation Accuracy: 0.96\n",
            "Epoch: 373, Testing Loss: 21.54, Testing Accuracy: 0.95\n",
            "Epoch: 374, Training Loss: 21.52, Training Accuracy: 0.96\n",
            "Epoch: 374, Validation Loss: 21.52, Validation Accuracy: 0.96\n",
            "Epoch: 374, Testing Loss: 21.52, Testing Accuracy: 0.95\n",
            "Epoch: 375, Training Loss: 21.49, Training Accuracy: 0.96\n",
            "Epoch: 375, Validation Loss: 21.49, Validation Accuracy: 0.96\n",
            "Epoch: 375, Testing Loss: 21.49, Testing Accuracy: 0.95\n",
            "Epoch: 376, Training Loss: 21.46, Training Accuracy: 0.96\n",
            "Epoch: 376, Validation Loss: 21.46, Validation Accuracy: 0.96\n",
            "Epoch: 376, Testing Loss: 21.46, Testing Accuracy: 0.95\n",
            "Epoch: 377, Training Loss: 21.44, Training Accuracy: 0.96\n",
            "Epoch: 377, Validation Loss: 21.44, Validation Accuracy: 0.96\n",
            "Epoch: 377, Testing Loss: 21.44, Testing Accuracy: 0.95\n",
            "Epoch: 378, Training Loss: 21.41, Training Accuracy: 0.96\n",
            "Epoch: 378, Validation Loss: 21.41, Validation Accuracy: 0.96\n",
            "Epoch: 378, Testing Loss: 21.41, Testing Accuracy: 0.95\n",
            "Epoch: 379, Training Loss: 21.38, Training Accuracy: 0.96\n",
            "Epoch: 379, Validation Loss: 21.38, Validation Accuracy: 0.96\n",
            "Epoch: 379, Testing Loss: 21.38, Testing Accuracy: 0.95\n",
            "Epoch: 380, Training Loss: 21.36, Training Accuracy: 0.96\n",
            "Epoch: 380, Validation Loss: 21.36, Validation Accuracy: 0.96\n",
            "Epoch: 380, Testing Loss: 21.36, Testing Accuracy: 0.95\n",
            "Epoch: 381, Training Loss: 21.33, Training Accuracy: 0.96\n",
            "Epoch: 381, Validation Loss: 21.33, Validation Accuracy: 0.96\n",
            "Epoch: 381, Testing Loss: 21.33, Testing Accuracy: 0.95\n",
            "Epoch: 382, Training Loss: 21.30, Training Accuracy: 0.96\n",
            "Epoch: 382, Validation Loss: 21.30, Validation Accuracy: 0.96\n",
            "Epoch: 382, Testing Loss: 21.30, Testing Accuracy: 0.95\n",
            "Epoch: 383, Training Loss: 21.28, Training Accuracy: 0.96\n",
            "Epoch: 383, Validation Loss: 21.28, Validation Accuracy: 0.96\n",
            "Epoch: 383, Testing Loss: 21.28, Testing Accuracy: 0.95\n",
            "Epoch: 384, Training Loss: 21.25, Training Accuracy: 0.96\n",
            "Epoch: 384, Validation Loss: 21.25, Validation Accuracy: 0.96\n",
            "Epoch: 384, Testing Loss: 21.25, Testing Accuracy: 0.95\n",
            "Epoch: 385, Training Loss: 21.23, Training Accuracy: 0.96\n",
            "Epoch: 385, Validation Loss: 21.23, Validation Accuracy: 0.96\n",
            "Epoch: 385, Testing Loss: 21.23, Testing Accuracy: 0.95\n",
            "Epoch: 386, Training Loss: 21.20, Training Accuracy: 0.96\n",
            "Epoch: 386, Validation Loss: 21.20, Validation Accuracy: 0.96\n",
            "Epoch: 386, Testing Loss: 21.20, Testing Accuracy: 0.95\n",
            "Epoch: 387, Training Loss: 21.18, Training Accuracy: 0.96\n",
            "Epoch: 387, Validation Loss: 21.18, Validation Accuracy: 0.96\n",
            "Epoch: 387, Testing Loss: 21.18, Testing Accuracy: 0.95\n",
            "Epoch: 388, Training Loss: 21.15, Training Accuracy: 0.96\n",
            "Epoch: 388, Validation Loss: 21.15, Validation Accuracy: 0.96\n",
            "Epoch: 388, Testing Loss: 21.15, Testing Accuracy: 0.95\n",
            "Epoch: 389, Training Loss: 21.12, Training Accuracy: 0.96\n",
            "Epoch: 389, Validation Loss: 21.12, Validation Accuracy: 0.96\n",
            "Epoch: 389, Testing Loss: 21.12, Testing Accuracy: 0.95\n",
            "Epoch: 390, Training Loss: 21.10, Training Accuracy: 0.96\n",
            "Epoch: 390, Validation Loss: 21.10, Validation Accuracy: 0.96\n",
            "Epoch: 390, Testing Loss: 21.10, Testing Accuracy: 0.95\n",
            "Epoch: 391, Training Loss: 21.07, Training Accuracy: 0.96\n",
            "Epoch: 391, Validation Loss: 21.07, Validation Accuracy: 0.96\n",
            "Epoch: 391, Testing Loss: 21.07, Testing Accuracy: 0.95\n",
            "Epoch: 392, Training Loss: 21.05, Training Accuracy: 0.96\n",
            "Epoch: 392, Validation Loss: 21.05, Validation Accuracy: 0.96\n",
            "Epoch: 392, Testing Loss: 21.05, Testing Accuracy: 0.95\n",
            "Epoch: 393, Training Loss: 21.02, Training Accuracy: 0.96\n",
            "Epoch: 393, Validation Loss: 21.02, Validation Accuracy: 0.96\n",
            "Epoch: 393, Testing Loss: 21.02, Testing Accuracy: 0.95\n",
            "Epoch: 394, Training Loss: 21.00, Training Accuracy: 0.96\n",
            "Epoch: 394, Validation Loss: 21.00, Validation Accuracy: 0.96\n",
            "Epoch: 394, Testing Loss: 21.00, Testing Accuracy: 0.95\n",
            "Epoch: 395, Training Loss: 20.97, Training Accuracy: 0.96\n",
            "Epoch: 395, Validation Loss: 20.97, Validation Accuracy: 0.96\n",
            "Epoch: 395, Testing Loss: 20.97, Testing Accuracy: 0.95\n",
            "Epoch: 396, Training Loss: 20.95, Training Accuracy: 0.96\n",
            "Epoch: 396, Validation Loss: 20.95, Validation Accuracy: 0.96\n",
            "Epoch: 396, Testing Loss: 20.95, Testing Accuracy: 0.95\n",
            "Epoch: 397, Training Loss: 20.92, Training Accuracy: 0.96\n",
            "Epoch: 397, Validation Loss: 20.92, Validation Accuracy: 0.96\n",
            "Epoch: 397, Testing Loss: 20.92, Testing Accuracy: 0.95\n",
            "Epoch: 398, Training Loss: 20.90, Training Accuracy: 0.96\n",
            "Epoch: 398, Validation Loss: 20.90, Validation Accuracy: 0.96\n",
            "Epoch: 398, Testing Loss: 20.90, Testing Accuracy: 0.95\n",
            "Epoch: 399, Training Loss: 20.88, Training Accuracy: 0.96\n",
            "Epoch: 399, Validation Loss: 20.88, Validation Accuracy: 0.96\n",
            "Epoch: 399, Testing Loss: 20.88, Testing Accuracy: 0.95\n",
            "Epoch: 400, Training Loss: 20.85, Training Accuracy: 0.96\n",
            "Epoch: 400, Validation Loss: 20.85, Validation Accuracy: 0.96\n",
            "Epoch: 400, Testing Loss: 20.85, Testing Accuracy: 0.95\n",
            "Epoch: 401, Training Loss: 20.83, Training Accuracy: 0.96\n",
            "Epoch: 401, Validation Loss: 20.83, Validation Accuracy: 0.96\n",
            "Epoch: 401, Testing Loss: 20.83, Testing Accuracy: 0.95\n",
            "Epoch: 402, Training Loss: 20.80, Training Accuracy: 0.96\n",
            "Epoch: 402, Validation Loss: 20.80, Validation Accuracy: 0.96\n",
            "Epoch: 402, Testing Loss: 20.80, Testing Accuracy: 0.95\n",
            "Epoch: 403, Training Loss: 20.78, Training Accuracy: 0.96\n",
            "Epoch: 403, Validation Loss: 20.78, Validation Accuracy: 0.96\n",
            "Epoch: 403, Testing Loss: 20.78, Testing Accuracy: 0.95\n",
            "Epoch: 404, Training Loss: 20.76, Training Accuracy: 0.96\n",
            "Epoch: 404, Validation Loss: 20.76, Validation Accuracy: 0.96\n",
            "Epoch: 404, Testing Loss: 20.76, Testing Accuracy: 0.95\n",
            "Epoch: 405, Training Loss: 20.73, Training Accuracy: 0.96\n",
            "Epoch: 405, Validation Loss: 20.73, Validation Accuracy: 0.96\n",
            "Epoch: 405, Testing Loss: 20.73, Testing Accuracy: 0.95\n",
            "Epoch: 406, Training Loss: 20.71, Training Accuracy: 0.96\n",
            "Epoch: 406, Validation Loss: 20.71, Validation Accuracy: 0.96\n",
            "Epoch: 406, Testing Loss: 20.71, Testing Accuracy: 0.95\n",
            "Epoch: 407, Training Loss: 20.68, Training Accuracy: 0.96\n",
            "Epoch: 407, Validation Loss: 20.68, Validation Accuracy: 0.96\n",
            "Epoch: 407, Testing Loss: 20.68, Testing Accuracy: 0.95\n",
            "Epoch: 408, Training Loss: 20.66, Training Accuracy: 0.96\n",
            "Epoch: 408, Validation Loss: 20.66, Validation Accuracy: 0.96\n",
            "Epoch: 408, Testing Loss: 20.66, Testing Accuracy: 0.95\n",
            "Epoch: 409, Training Loss: 20.64, Training Accuracy: 0.96\n",
            "Epoch: 409, Validation Loss: 20.64, Validation Accuracy: 0.96\n",
            "Epoch: 409, Testing Loss: 20.64, Testing Accuracy: 0.95\n",
            "Epoch: 410, Training Loss: 20.61, Training Accuracy: 0.96\n",
            "Epoch: 410, Validation Loss: 20.61, Validation Accuracy: 0.96\n",
            "Epoch: 410, Testing Loss: 20.61, Testing Accuracy: 0.95\n",
            "Epoch: 411, Training Loss: 20.59, Training Accuracy: 0.96\n",
            "Epoch: 411, Validation Loss: 20.59, Validation Accuracy: 0.96\n",
            "Epoch: 411, Testing Loss: 20.59, Testing Accuracy: 0.95\n",
            "Epoch: 412, Training Loss: 20.57, Training Accuracy: 0.96\n",
            "Epoch: 412, Validation Loss: 20.57, Validation Accuracy: 0.96\n",
            "Epoch: 412, Testing Loss: 20.57, Testing Accuracy: 0.95\n",
            "Epoch: 413, Training Loss: 20.54, Training Accuracy: 0.96\n",
            "Epoch: 413, Validation Loss: 20.54, Validation Accuracy: 0.96\n",
            "Epoch: 413, Testing Loss: 20.54, Testing Accuracy: 0.95\n",
            "Epoch: 414, Training Loss: 20.52, Training Accuracy: 0.96\n",
            "Epoch: 414, Validation Loss: 20.52, Validation Accuracy: 0.96\n",
            "Epoch: 414, Testing Loss: 20.52, Testing Accuracy: 0.95\n",
            "Epoch: 415, Training Loss: 20.50, Training Accuracy: 0.96\n",
            "Epoch: 415, Validation Loss: 20.50, Validation Accuracy: 0.96\n",
            "Epoch: 415, Testing Loss: 20.50, Testing Accuracy: 0.95\n",
            "Epoch: 416, Training Loss: 20.48, Training Accuracy: 0.96\n",
            "Epoch: 416, Validation Loss: 20.48, Validation Accuracy: 0.96\n",
            "Epoch: 416, Testing Loss: 20.48, Testing Accuracy: 0.95\n",
            "Epoch: 417, Training Loss: 20.45, Training Accuracy: 0.96\n",
            "Epoch: 417, Validation Loss: 20.45, Validation Accuracy: 0.96\n",
            "Epoch: 417, Testing Loss: 20.45, Testing Accuracy: 0.95\n",
            "Epoch: 418, Training Loss: 20.43, Training Accuracy: 0.96\n",
            "Epoch: 418, Validation Loss: 20.43, Validation Accuracy: 0.96\n",
            "Epoch: 418, Testing Loss: 20.43, Testing Accuracy: 0.95\n",
            "Epoch: 419, Training Loss: 20.41, Training Accuracy: 0.96\n",
            "Epoch: 419, Validation Loss: 20.41, Validation Accuracy: 0.96\n",
            "Epoch: 419, Testing Loss: 20.41, Testing Accuracy: 0.95\n",
            "Epoch: 420, Training Loss: 20.39, Training Accuracy: 0.96\n",
            "Epoch: 420, Validation Loss: 20.39, Validation Accuracy: 0.96\n",
            "Epoch: 420, Testing Loss: 20.39, Testing Accuracy: 0.95\n",
            "Epoch: 421, Training Loss: 20.36, Training Accuracy: 0.96\n",
            "Epoch: 421, Validation Loss: 20.36, Validation Accuracy: 0.96\n",
            "Epoch: 421, Testing Loss: 20.36, Testing Accuracy: 0.95\n",
            "Epoch: 422, Training Loss: 20.34, Training Accuracy: 0.96\n",
            "Epoch: 422, Validation Loss: 20.34, Validation Accuracy: 0.96\n",
            "Epoch: 422, Testing Loss: 20.34, Testing Accuracy: 0.95\n",
            "Epoch: 423, Training Loss: 20.32, Training Accuracy: 0.96\n",
            "Epoch: 423, Validation Loss: 20.32, Validation Accuracy: 0.96\n",
            "Epoch: 423, Testing Loss: 20.32, Testing Accuracy: 0.95\n",
            "Epoch: 424, Training Loss: 20.30, Training Accuracy: 0.96\n",
            "Epoch: 424, Validation Loss: 20.30, Validation Accuracy: 0.96\n",
            "Epoch: 424, Testing Loss: 20.30, Testing Accuracy: 0.95\n",
            "Epoch: 425, Training Loss: 20.28, Training Accuracy: 0.96\n",
            "Epoch: 425, Validation Loss: 20.28, Validation Accuracy: 0.96\n",
            "Epoch: 425, Testing Loss: 20.28, Testing Accuracy: 0.95\n",
            "Epoch: 426, Training Loss: 20.25, Training Accuracy: 0.96\n",
            "Epoch: 426, Validation Loss: 20.25, Validation Accuracy: 0.96\n",
            "Epoch: 426, Testing Loss: 20.25, Testing Accuracy: 0.95\n",
            "Epoch: 427, Training Loss: 20.23, Training Accuracy: 0.96\n",
            "Epoch: 427, Validation Loss: 20.23, Validation Accuracy: 0.96\n",
            "Epoch: 427, Testing Loss: 20.23, Testing Accuracy: 0.95\n",
            "Epoch: 428, Training Loss: 20.21, Training Accuracy: 0.96\n",
            "Epoch: 428, Validation Loss: 20.21, Validation Accuracy: 0.96\n",
            "Epoch: 428, Testing Loss: 20.21, Testing Accuracy: 0.95\n",
            "Epoch: 429, Training Loss: 20.19, Training Accuracy: 0.96\n",
            "Epoch: 429, Validation Loss: 20.19, Validation Accuracy: 0.96\n",
            "Epoch: 429, Testing Loss: 20.19, Testing Accuracy: 0.95\n",
            "Epoch: 430, Training Loss: 20.17, Training Accuracy: 0.96\n",
            "Epoch: 430, Validation Loss: 20.17, Validation Accuracy: 0.96\n",
            "Epoch: 430, Testing Loss: 20.17, Testing Accuracy: 0.95\n",
            "Epoch: 431, Training Loss: 20.15, Training Accuracy: 0.96\n",
            "Epoch: 431, Validation Loss: 20.15, Validation Accuracy: 0.96\n",
            "Epoch: 431, Testing Loss: 20.15, Testing Accuracy: 0.95\n",
            "Epoch: 432, Training Loss: 20.12, Training Accuracy: 0.96\n",
            "Epoch: 432, Validation Loss: 20.12, Validation Accuracy: 0.96\n",
            "Epoch: 432, Testing Loss: 20.12, Testing Accuracy: 0.95\n",
            "Epoch: 433, Training Loss: 20.10, Training Accuracy: 0.96\n",
            "Epoch: 433, Validation Loss: 20.10, Validation Accuracy: 0.96\n",
            "Epoch: 433, Testing Loss: 20.10, Testing Accuracy: 0.95\n",
            "Epoch: 434, Training Loss: 20.08, Training Accuracy: 0.96\n",
            "Epoch: 434, Validation Loss: 20.08, Validation Accuracy: 0.96\n",
            "Epoch: 434, Testing Loss: 20.08, Testing Accuracy: 0.95\n",
            "Epoch: 435, Training Loss: 20.06, Training Accuracy: 0.96\n",
            "Epoch: 435, Validation Loss: 20.06, Validation Accuracy: 0.96\n",
            "Epoch: 435, Testing Loss: 20.06, Testing Accuracy: 0.95\n",
            "Epoch: 436, Training Loss: 20.04, Training Accuracy: 0.96\n",
            "Epoch: 436, Validation Loss: 20.04, Validation Accuracy: 0.96\n",
            "Epoch: 436, Testing Loss: 20.04, Testing Accuracy: 0.95\n",
            "Epoch: 437, Training Loss: 20.02, Training Accuracy: 0.96\n",
            "Epoch: 437, Validation Loss: 20.02, Validation Accuracy: 0.96\n",
            "Epoch: 437, Testing Loss: 20.02, Testing Accuracy: 0.95\n",
            "Epoch: 438, Training Loss: 20.00, Training Accuracy: 0.96\n",
            "Epoch: 438, Validation Loss: 20.00, Validation Accuracy: 0.96\n",
            "Epoch: 438, Testing Loss: 20.00, Testing Accuracy: 0.95\n",
            "Epoch: 439, Training Loss: 19.98, Training Accuracy: 0.96\n",
            "Epoch: 439, Validation Loss: 19.98, Validation Accuracy: 0.96\n",
            "Epoch: 439, Testing Loss: 19.98, Testing Accuracy: 0.95\n",
            "Epoch: 440, Training Loss: 19.96, Training Accuracy: 0.96\n",
            "Epoch: 440, Validation Loss: 19.96, Validation Accuracy: 0.96\n",
            "Epoch: 440, Testing Loss: 19.96, Testing Accuracy: 0.95\n",
            "Epoch: 441, Training Loss: 19.94, Training Accuracy: 0.96\n",
            "Epoch: 441, Validation Loss: 19.94, Validation Accuracy: 0.96\n",
            "Epoch: 441, Testing Loss: 19.94, Testing Accuracy: 0.95\n",
            "Epoch: 442, Training Loss: 19.92, Training Accuracy: 0.96\n",
            "Epoch: 442, Validation Loss: 19.92, Validation Accuracy: 0.96\n",
            "Epoch: 442, Testing Loss: 19.92, Testing Accuracy: 0.95\n",
            "Epoch: 443, Training Loss: 19.90, Training Accuracy: 0.96\n",
            "Epoch: 443, Validation Loss: 19.90, Validation Accuracy: 0.96\n",
            "Epoch: 443, Testing Loss: 19.90, Testing Accuracy: 0.95\n",
            "Epoch: 444, Training Loss: 19.87, Training Accuracy: 0.96\n",
            "Epoch: 444, Validation Loss: 19.87, Validation Accuracy: 0.96\n",
            "Epoch: 444, Testing Loss: 19.87, Testing Accuracy: 0.95\n",
            "Epoch: 445, Training Loss: 19.85, Training Accuracy: 0.96\n",
            "Epoch: 445, Validation Loss: 19.85, Validation Accuracy: 0.96\n",
            "Epoch: 445, Testing Loss: 19.85, Testing Accuracy: 0.95\n",
            "Epoch: 446, Training Loss: 19.83, Training Accuracy: 0.96\n",
            "Epoch: 446, Validation Loss: 19.83, Validation Accuracy: 0.96\n",
            "Epoch: 446, Testing Loss: 19.83, Testing Accuracy: 0.95\n",
            "Epoch: 447, Training Loss: 19.81, Training Accuracy: 0.96\n",
            "Epoch: 447, Validation Loss: 19.81, Validation Accuracy: 0.96\n",
            "Epoch: 447, Testing Loss: 19.81, Testing Accuracy: 0.95\n",
            "Epoch: 448, Training Loss: 19.79, Training Accuracy: 0.96\n",
            "Epoch: 448, Validation Loss: 19.79, Validation Accuracy: 0.96\n",
            "Epoch: 448, Testing Loss: 19.79, Testing Accuracy: 0.95\n",
            "Epoch: 449, Training Loss: 19.77, Training Accuracy: 0.96\n",
            "Epoch: 449, Validation Loss: 19.77, Validation Accuracy: 0.96\n",
            "Epoch: 449, Testing Loss: 19.77, Testing Accuracy: 0.95\n",
            "Epoch: 450, Training Loss: 19.75, Training Accuracy: 0.96\n",
            "Epoch: 450, Validation Loss: 19.75, Validation Accuracy: 0.96\n",
            "Epoch: 450, Testing Loss: 19.75, Testing Accuracy: 0.95\n",
            "Epoch: 451, Training Loss: 19.73, Training Accuracy: 0.96\n",
            "Epoch: 451, Validation Loss: 19.73, Validation Accuracy: 0.96\n",
            "Epoch: 451, Testing Loss: 19.73, Testing Accuracy: 0.95\n",
            "Epoch: 452, Training Loss: 19.71, Training Accuracy: 0.96\n",
            "Epoch: 452, Validation Loss: 19.71, Validation Accuracy: 0.96\n",
            "Epoch: 452, Testing Loss: 19.71, Testing Accuracy: 0.95\n",
            "Epoch: 453, Training Loss: 19.69, Training Accuracy: 0.96\n",
            "Epoch: 453, Validation Loss: 19.69, Validation Accuracy: 0.96\n",
            "Epoch: 453, Testing Loss: 19.69, Testing Accuracy: 0.95\n",
            "Epoch: 454, Training Loss: 19.67, Training Accuracy: 0.96\n",
            "Epoch: 454, Validation Loss: 19.67, Validation Accuracy: 0.96\n",
            "Epoch: 454, Testing Loss: 19.67, Testing Accuracy: 0.95\n",
            "Epoch: 455, Training Loss: 19.66, Training Accuracy: 0.96\n",
            "Epoch: 455, Validation Loss: 19.66, Validation Accuracy: 0.96\n",
            "Epoch: 455, Testing Loss: 19.66, Testing Accuracy: 0.95\n",
            "Epoch: 456, Training Loss: 19.64, Training Accuracy: 0.96\n",
            "Epoch: 456, Validation Loss: 19.64, Validation Accuracy: 0.96\n",
            "Epoch: 456, Testing Loss: 19.64, Testing Accuracy: 0.95\n",
            "Epoch: 457, Training Loss: 19.62, Training Accuracy: 0.96\n",
            "Epoch: 457, Validation Loss: 19.62, Validation Accuracy: 0.96\n",
            "Epoch: 457, Testing Loss: 19.62, Testing Accuracy: 0.95\n",
            "Epoch: 458, Training Loss: 19.60, Training Accuracy: 0.96\n",
            "Epoch: 458, Validation Loss: 19.60, Validation Accuracy: 0.96\n",
            "Epoch: 458, Testing Loss: 19.60, Testing Accuracy: 0.95\n",
            "Epoch: 459, Training Loss: 19.58, Training Accuracy: 0.96\n",
            "Epoch: 459, Validation Loss: 19.58, Validation Accuracy: 0.96\n",
            "Epoch: 459, Testing Loss: 19.58, Testing Accuracy: 0.95\n",
            "Epoch: 460, Training Loss: 19.56, Training Accuracy: 0.96\n",
            "Epoch: 460, Validation Loss: 19.56, Validation Accuracy: 0.96\n",
            "Epoch: 460, Testing Loss: 19.56, Testing Accuracy: 0.95\n",
            "Epoch: 461, Training Loss: 19.54, Training Accuracy: 0.96\n",
            "Epoch: 461, Validation Loss: 19.54, Validation Accuracy: 0.96\n",
            "Epoch: 461, Testing Loss: 19.54, Testing Accuracy: 0.95\n",
            "Epoch: 462, Training Loss: 19.52, Training Accuracy: 0.96\n",
            "Epoch: 462, Validation Loss: 19.52, Validation Accuracy: 0.96\n",
            "Epoch: 462, Testing Loss: 19.52, Testing Accuracy: 0.95\n",
            "Epoch: 463, Training Loss: 19.50, Training Accuracy: 0.96\n",
            "Epoch: 463, Validation Loss: 19.50, Validation Accuracy: 0.96\n",
            "Epoch: 463, Testing Loss: 19.50, Testing Accuracy: 0.95\n",
            "Epoch: 464, Training Loss: 19.48, Training Accuracy: 0.96\n",
            "Epoch: 464, Validation Loss: 19.48, Validation Accuracy: 0.96\n",
            "Epoch: 464, Testing Loss: 19.48, Testing Accuracy: 0.95\n",
            "Epoch: 465, Training Loss: 19.46, Training Accuracy: 0.96\n",
            "Epoch: 465, Validation Loss: 19.46, Validation Accuracy: 0.96\n",
            "Epoch: 465, Testing Loss: 19.46, Testing Accuracy: 0.95\n",
            "Epoch: 466, Training Loss: 19.44, Training Accuracy: 0.96\n",
            "Epoch: 466, Validation Loss: 19.44, Validation Accuracy: 0.96\n",
            "Epoch: 466, Testing Loss: 19.44, Testing Accuracy: 0.95\n",
            "Epoch: 467, Training Loss: 19.43, Training Accuracy: 0.96\n",
            "Epoch: 467, Validation Loss: 19.43, Validation Accuracy: 0.96\n",
            "Epoch: 467, Testing Loss: 19.43, Testing Accuracy: 0.95\n",
            "Epoch: 468, Training Loss: 19.41, Training Accuracy: 0.96\n",
            "Epoch: 468, Validation Loss: 19.41, Validation Accuracy: 0.96\n",
            "Epoch: 468, Testing Loss: 19.41, Testing Accuracy: 0.95\n",
            "Epoch: 469, Training Loss: 19.39, Training Accuracy: 0.96\n",
            "Epoch: 469, Validation Loss: 19.39, Validation Accuracy: 0.96\n",
            "Epoch: 469, Testing Loss: 19.39, Testing Accuracy: 0.95\n",
            "Epoch: 470, Training Loss: 19.37, Training Accuracy: 0.96\n",
            "Epoch: 470, Validation Loss: 19.37, Validation Accuracy: 0.96\n",
            "Epoch: 470, Testing Loss: 19.37, Testing Accuracy: 0.95\n",
            "Epoch: 471, Training Loss: 19.35, Training Accuracy: 0.96\n",
            "Epoch: 471, Validation Loss: 19.35, Validation Accuracy: 0.96\n",
            "Epoch: 471, Testing Loss: 19.35, Testing Accuracy: 0.95\n",
            "Epoch: 472, Training Loss: 19.33, Training Accuracy: 0.96\n",
            "Epoch: 472, Validation Loss: 19.33, Validation Accuracy: 0.96\n",
            "Epoch: 472, Testing Loss: 19.33, Testing Accuracy: 0.95\n",
            "Epoch: 473, Training Loss: 19.32, Training Accuracy: 0.96\n",
            "Epoch: 473, Validation Loss: 19.32, Validation Accuracy: 0.96\n",
            "Epoch: 473, Testing Loss: 19.32, Testing Accuracy: 0.95\n",
            "Epoch: 474, Training Loss: 19.30, Training Accuracy: 0.96\n",
            "Epoch: 474, Validation Loss: 19.30, Validation Accuracy: 0.96\n",
            "Epoch: 474, Testing Loss: 19.30, Testing Accuracy: 0.95\n",
            "Epoch: 475, Training Loss: 19.28, Training Accuracy: 0.96\n",
            "Epoch: 475, Validation Loss: 19.28, Validation Accuracy: 0.96\n",
            "Epoch: 475, Testing Loss: 19.28, Testing Accuracy: 0.95\n",
            "Epoch: 476, Training Loss: 19.26, Training Accuracy: 0.96\n",
            "Epoch: 476, Validation Loss: 19.26, Validation Accuracy: 0.96\n",
            "Epoch: 476, Testing Loss: 19.26, Testing Accuracy: 0.95\n",
            "Epoch: 477, Training Loss: 19.24, Training Accuracy: 0.96\n",
            "Epoch: 477, Validation Loss: 19.24, Validation Accuracy: 0.96\n",
            "Epoch: 477, Testing Loss: 19.24, Testing Accuracy: 0.95\n",
            "Epoch: 478, Training Loss: 19.22, Training Accuracy: 0.96\n",
            "Epoch: 478, Validation Loss: 19.22, Validation Accuracy: 0.96\n",
            "Epoch: 478, Testing Loss: 19.22, Testing Accuracy: 0.95\n",
            "Epoch: 479, Training Loss: 19.21, Training Accuracy: 0.96\n",
            "Epoch: 479, Validation Loss: 19.21, Validation Accuracy: 0.96\n",
            "Epoch: 479, Testing Loss: 19.21, Testing Accuracy: 0.95\n",
            "Epoch: 480, Training Loss: 19.19, Training Accuracy: 0.96\n",
            "Epoch: 480, Validation Loss: 19.19, Validation Accuracy: 0.96\n",
            "Epoch: 480, Testing Loss: 19.19, Testing Accuracy: 0.95\n",
            "Epoch: 481, Training Loss: 19.17, Training Accuracy: 0.96\n",
            "Epoch: 481, Validation Loss: 19.17, Validation Accuracy: 0.96\n",
            "Epoch: 481, Testing Loss: 19.17, Testing Accuracy: 0.95\n",
            "Epoch: 482, Training Loss: 19.15, Training Accuracy: 0.96\n",
            "Epoch: 482, Validation Loss: 19.15, Validation Accuracy: 0.96\n",
            "Epoch: 482, Testing Loss: 19.15, Testing Accuracy: 0.95\n",
            "Epoch: 483, Training Loss: 19.14, Training Accuracy: 0.96\n",
            "Epoch: 483, Validation Loss: 19.14, Validation Accuracy: 0.96\n",
            "Epoch: 483, Testing Loss: 19.14, Testing Accuracy: 0.95\n",
            "Epoch: 484, Training Loss: 19.12, Training Accuracy: 0.96\n",
            "Epoch: 484, Validation Loss: 19.12, Validation Accuracy: 0.96\n",
            "Epoch: 484, Testing Loss: 19.12, Testing Accuracy: 0.95\n",
            "Epoch: 485, Training Loss: 19.10, Training Accuracy: 0.96\n",
            "Epoch: 485, Validation Loss: 19.10, Validation Accuracy: 0.96\n",
            "Epoch: 485, Testing Loss: 19.10, Testing Accuracy: 0.95\n",
            "Epoch: 486, Training Loss: 19.08, Training Accuracy: 0.96\n",
            "Epoch: 486, Validation Loss: 19.08, Validation Accuracy: 0.96\n",
            "Epoch: 486, Testing Loss: 19.08, Testing Accuracy: 0.95\n",
            "Epoch: 487, Training Loss: 19.07, Training Accuracy: 0.96\n",
            "Epoch: 487, Validation Loss: 19.07, Validation Accuracy: 0.96\n",
            "Epoch: 487, Testing Loss: 19.07, Testing Accuracy: 0.95\n",
            "Epoch: 488, Training Loss: 19.05, Training Accuracy: 0.96\n",
            "Epoch: 488, Validation Loss: 19.05, Validation Accuracy: 0.96\n",
            "Epoch: 488, Testing Loss: 19.05, Testing Accuracy: 0.95\n",
            "Epoch: 489, Training Loss: 19.03, Training Accuracy: 0.96\n",
            "Epoch: 489, Validation Loss: 19.03, Validation Accuracy: 0.96\n",
            "Epoch: 489, Testing Loss: 19.03, Testing Accuracy: 0.95\n",
            "Epoch: 490, Training Loss: 19.01, Training Accuracy: 0.96\n",
            "Epoch: 490, Validation Loss: 19.01, Validation Accuracy: 0.96\n",
            "Epoch: 490, Testing Loss: 19.01, Testing Accuracy: 0.95\n",
            "Epoch: 491, Training Loss: 19.00, Training Accuracy: 0.96\n",
            "Epoch: 491, Validation Loss: 19.00, Validation Accuracy: 0.96\n",
            "Epoch: 491, Testing Loss: 19.00, Testing Accuracy: 0.95\n",
            "Epoch: 492, Training Loss: 18.98, Training Accuracy: 0.96\n",
            "Epoch: 492, Validation Loss: 18.98, Validation Accuracy: 0.96\n",
            "Epoch: 492, Testing Loss: 18.98, Testing Accuracy: 0.95\n",
            "Epoch: 493, Training Loss: 18.96, Training Accuracy: 0.96\n",
            "Epoch: 493, Validation Loss: 18.96, Validation Accuracy: 0.96\n",
            "Epoch: 493, Testing Loss: 18.96, Testing Accuracy: 0.95\n",
            "Epoch: 494, Training Loss: 18.94, Training Accuracy: 0.96\n",
            "Epoch: 494, Validation Loss: 18.94, Validation Accuracy: 0.96\n",
            "Epoch: 494, Testing Loss: 18.94, Testing Accuracy: 0.95\n",
            "Epoch: 495, Training Loss: 18.93, Training Accuracy: 0.96\n",
            "Epoch: 495, Validation Loss: 18.93, Validation Accuracy: 0.96\n",
            "Epoch: 495, Testing Loss: 18.93, Testing Accuracy: 0.95\n",
            "Epoch: 496, Training Loss: 18.91, Training Accuracy: 0.96\n",
            "Epoch: 496, Validation Loss: 18.91, Validation Accuracy: 0.96\n",
            "Epoch: 496, Testing Loss: 18.91, Testing Accuracy: 0.95\n",
            "Epoch: 497, Training Loss: 18.89, Training Accuracy: 0.96\n",
            "Epoch: 497, Validation Loss: 18.89, Validation Accuracy: 0.96\n",
            "Epoch: 497, Testing Loss: 18.89, Testing Accuracy: 0.95\n",
            "Epoch: 498, Training Loss: 18.88, Training Accuracy: 0.96\n",
            "Epoch: 498, Validation Loss: 18.88, Validation Accuracy: 0.96\n",
            "Epoch: 498, Testing Loss: 18.88, Testing Accuracy: 0.95\n",
            "Epoch: 499, Training Loss: 18.86, Training Accuracy: 0.96\n",
            "Epoch: 499, Validation Loss: 18.86, Validation Accuracy: 0.96\n",
            "Epoch: 499, Testing Loss: 18.86, Testing Accuracy: 0.95\n",
            "Epoch: 500, Training Loss: 18.84, Training Accuracy: 0.96\n",
            "Epoch: 500, Validation Loss: 18.84, Validation Accuracy: 0.96\n",
            "Epoch: 500, Testing Loss: 18.84, Testing Accuracy: 0.95\n",
            "Epoch: 501, Training Loss: 18.83, Training Accuracy: 0.96\n",
            "Epoch: 501, Validation Loss: 18.83, Validation Accuracy: 0.96\n",
            "Epoch: 501, Testing Loss: 18.83, Testing Accuracy: 0.95\n",
            "Epoch: 502, Training Loss: 18.81, Training Accuracy: 0.96\n",
            "Epoch: 502, Validation Loss: 18.81, Validation Accuracy: 0.96\n",
            "Epoch: 502, Testing Loss: 18.81, Testing Accuracy: 0.95\n",
            "Epoch: 503, Training Loss: 18.79, Training Accuracy: 0.96\n",
            "Epoch: 503, Validation Loss: 18.79, Validation Accuracy: 0.96\n",
            "Epoch: 503, Testing Loss: 18.79, Testing Accuracy: 0.95\n",
            "Epoch: 504, Training Loss: 18.78, Training Accuracy: 0.96\n",
            "Epoch: 504, Validation Loss: 18.78, Validation Accuracy: 0.96\n",
            "Epoch: 504, Testing Loss: 18.78, Testing Accuracy: 0.95\n",
            "Epoch: 505, Training Loss: 18.76, Training Accuracy: 0.96\n",
            "Epoch: 505, Validation Loss: 18.76, Validation Accuracy: 0.96\n",
            "Epoch: 505, Testing Loss: 18.76, Testing Accuracy: 0.95\n",
            "Epoch: 506, Training Loss: 18.74, Training Accuracy: 0.96\n",
            "Epoch: 506, Validation Loss: 18.74, Validation Accuracy: 0.96\n",
            "Epoch: 506, Testing Loss: 18.74, Testing Accuracy: 0.95\n",
            "Epoch: 507, Training Loss: 18.73, Training Accuracy: 0.96\n",
            "Epoch: 507, Validation Loss: 18.73, Validation Accuracy: 0.96\n",
            "Epoch: 507, Testing Loss: 18.73, Testing Accuracy: 0.95\n",
            "Epoch: 508, Training Loss: 18.71, Training Accuracy: 0.96\n",
            "Epoch: 508, Validation Loss: 18.71, Validation Accuracy: 0.96\n",
            "Epoch: 508, Testing Loss: 18.71, Testing Accuracy: 0.95\n",
            "Epoch: 509, Training Loss: 18.70, Training Accuracy: 0.96\n",
            "Epoch: 509, Validation Loss: 18.70, Validation Accuracy: 0.96\n",
            "Epoch: 509, Testing Loss: 18.70, Testing Accuracy: 0.95\n",
            "Epoch: 510, Training Loss: 18.68, Training Accuracy: 0.96\n",
            "Epoch: 510, Validation Loss: 18.68, Validation Accuracy: 0.96\n",
            "Epoch: 510, Testing Loss: 18.68, Testing Accuracy: 0.95\n",
            "Epoch: 511, Training Loss: 18.66, Training Accuracy: 0.96\n",
            "Epoch: 511, Validation Loss: 18.66, Validation Accuracy: 0.96\n",
            "Epoch: 511, Testing Loss: 18.66, Testing Accuracy: 0.95\n",
            "Epoch: 512, Training Loss: 18.65, Training Accuracy: 0.96\n",
            "Epoch: 512, Validation Loss: 18.65, Validation Accuracy: 0.96\n",
            "Epoch: 512, Testing Loss: 18.65, Testing Accuracy: 0.95\n",
            "Epoch: 513, Training Loss: 18.63, Training Accuracy: 0.96\n",
            "Epoch: 513, Validation Loss: 18.63, Validation Accuracy: 0.96\n",
            "Epoch: 513, Testing Loss: 18.63, Testing Accuracy: 0.95\n",
            "Epoch: 514, Training Loss: 18.62, Training Accuracy: 0.96\n",
            "Epoch: 514, Validation Loss: 18.62, Validation Accuracy: 0.96\n",
            "Epoch: 514, Testing Loss: 18.62, Testing Accuracy: 0.95\n",
            "Epoch: 515, Training Loss: 18.60, Training Accuracy: 0.96\n",
            "Epoch: 515, Validation Loss: 18.60, Validation Accuracy: 0.96\n",
            "Epoch: 515, Testing Loss: 18.60, Testing Accuracy: 0.95\n",
            "Epoch: 516, Training Loss: 18.58, Training Accuracy: 0.96\n",
            "Epoch: 516, Validation Loss: 18.58, Validation Accuracy: 0.96\n",
            "Epoch: 516, Testing Loss: 18.58, Testing Accuracy: 0.95\n",
            "Epoch: 517, Training Loss: 18.57, Training Accuracy: 0.96\n",
            "Epoch: 517, Validation Loss: 18.57, Validation Accuracy: 0.96\n",
            "Epoch: 517, Testing Loss: 18.57, Testing Accuracy: 0.95\n",
            "Epoch: 518, Training Loss: 18.55, Training Accuracy: 0.96\n",
            "Epoch: 518, Validation Loss: 18.55, Validation Accuracy: 0.96\n",
            "Epoch: 518, Testing Loss: 18.55, Testing Accuracy: 0.95\n",
            "Epoch: 519, Training Loss: 18.54, Training Accuracy: 0.96\n",
            "Epoch: 519, Validation Loss: 18.54, Validation Accuracy: 0.96\n",
            "Epoch: 519, Testing Loss: 18.54, Testing Accuracy: 0.95\n",
            "Epoch: 520, Training Loss: 18.52, Training Accuracy: 0.96\n",
            "Epoch: 520, Validation Loss: 18.52, Validation Accuracy: 0.96\n",
            "Epoch: 520, Testing Loss: 18.52, Testing Accuracy: 0.95\n",
            "Epoch: 521, Training Loss: 18.51, Training Accuracy: 0.96\n",
            "Epoch: 521, Validation Loss: 18.51, Validation Accuracy: 0.96\n",
            "Epoch: 521, Testing Loss: 18.51, Testing Accuracy: 0.95\n",
            "Epoch: 522, Training Loss: 18.49, Training Accuracy: 0.96\n",
            "Epoch: 522, Validation Loss: 18.49, Validation Accuracy: 0.96\n",
            "Epoch: 522, Testing Loss: 18.49, Testing Accuracy: 0.95\n",
            "Epoch: 523, Training Loss: 18.47, Training Accuracy: 0.96\n",
            "Epoch: 523, Validation Loss: 18.47, Validation Accuracy: 0.96\n",
            "Epoch: 523, Testing Loss: 18.47, Testing Accuracy: 0.95\n",
            "Epoch: 524, Training Loss: 18.46, Training Accuracy: 0.96\n",
            "Epoch: 524, Validation Loss: 18.46, Validation Accuracy: 0.96\n",
            "Epoch: 524, Testing Loss: 18.46, Testing Accuracy: 0.95\n",
            "Epoch: 525, Training Loss: 18.44, Training Accuracy: 0.96\n",
            "Epoch: 525, Validation Loss: 18.44, Validation Accuracy: 0.96\n",
            "Epoch: 525, Testing Loss: 18.44, Testing Accuracy: 0.95\n",
            "Epoch: 526, Training Loss: 18.43, Training Accuracy: 0.96\n",
            "Epoch: 526, Validation Loss: 18.43, Validation Accuracy: 0.96\n",
            "Epoch: 526, Testing Loss: 18.43, Testing Accuracy: 0.95\n",
            "Epoch: 527, Training Loss: 18.41, Training Accuracy: 0.96\n",
            "Epoch: 527, Validation Loss: 18.41, Validation Accuracy: 0.96\n",
            "Epoch: 527, Testing Loss: 18.41, Testing Accuracy: 0.95\n",
            "Epoch: 528, Training Loss: 18.40, Training Accuracy: 0.96\n",
            "Epoch: 528, Validation Loss: 18.40, Validation Accuracy: 0.96\n",
            "Epoch: 528, Testing Loss: 18.40, Testing Accuracy: 0.95\n",
            "Epoch: 529, Training Loss: 18.38, Training Accuracy: 0.96\n",
            "Epoch: 529, Validation Loss: 18.38, Validation Accuracy: 0.96\n",
            "Epoch: 529, Testing Loss: 18.38, Testing Accuracy: 0.95\n",
            "Epoch: 530, Training Loss: 18.37, Training Accuracy: 0.96\n",
            "Epoch: 530, Validation Loss: 18.37, Validation Accuracy: 0.97\n",
            "Epoch: 530, Testing Loss: 18.37, Testing Accuracy: 0.95\n",
            "Epoch: 531, Training Loss: 18.35, Training Accuracy: 0.96\n",
            "Epoch: 531, Validation Loss: 18.35, Validation Accuracy: 0.97\n",
            "Epoch: 531, Testing Loss: 18.35, Testing Accuracy: 0.95\n",
            "Epoch: 532, Training Loss: 18.34, Training Accuracy: 0.96\n",
            "Epoch: 532, Validation Loss: 18.34, Validation Accuracy: 0.97\n",
            "Epoch: 532, Testing Loss: 18.34, Testing Accuracy: 0.95\n",
            "Epoch: 533, Training Loss: 18.32, Training Accuracy: 0.96\n",
            "Epoch: 533, Validation Loss: 18.32, Validation Accuracy: 0.97\n",
            "Epoch: 533, Testing Loss: 18.32, Testing Accuracy: 0.95\n",
            "Epoch: 534, Training Loss: 18.31, Training Accuracy: 0.96\n",
            "Epoch: 534, Validation Loss: 18.31, Validation Accuracy: 0.97\n",
            "Epoch: 534, Testing Loss: 18.31, Testing Accuracy: 0.95\n",
            "Epoch: 535, Training Loss: 18.29, Training Accuracy: 0.96\n",
            "Epoch: 535, Validation Loss: 18.29, Validation Accuracy: 0.97\n",
            "Epoch: 535, Testing Loss: 18.29, Testing Accuracy: 0.95\n",
            "Epoch: 536, Training Loss: 18.28, Training Accuracy: 0.96\n",
            "Epoch: 536, Validation Loss: 18.28, Validation Accuracy: 0.97\n",
            "Epoch: 536, Testing Loss: 18.28, Testing Accuracy: 0.95\n",
            "Epoch: 537, Training Loss: 18.26, Training Accuracy: 0.96\n",
            "Epoch: 537, Validation Loss: 18.26, Validation Accuracy: 0.97\n",
            "Epoch: 537, Testing Loss: 18.26, Testing Accuracy: 0.95\n",
            "Epoch: 538, Training Loss: 18.25, Training Accuracy: 0.96\n",
            "Epoch: 538, Validation Loss: 18.25, Validation Accuracy: 0.97\n",
            "Epoch: 538, Testing Loss: 18.25, Testing Accuracy: 0.95\n",
            "Epoch: 539, Training Loss: 18.23, Training Accuracy: 0.96\n",
            "Epoch: 539, Validation Loss: 18.23, Validation Accuracy: 0.97\n",
            "Epoch: 539, Testing Loss: 18.23, Testing Accuracy: 0.95\n",
            "Epoch: 540, Training Loss: 18.22, Training Accuracy: 0.96\n",
            "Epoch: 540, Validation Loss: 18.22, Validation Accuracy: 0.97\n",
            "Epoch: 540, Testing Loss: 18.22, Testing Accuracy: 0.95\n",
            "Epoch: 541, Training Loss: 18.20, Training Accuracy: 0.96\n",
            "Epoch: 541, Validation Loss: 18.20, Validation Accuracy: 0.97\n",
            "Epoch: 541, Testing Loss: 18.20, Testing Accuracy: 0.95\n",
            "Epoch: 542, Training Loss: 18.19, Training Accuracy: 0.96\n",
            "Epoch: 542, Validation Loss: 18.19, Validation Accuracy: 0.97\n",
            "Epoch: 542, Testing Loss: 18.19, Testing Accuracy: 0.95\n",
            "Epoch: 543, Training Loss: 18.17, Training Accuracy: 0.96\n",
            "Epoch: 543, Validation Loss: 18.17, Validation Accuracy: 0.97\n",
            "Epoch: 543, Testing Loss: 18.17, Testing Accuracy: 0.95\n",
            "Epoch: 544, Training Loss: 18.16, Training Accuracy: 0.96\n",
            "Epoch: 544, Validation Loss: 18.16, Validation Accuracy: 0.97\n",
            "Epoch: 544, Testing Loss: 18.16, Testing Accuracy: 0.95\n",
            "Epoch: 545, Training Loss: 18.15, Training Accuracy: 0.96\n",
            "Epoch: 545, Validation Loss: 18.15, Validation Accuracy: 0.97\n",
            "Epoch: 545, Testing Loss: 18.15, Testing Accuracy: 0.95\n",
            "Epoch: 546, Training Loss: 18.13, Training Accuracy: 0.96\n",
            "Epoch: 546, Validation Loss: 18.13, Validation Accuracy: 0.97\n",
            "Epoch: 546, Testing Loss: 18.13, Testing Accuracy: 0.95\n",
            "Epoch: 547, Training Loss: 18.12, Training Accuracy: 0.96\n",
            "Epoch: 547, Validation Loss: 18.12, Validation Accuracy: 0.97\n",
            "Epoch: 547, Testing Loss: 18.12, Testing Accuracy: 0.95\n",
            "Epoch: 548, Training Loss: 18.10, Training Accuracy: 0.96\n",
            "Epoch: 548, Validation Loss: 18.10, Validation Accuracy: 0.97\n",
            "Epoch: 548, Testing Loss: 18.10, Testing Accuracy: 0.95\n",
            "Epoch: 549, Training Loss: 18.09, Training Accuracy: 0.96\n",
            "Epoch: 549, Validation Loss: 18.09, Validation Accuracy: 0.97\n",
            "Epoch: 549, Testing Loss: 18.09, Testing Accuracy: 0.95\n",
            "Epoch: 550, Training Loss: 18.07, Training Accuracy: 0.96\n",
            "Epoch: 550, Validation Loss: 18.07, Validation Accuracy: 0.97\n",
            "Epoch: 550, Testing Loss: 18.07, Testing Accuracy: 0.95\n",
            "Epoch: 551, Training Loss: 18.06, Training Accuracy: 0.96\n",
            "Epoch: 551, Validation Loss: 18.06, Validation Accuracy: 0.97\n",
            "Epoch: 551, Testing Loss: 18.06, Testing Accuracy: 0.95\n",
            "Epoch: 552, Training Loss: 18.05, Training Accuracy: 0.96\n",
            "Epoch: 552, Validation Loss: 18.05, Validation Accuracy: 0.97\n",
            "Epoch: 552, Testing Loss: 18.05, Testing Accuracy: 0.95\n",
            "Epoch: 553, Training Loss: 18.03, Training Accuracy: 0.96\n",
            "Epoch: 553, Validation Loss: 18.03, Validation Accuracy: 0.97\n",
            "Epoch: 553, Testing Loss: 18.03, Testing Accuracy: 0.95\n",
            "Epoch: 554, Training Loss: 18.02, Training Accuracy: 0.96\n",
            "Epoch: 554, Validation Loss: 18.02, Validation Accuracy: 0.97\n",
            "Epoch: 554, Testing Loss: 18.02, Testing Accuracy: 0.95\n",
            "Epoch: 555, Training Loss: 18.00, Training Accuracy: 0.96\n",
            "Epoch: 555, Validation Loss: 18.00, Validation Accuracy: 0.97\n",
            "Epoch: 555, Testing Loss: 18.00, Testing Accuracy: 0.95\n",
            "Epoch: 556, Training Loss: 17.99, Training Accuracy: 0.96\n",
            "Epoch: 556, Validation Loss: 17.99, Validation Accuracy: 0.97\n",
            "Epoch: 556, Testing Loss: 17.99, Testing Accuracy: 0.95\n",
            "Epoch: 557, Training Loss: 17.98, Training Accuracy: 0.96\n",
            "Epoch: 557, Validation Loss: 17.98, Validation Accuracy: 0.97\n",
            "Epoch: 557, Testing Loss: 17.98, Testing Accuracy: 0.95\n",
            "Epoch: 558, Training Loss: 17.96, Training Accuracy: 0.96\n",
            "Epoch: 558, Validation Loss: 17.96, Validation Accuracy: 0.97\n",
            "Epoch: 558, Testing Loss: 17.96, Testing Accuracy: 0.95\n",
            "Epoch: 559, Training Loss: 17.95, Training Accuracy: 0.96\n",
            "Epoch: 559, Validation Loss: 17.95, Validation Accuracy: 0.97\n",
            "Epoch: 559, Testing Loss: 17.95, Testing Accuracy: 0.95\n",
            "Epoch: 560, Training Loss: 17.93, Training Accuracy: 0.96\n",
            "Epoch: 560, Validation Loss: 17.93, Validation Accuracy: 0.97\n",
            "Epoch: 560, Testing Loss: 17.93, Testing Accuracy: 0.95\n",
            "Epoch: 561, Training Loss: 17.92, Training Accuracy: 0.96\n",
            "Epoch: 561, Validation Loss: 17.92, Validation Accuracy: 0.97\n",
            "Epoch: 561, Testing Loss: 17.92, Testing Accuracy: 0.95\n",
            "Epoch: 562, Training Loss: 17.91, Training Accuracy: 0.96\n",
            "Epoch: 562, Validation Loss: 17.91, Validation Accuracy: 0.97\n",
            "Epoch: 562, Testing Loss: 17.91, Testing Accuracy: 0.95\n",
            "Epoch: 563, Training Loss: 17.89, Training Accuracy: 0.96\n",
            "Epoch: 563, Validation Loss: 17.89, Validation Accuracy: 0.97\n",
            "Epoch: 563, Testing Loss: 17.89, Testing Accuracy: 0.95\n",
            "Epoch: 564, Training Loss: 17.88, Training Accuracy: 0.96\n",
            "Epoch: 564, Validation Loss: 17.88, Validation Accuracy: 0.97\n",
            "Epoch: 564, Testing Loss: 17.88, Testing Accuracy: 0.95\n",
            "Epoch: 565, Training Loss: 17.87, Training Accuracy: 0.96\n",
            "Epoch: 565, Validation Loss: 17.87, Validation Accuracy: 0.97\n",
            "Epoch: 565, Testing Loss: 17.87, Testing Accuracy: 0.95\n",
            "Epoch: 566, Training Loss: 17.85, Training Accuracy: 0.96\n",
            "Epoch: 566, Validation Loss: 17.85, Validation Accuracy: 0.97\n",
            "Epoch: 566, Testing Loss: 17.85, Testing Accuracy: 0.95\n",
            "Epoch: 567, Training Loss: 17.84, Training Accuracy: 0.96\n",
            "Epoch: 567, Validation Loss: 17.84, Validation Accuracy: 0.97\n",
            "Epoch: 567, Testing Loss: 17.84, Testing Accuracy: 0.95\n",
            "Epoch: 568, Training Loss: 17.83, Training Accuracy: 0.96\n",
            "Epoch: 568, Validation Loss: 17.83, Validation Accuracy: 0.97\n",
            "Epoch: 568, Testing Loss: 17.83, Testing Accuracy: 0.95\n",
            "Epoch: 569, Training Loss: 17.81, Training Accuracy: 0.96\n",
            "Epoch: 569, Validation Loss: 17.81, Validation Accuracy: 0.97\n",
            "Epoch: 569, Testing Loss: 17.81, Testing Accuracy: 0.95\n",
            "Epoch: 570, Training Loss: 17.80, Training Accuracy: 0.96\n",
            "Epoch: 570, Validation Loss: 17.80, Validation Accuracy: 0.97\n",
            "Epoch: 570, Testing Loss: 17.80, Testing Accuracy: 0.95\n",
            "Epoch: 571, Training Loss: 17.78, Training Accuracy: 0.96\n",
            "Epoch: 571, Validation Loss: 17.78, Validation Accuracy: 0.97\n",
            "Epoch: 571, Testing Loss: 17.78, Testing Accuracy: 0.95\n",
            "Epoch: 572, Training Loss: 17.77, Training Accuracy: 0.96\n",
            "Epoch: 572, Validation Loss: 17.77, Validation Accuracy: 0.97\n",
            "Epoch: 572, Testing Loss: 17.77, Testing Accuracy: 0.95\n",
            "Epoch: 573, Training Loss: 17.76, Training Accuracy: 0.96\n",
            "Epoch: 573, Validation Loss: 17.76, Validation Accuracy: 0.97\n",
            "Epoch: 573, Testing Loss: 17.76, Testing Accuracy: 0.95\n",
            "Epoch: 574, Training Loss: 17.75, Training Accuracy: 0.96\n",
            "Epoch: 574, Validation Loss: 17.75, Validation Accuracy: 0.97\n",
            "Epoch: 574, Testing Loss: 17.75, Testing Accuracy: 0.95\n",
            "Epoch: 575, Training Loss: 17.73, Training Accuracy: 0.96\n",
            "Epoch: 575, Validation Loss: 17.73, Validation Accuracy: 0.97\n",
            "Epoch: 575, Testing Loss: 17.73, Testing Accuracy: 0.95\n",
            "Epoch: 576, Training Loss: 17.72, Training Accuracy: 0.96\n",
            "Epoch: 576, Validation Loss: 17.72, Validation Accuracy: 0.97\n",
            "Epoch: 576, Testing Loss: 17.72, Testing Accuracy: 0.95\n",
            "Epoch: 577, Training Loss: 17.71, Training Accuracy: 0.96\n",
            "Epoch: 577, Validation Loss: 17.71, Validation Accuracy: 0.97\n",
            "Epoch: 577, Testing Loss: 17.71, Testing Accuracy: 0.95\n",
            "Epoch: 578, Training Loss: 17.69, Training Accuracy: 0.96\n",
            "Epoch: 578, Validation Loss: 17.69, Validation Accuracy: 0.97\n",
            "Epoch: 578, Testing Loss: 17.69, Testing Accuracy: 0.95\n",
            "Epoch: 579, Training Loss: 17.68, Training Accuracy: 0.96\n",
            "Epoch: 579, Validation Loss: 17.68, Validation Accuracy: 0.97\n",
            "Epoch: 579, Testing Loss: 17.68, Testing Accuracy: 0.95\n",
            "Epoch: 580, Training Loss: 17.67, Training Accuracy: 0.96\n",
            "Epoch: 580, Validation Loss: 17.67, Validation Accuracy: 0.97\n",
            "Epoch: 580, Testing Loss: 17.67, Testing Accuracy: 0.95\n",
            "Epoch: 581, Training Loss: 17.65, Training Accuracy: 0.96\n",
            "Epoch: 581, Validation Loss: 17.65, Validation Accuracy: 0.97\n",
            "Epoch: 581, Testing Loss: 17.65, Testing Accuracy: 0.95\n",
            "Epoch: 582, Training Loss: 17.64, Training Accuracy: 0.96\n",
            "Epoch: 582, Validation Loss: 17.64, Validation Accuracy: 0.97\n",
            "Epoch: 582, Testing Loss: 17.64, Testing Accuracy: 0.95\n",
            "Epoch: 583, Training Loss: 17.63, Training Accuracy: 0.96\n",
            "Epoch: 583, Validation Loss: 17.63, Validation Accuracy: 0.97\n",
            "Epoch: 583, Testing Loss: 17.63, Testing Accuracy: 0.95\n",
            "Epoch: 584, Training Loss: 17.61, Training Accuracy: 0.96\n",
            "Epoch: 584, Validation Loss: 17.61, Validation Accuracy: 0.97\n",
            "Epoch: 584, Testing Loss: 17.61, Testing Accuracy: 0.95\n",
            "Epoch: 585, Training Loss: 17.60, Training Accuracy: 0.96\n",
            "Epoch: 585, Validation Loss: 17.60, Validation Accuracy: 0.97\n",
            "Epoch: 585, Testing Loss: 17.60, Testing Accuracy: 0.95\n",
            "Epoch: 586, Training Loss: 17.59, Training Accuracy: 0.96\n",
            "Epoch: 586, Validation Loss: 17.59, Validation Accuracy: 0.97\n",
            "Epoch: 586, Testing Loss: 17.59, Testing Accuracy: 0.95\n",
            "Epoch: 587, Training Loss: 17.58, Training Accuracy: 0.96\n",
            "Epoch: 587, Validation Loss: 17.58, Validation Accuracy: 0.97\n",
            "Epoch: 587, Testing Loss: 17.58, Testing Accuracy: 0.95\n",
            "Epoch: 588, Training Loss: 17.56, Training Accuracy: 0.96\n",
            "Epoch: 588, Validation Loss: 17.56, Validation Accuracy: 0.97\n",
            "Epoch: 588, Testing Loss: 17.56, Testing Accuracy: 0.95\n",
            "Epoch: 589, Training Loss: 17.55, Training Accuracy: 0.96\n",
            "Epoch: 589, Validation Loss: 17.55, Validation Accuracy: 0.97\n",
            "Epoch: 589, Testing Loss: 17.55, Testing Accuracy: 0.95\n",
            "Epoch: 590, Training Loss: 17.54, Training Accuracy: 0.96\n",
            "Epoch: 590, Validation Loss: 17.54, Validation Accuracy: 0.97\n",
            "Epoch: 590, Testing Loss: 17.54, Testing Accuracy: 0.95\n",
            "Epoch: 591, Training Loss: 17.53, Training Accuracy: 0.96\n",
            "Epoch: 591, Validation Loss: 17.53, Validation Accuracy: 0.97\n",
            "Epoch: 591, Testing Loss: 17.53, Testing Accuracy: 0.95\n",
            "Epoch: 592, Training Loss: 17.51, Training Accuracy: 0.96\n",
            "Epoch: 592, Validation Loss: 17.51, Validation Accuracy: 0.97\n",
            "Epoch: 592, Testing Loss: 17.51, Testing Accuracy: 0.95\n",
            "Epoch: 593, Training Loss: 17.50, Training Accuracy: 0.96\n",
            "Epoch: 593, Validation Loss: 17.50, Validation Accuracy: 0.97\n",
            "Epoch: 593, Testing Loss: 17.50, Testing Accuracy: 0.95\n",
            "Epoch: 594, Training Loss: 17.49, Training Accuracy: 0.96\n",
            "Epoch: 594, Validation Loss: 17.49, Validation Accuracy: 0.97\n",
            "Epoch: 594, Testing Loss: 17.49, Testing Accuracy: 0.95\n",
            "Epoch: 595, Training Loss: 17.48, Training Accuracy: 0.96\n",
            "Epoch: 595, Validation Loss: 17.48, Validation Accuracy: 0.97\n",
            "Epoch: 595, Testing Loss: 17.48, Testing Accuracy: 0.95\n",
            "Epoch: 596, Training Loss: 17.46, Training Accuracy: 0.96\n",
            "Epoch: 596, Validation Loss: 17.46, Validation Accuracy: 0.97\n",
            "Epoch: 596, Testing Loss: 17.46, Testing Accuracy: 0.95\n",
            "Epoch: 597, Training Loss: 17.45, Training Accuracy: 0.96\n",
            "Epoch: 597, Validation Loss: 17.45, Validation Accuracy: 0.97\n",
            "Epoch: 597, Testing Loss: 17.45, Testing Accuracy: 0.95\n",
            "Epoch: 598, Training Loss: 17.44, Training Accuracy: 0.96\n",
            "Epoch: 598, Validation Loss: 17.44, Validation Accuracy: 0.97\n",
            "Epoch: 598, Testing Loss: 17.44, Testing Accuracy: 0.95\n",
            "Epoch: 599, Training Loss: 17.43, Training Accuracy: 0.96\n",
            "Epoch: 599, Validation Loss: 17.43, Validation Accuracy: 0.97\n",
            "Epoch: 599, Testing Loss: 17.43, Testing Accuracy: 0.95\n",
            "Epoch: 600, Training Loss: 17.41, Training Accuracy: 0.96\n",
            "Epoch: 600, Validation Loss: 17.41, Validation Accuracy: 0.97\n",
            "Epoch: 600, Testing Loss: 17.41, Testing Accuracy: 0.95\n",
            "Epoch: 601, Training Loss: 17.40, Training Accuracy: 0.96\n",
            "Epoch: 601, Validation Loss: 17.40, Validation Accuracy: 0.97\n",
            "Epoch: 601, Testing Loss: 17.40, Testing Accuracy: 0.95\n",
            "Epoch: 602, Training Loss: 17.39, Training Accuracy: 0.96\n",
            "Epoch: 602, Validation Loss: 17.39, Validation Accuracy: 0.97\n",
            "Epoch: 602, Testing Loss: 17.39, Testing Accuracy: 0.95\n",
            "Epoch: 603, Training Loss: 17.38, Training Accuracy: 0.96\n",
            "Epoch: 603, Validation Loss: 17.38, Validation Accuracy: 0.97\n",
            "Epoch: 603, Testing Loss: 17.38, Testing Accuracy: 0.95\n",
            "Epoch: 604, Training Loss: 17.37, Training Accuracy: 0.96\n",
            "Epoch: 604, Validation Loss: 17.37, Validation Accuracy: 0.97\n",
            "Epoch: 604, Testing Loss: 17.37, Testing Accuracy: 0.95\n",
            "Epoch: 605, Training Loss: 17.35, Training Accuracy: 0.96\n",
            "Epoch: 605, Validation Loss: 17.35, Validation Accuracy: 0.97\n",
            "Epoch: 605, Testing Loss: 17.35, Testing Accuracy: 0.95\n",
            "Epoch: 606, Training Loss: 17.34, Training Accuracy: 0.96\n",
            "Epoch: 606, Validation Loss: 17.34, Validation Accuracy: 0.97\n",
            "Epoch: 606, Testing Loss: 17.34, Testing Accuracy: 0.95\n",
            "Epoch: 607, Training Loss: 17.33, Training Accuracy: 0.96\n",
            "Epoch: 607, Validation Loss: 17.33, Validation Accuracy: 0.97\n",
            "Epoch: 607, Testing Loss: 17.33, Testing Accuracy: 0.95\n",
            "Epoch: 608, Training Loss: 17.32, Training Accuracy: 0.96\n",
            "Epoch: 608, Validation Loss: 17.32, Validation Accuracy: 0.97\n",
            "Epoch: 608, Testing Loss: 17.32, Testing Accuracy: 0.95\n",
            "Epoch: 609, Training Loss: 17.30, Training Accuracy: 0.96\n",
            "Epoch: 609, Validation Loss: 17.30, Validation Accuracy: 0.97\n",
            "Epoch: 609, Testing Loss: 17.30, Testing Accuracy: 0.95\n",
            "Epoch: 610, Training Loss: 17.29, Training Accuracy: 0.96\n",
            "Epoch: 610, Validation Loss: 17.29, Validation Accuracy: 0.97\n",
            "Epoch: 610, Testing Loss: 17.29, Testing Accuracy: 0.95\n",
            "Epoch: 611, Training Loss: 17.28, Training Accuracy: 0.96\n",
            "Epoch: 611, Validation Loss: 17.28, Validation Accuracy: 0.97\n",
            "Epoch: 611, Testing Loss: 17.28, Testing Accuracy: 0.95\n",
            "Epoch: 612, Training Loss: 17.27, Training Accuracy: 0.96\n",
            "Epoch: 612, Validation Loss: 17.27, Validation Accuracy: 0.97\n",
            "Epoch: 612, Testing Loss: 17.27, Testing Accuracy: 0.95\n",
            "Epoch: 613, Training Loss: 17.26, Training Accuracy: 0.96\n",
            "Epoch: 613, Validation Loss: 17.26, Validation Accuracy: 0.97\n",
            "Epoch: 613, Testing Loss: 17.26, Testing Accuracy: 0.95\n",
            "Epoch: 614, Training Loss: 17.25, Training Accuracy: 0.96\n",
            "Epoch: 614, Validation Loss: 17.25, Validation Accuracy: 0.97\n",
            "Epoch: 614, Testing Loss: 17.25, Testing Accuracy: 0.95\n",
            "Epoch: 615, Training Loss: 17.23, Training Accuracy: 0.96\n",
            "Epoch: 615, Validation Loss: 17.23, Validation Accuracy: 0.97\n",
            "Epoch: 615, Testing Loss: 17.23, Testing Accuracy: 0.95\n",
            "Epoch: 616, Training Loss: 17.22, Training Accuracy: 0.96\n",
            "Epoch: 616, Validation Loss: 17.22, Validation Accuracy: 0.97\n",
            "Epoch: 616, Testing Loss: 17.22, Testing Accuracy: 0.95\n",
            "Epoch: 617, Training Loss: 17.21, Training Accuracy: 0.96\n",
            "Epoch: 617, Validation Loss: 17.21, Validation Accuracy: 0.97\n",
            "Epoch: 617, Testing Loss: 17.21, Testing Accuracy: 0.95\n",
            "Epoch: 618, Training Loss: 17.20, Training Accuracy: 0.96\n",
            "Epoch: 618, Validation Loss: 17.20, Validation Accuracy: 0.97\n",
            "Epoch: 618, Testing Loss: 17.20, Testing Accuracy: 0.95\n",
            "Epoch: 619, Training Loss: 17.19, Training Accuracy: 0.96\n",
            "Epoch: 619, Validation Loss: 17.19, Validation Accuracy: 0.97\n",
            "Epoch: 619, Testing Loss: 17.19, Testing Accuracy: 0.95\n",
            "Epoch: 620, Training Loss: 17.18, Training Accuracy: 0.96\n",
            "Epoch: 620, Validation Loss: 17.18, Validation Accuracy: 0.97\n",
            "Epoch: 620, Testing Loss: 17.18, Testing Accuracy: 0.95\n",
            "Epoch: 621, Training Loss: 17.16, Training Accuracy: 0.96\n",
            "Epoch: 621, Validation Loss: 17.16, Validation Accuracy: 0.97\n",
            "Epoch: 621, Testing Loss: 17.16, Testing Accuracy: 0.95\n",
            "Epoch: 622, Training Loss: 17.15, Training Accuracy: 0.96\n",
            "Epoch: 622, Validation Loss: 17.15, Validation Accuracy: 0.97\n",
            "Epoch: 622, Testing Loss: 17.15, Testing Accuracy: 0.95\n",
            "Epoch: 623, Training Loss: 17.14, Training Accuracy: 0.96\n",
            "Epoch: 623, Validation Loss: 17.14, Validation Accuracy: 0.97\n",
            "Epoch: 623, Testing Loss: 17.14, Testing Accuracy: 0.95\n",
            "Epoch: 624, Training Loss: 17.13, Training Accuracy: 0.96\n",
            "Epoch: 624, Validation Loss: 17.13, Validation Accuracy: 0.97\n",
            "Epoch: 624, Testing Loss: 17.13, Testing Accuracy: 0.95\n",
            "Epoch: 625, Training Loss: 17.12, Training Accuracy: 0.96\n",
            "Epoch: 625, Validation Loss: 17.12, Validation Accuracy: 0.97\n",
            "Epoch: 625, Testing Loss: 17.12, Testing Accuracy: 0.95\n",
            "Epoch: 626, Training Loss: 17.11, Training Accuracy: 0.96\n",
            "Epoch: 626, Validation Loss: 17.11, Validation Accuracy: 0.97\n",
            "Epoch: 626, Testing Loss: 17.11, Testing Accuracy: 0.95\n",
            "Epoch: 627, Training Loss: 17.09, Training Accuracy: 0.96\n",
            "Epoch: 627, Validation Loss: 17.09, Validation Accuracy: 0.97\n",
            "Epoch: 627, Testing Loss: 17.09, Testing Accuracy: 0.95\n",
            "Epoch: 628, Training Loss: 17.08, Training Accuracy: 0.96\n",
            "Epoch: 628, Validation Loss: 17.08, Validation Accuracy: 0.97\n",
            "Epoch: 628, Testing Loss: 17.08, Testing Accuracy: 0.95\n",
            "Epoch: 629, Training Loss: 17.07, Training Accuracy: 0.96\n",
            "Epoch: 629, Validation Loss: 17.07, Validation Accuracy: 0.97\n",
            "Epoch: 629, Testing Loss: 17.07, Testing Accuracy: 0.95\n",
            "Epoch: 630, Training Loss: 17.06, Training Accuracy: 0.96\n",
            "Epoch: 630, Validation Loss: 17.06, Validation Accuracy: 0.97\n",
            "Epoch: 630, Testing Loss: 17.06, Testing Accuracy: 0.95\n",
            "Epoch: 631, Training Loss: 17.05, Training Accuracy: 0.96\n",
            "Epoch: 631, Validation Loss: 17.05, Validation Accuracy: 0.97\n",
            "Epoch: 631, Testing Loss: 17.05, Testing Accuracy: 0.95\n",
            "Epoch: 632, Training Loss: 17.04, Training Accuracy: 0.96\n",
            "Epoch: 632, Validation Loss: 17.04, Validation Accuracy: 0.97\n",
            "Epoch: 632, Testing Loss: 17.04, Testing Accuracy: 0.95\n",
            "Epoch: 633, Training Loss: 17.03, Training Accuracy: 0.96\n",
            "Epoch: 633, Validation Loss: 17.03, Validation Accuracy: 0.97\n",
            "Epoch: 633, Testing Loss: 17.03, Testing Accuracy: 0.95\n",
            "Epoch: 634, Training Loss: 17.02, Training Accuracy: 0.96\n",
            "Epoch: 634, Validation Loss: 17.02, Validation Accuracy: 0.97\n",
            "Epoch: 634, Testing Loss: 17.02, Testing Accuracy: 0.95\n",
            "Epoch: 635, Training Loss: 17.00, Training Accuracy: 0.96\n",
            "Epoch: 635, Validation Loss: 17.00, Validation Accuracy: 0.97\n",
            "Epoch: 635, Testing Loss: 17.00, Testing Accuracy: 0.95\n",
            "Epoch: 636, Training Loss: 16.99, Training Accuracy: 0.96\n",
            "Epoch: 636, Validation Loss: 16.99, Validation Accuracy: 0.97\n",
            "Epoch: 636, Testing Loss: 16.99, Testing Accuracy: 0.95\n",
            "Epoch: 637, Training Loss: 16.98, Training Accuracy: 0.96\n",
            "Epoch: 637, Validation Loss: 16.98, Validation Accuracy: 0.97\n",
            "Epoch: 637, Testing Loss: 16.98, Testing Accuracy: 0.95\n",
            "Epoch: 638, Training Loss: 16.97, Training Accuracy: 0.96\n",
            "Epoch: 638, Validation Loss: 16.97, Validation Accuracy: 0.97\n",
            "Epoch: 638, Testing Loss: 16.97, Testing Accuracy: 0.95\n",
            "Epoch: 639, Training Loss: 16.96, Training Accuracy: 0.96\n",
            "Epoch: 639, Validation Loss: 16.96, Validation Accuracy: 0.97\n",
            "Epoch: 639, Testing Loss: 16.96, Testing Accuracy: 0.95\n",
            "Epoch: 640, Training Loss: 16.95, Training Accuracy: 0.96\n",
            "Epoch: 640, Validation Loss: 16.95, Validation Accuracy: 0.97\n",
            "Epoch: 640, Testing Loss: 16.95, Testing Accuracy: 0.95\n",
            "Epoch: 641, Training Loss: 16.94, Training Accuracy: 0.96\n",
            "Epoch: 641, Validation Loss: 16.94, Validation Accuracy: 0.97\n",
            "Epoch: 641, Testing Loss: 16.94, Testing Accuracy: 0.95\n",
            "Epoch: 642, Training Loss: 16.93, Training Accuracy: 0.96\n",
            "Epoch: 642, Validation Loss: 16.93, Validation Accuracy: 0.97\n",
            "Epoch: 642, Testing Loss: 16.93, Testing Accuracy: 0.95\n",
            "Epoch: 643, Training Loss: 16.92, Training Accuracy: 0.96\n",
            "Epoch: 643, Validation Loss: 16.92, Validation Accuracy: 0.97\n",
            "Epoch: 643, Testing Loss: 16.92, Testing Accuracy: 0.95\n",
            "Epoch: 644, Training Loss: 16.91, Training Accuracy: 0.96\n",
            "Epoch: 644, Validation Loss: 16.91, Validation Accuracy: 0.97\n",
            "Epoch: 644, Testing Loss: 16.91, Testing Accuracy: 0.95\n",
            "Epoch: 645, Training Loss: 16.89, Training Accuracy: 0.96\n",
            "Epoch: 645, Validation Loss: 16.89, Validation Accuracy: 0.97\n",
            "Epoch: 645, Testing Loss: 16.89, Testing Accuracy: 0.95\n",
            "Epoch: 646, Training Loss: 16.88, Training Accuracy: 0.96\n",
            "Epoch: 646, Validation Loss: 16.88, Validation Accuracy: 0.97\n",
            "Epoch: 646, Testing Loss: 16.88, Testing Accuracy: 0.95\n",
            "Epoch: 647, Training Loss: 16.87, Training Accuracy: 0.96\n",
            "Epoch: 647, Validation Loss: 16.87, Validation Accuracy: 0.97\n",
            "Epoch: 647, Testing Loss: 16.87, Testing Accuracy: 0.95\n",
            "Epoch: 648, Training Loss: 16.86, Training Accuracy: 0.96\n",
            "Epoch: 648, Validation Loss: 16.86, Validation Accuracy: 0.97\n",
            "Epoch: 648, Testing Loss: 16.86, Testing Accuracy: 0.95\n",
            "Epoch: 649, Training Loss: 16.85, Training Accuracy: 0.96\n",
            "Epoch: 649, Validation Loss: 16.85, Validation Accuracy: 0.97\n",
            "Epoch: 649, Testing Loss: 16.85, Testing Accuracy: 0.95\n",
            "Epoch: 650, Training Loss: 16.84, Training Accuracy: 0.96\n",
            "Epoch: 650, Validation Loss: 16.84, Validation Accuracy: 0.97\n",
            "Epoch: 650, Testing Loss: 16.84, Testing Accuracy: 0.95\n",
            "Epoch: 651, Training Loss: 16.83, Training Accuracy: 0.96\n",
            "Epoch: 651, Validation Loss: 16.83, Validation Accuracy: 0.97\n",
            "Epoch: 651, Testing Loss: 16.83, Testing Accuracy: 0.95\n",
            "Epoch: 652, Training Loss: 16.82, Training Accuracy: 0.96\n",
            "Epoch: 652, Validation Loss: 16.82, Validation Accuracy: 0.97\n",
            "Epoch: 652, Testing Loss: 16.82, Testing Accuracy: 0.95\n",
            "Epoch: 653, Training Loss: 16.81, Training Accuracy: 0.96\n",
            "Epoch: 653, Validation Loss: 16.81, Validation Accuracy: 0.97\n",
            "Epoch: 653, Testing Loss: 16.81, Testing Accuracy: 0.95\n",
            "Epoch: 654, Training Loss: 16.80, Training Accuracy: 0.96\n",
            "Epoch: 654, Validation Loss: 16.80, Validation Accuracy: 0.97\n",
            "Epoch: 654, Testing Loss: 16.80, Testing Accuracy: 0.95\n",
            "Epoch: 655, Training Loss: 16.79, Training Accuracy: 0.96\n",
            "Epoch: 655, Validation Loss: 16.79, Validation Accuracy: 0.97\n",
            "Epoch: 655, Testing Loss: 16.79, Testing Accuracy: 0.95\n",
            "Epoch: 656, Training Loss: 16.78, Training Accuracy: 0.96\n",
            "Epoch: 656, Validation Loss: 16.78, Validation Accuracy: 0.97\n",
            "Epoch: 656, Testing Loss: 16.78, Testing Accuracy: 0.95\n",
            "Epoch: 657, Training Loss: 16.77, Training Accuracy: 0.96\n",
            "Epoch: 657, Validation Loss: 16.77, Validation Accuracy: 0.97\n",
            "Epoch: 657, Testing Loss: 16.77, Testing Accuracy: 0.95\n",
            "Epoch: 658, Training Loss: 16.76, Training Accuracy: 0.96\n",
            "Epoch: 658, Validation Loss: 16.76, Validation Accuracy: 0.97\n",
            "Epoch: 658, Testing Loss: 16.76, Testing Accuracy: 0.95\n",
            "Epoch: 659, Training Loss: 16.74, Training Accuracy: 0.96\n",
            "Epoch: 659, Validation Loss: 16.74, Validation Accuracy: 0.97\n",
            "Epoch: 659, Testing Loss: 16.74, Testing Accuracy: 0.95\n",
            "Epoch: 660, Training Loss: 16.73, Training Accuracy: 0.96\n",
            "Epoch: 660, Validation Loss: 16.73, Validation Accuracy: 0.97\n",
            "Epoch: 660, Testing Loss: 16.73, Testing Accuracy: 0.95\n",
            "Epoch: 661, Training Loss: 16.72, Training Accuracy: 0.96\n",
            "Epoch: 661, Validation Loss: 16.72, Validation Accuracy: 0.97\n",
            "Epoch: 661, Testing Loss: 16.72, Testing Accuracy: 0.95\n",
            "Epoch: 662, Training Loss: 16.71, Training Accuracy: 0.96\n",
            "Epoch: 662, Validation Loss: 16.71, Validation Accuracy: 0.97\n",
            "Epoch: 662, Testing Loss: 16.71, Testing Accuracy: 0.95\n",
            "Epoch: 663, Training Loss: 16.70, Training Accuracy: 0.96\n",
            "Epoch: 663, Validation Loss: 16.70, Validation Accuracy: 0.97\n",
            "Epoch: 663, Testing Loss: 16.70, Testing Accuracy: 0.95\n",
            "Epoch: 664, Training Loss: 16.69, Training Accuracy: 0.96\n",
            "Epoch: 664, Validation Loss: 16.69, Validation Accuracy: 0.97\n",
            "Epoch: 664, Testing Loss: 16.69, Testing Accuracy: 0.95\n",
            "Epoch: 665, Training Loss: 16.68, Training Accuracy: 0.96\n",
            "Epoch: 665, Validation Loss: 16.68, Validation Accuracy: 0.97\n",
            "Epoch: 665, Testing Loss: 16.68, Testing Accuracy: 0.95\n",
            "Epoch: 666, Training Loss: 16.67, Training Accuracy: 0.96\n",
            "Epoch: 666, Validation Loss: 16.67, Validation Accuracy: 0.97\n",
            "Epoch: 666, Testing Loss: 16.67, Testing Accuracy: 0.95\n",
            "Epoch: 667, Training Loss: 16.66, Training Accuracy: 0.96\n",
            "Epoch: 667, Validation Loss: 16.66, Validation Accuracy: 0.97\n",
            "Epoch: 667, Testing Loss: 16.66, Testing Accuracy: 0.95\n",
            "Epoch: 668, Training Loss: 16.65, Training Accuracy: 0.96\n",
            "Epoch: 668, Validation Loss: 16.65, Validation Accuracy: 0.97\n",
            "Epoch: 668, Testing Loss: 16.65, Testing Accuracy: 0.95\n",
            "Epoch: 669, Training Loss: 16.64, Training Accuracy: 0.96\n",
            "Epoch: 669, Validation Loss: 16.64, Validation Accuracy: 0.97\n",
            "Epoch: 669, Testing Loss: 16.64, Testing Accuracy: 0.95\n",
            "Epoch: 670, Training Loss: 16.63, Training Accuracy: 0.96\n",
            "Epoch: 670, Validation Loss: 16.63, Validation Accuracy: 0.97\n",
            "Epoch: 670, Testing Loss: 16.63, Testing Accuracy: 0.95\n",
            "Epoch: 671, Training Loss: 16.62, Training Accuracy: 0.96\n",
            "Epoch: 671, Validation Loss: 16.62, Validation Accuracy: 0.97\n",
            "Epoch: 671, Testing Loss: 16.62, Testing Accuracy: 0.95\n",
            "Epoch: 672, Training Loss: 16.61, Training Accuracy: 0.96\n",
            "Epoch: 672, Validation Loss: 16.61, Validation Accuracy: 0.97\n",
            "Epoch: 672, Testing Loss: 16.61, Testing Accuracy: 0.95\n",
            "Epoch: 673, Training Loss: 16.60, Training Accuracy: 0.96\n",
            "Epoch: 673, Validation Loss: 16.60, Validation Accuracy: 0.97\n",
            "Epoch: 673, Testing Loss: 16.60, Testing Accuracy: 0.95\n",
            "Epoch: 674, Training Loss: 16.59, Training Accuracy: 0.96\n",
            "Epoch: 674, Validation Loss: 16.59, Validation Accuracy: 0.97\n",
            "Epoch: 674, Testing Loss: 16.59, Testing Accuracy: 0.95\n",
            "Epoch: 675, Training Loss: 16.58, Training Accuracy: 0.96\n",
            "Epoch: 675, Validation Loss: 16.58, Validation Accuracy: 0.97\n",
            "Epoch: 675, Testing Loss: 16.58, Testing Accuracy: 0.95\n",
            "Epoch: 676, Training Loss: 16.57, Training Accuracy: 0.96\n",
            "Epoch: 676, Validation Loss: 16.57, Validation Accuracy: 0.97\n",
            "Epoch: 676, Testing Loss: 16.57, Testing Accuracy: 0.95\n",
            "Epoch: 677, Training Loss: 16.56, Training Accuracy: 0.96\n",
            "Epoch: 677, Validation Loss: 16.56, Validation Accuracy: 0.97\n",
            "Epoch: 677, Testing Loss: 16.56, Testing Accuracy: 0.95\n",
            "Epoch: 678, Training Loss: 16.55, Training Accuracy: 0.96\n",
            "Epoch: 678, Validation Loss: 16.55, Validation Accuracy: 0.97\n",
            "Epoch: 678, Testing Loss: 16.55, Testing Accuracy: 0.95\n",
            "Epoch: 679, Training Loss: 16.54, Training Accuracy: 0.96\n",
            "Epoch: 679, Validation Loss: 16.54, Validation Accuracy: 0.97\n",
            "Epoch: 679, Testing Loss: 16.54, Testing Accuracy: 0.95\n",
            "Epoch: 680, Training Loss: 16.53, Training Accuracy: 0.96\n",
            "Epoch: 680, Validation Loss: 16.53, Validation Accuracy: 0.97\n",
            "Epoch: 680, Testing Loss: 16.53, Testing Accuracy: 0.95\n",
            "Epoch: 681, Training Loss: 16.52, Training Accuracy: 0.96\n",
            "Epoch: 681, Validation Loss: 16.52, Validation Accuracy: 0.97\n",
            "Epoch: 681, Testing Loss: 16.52, Testing Accuracy: 0.95\n",
            "Epoch: 682, Training Loss: 16.51, Training Accuracy: 0.96\n",
            "Epoch: 682, Validation Loss: 16.51, Validation Accuracy: 0.97\n",
            "Epoch: 682, Testing Loss: 16.51, Testing Accuracy: 0.95\n",
            "Epoch: 683, Training Loss: 16.50, Training Accuracy: 0.96\n",
            "Epoch: 683, Validation Loss: 16.50, Validation Accuracy: 0.97\n",
            "Epoch: 683, Testing Loss: 16.50, Testing Accuracy: 0.95\n",
            "Epoch: 684, Training Loss: 16.49, Training Accuracy: 0.96\n",
            "Epoch: 684, Validation Loss: 16.49, Validation Accuracy: 0.97\n",
            "Epoch: 684, Testing Loss: 16.49, Testing Accuracy: 0.95\n",
            "Epoch: 685, Training Loss: 16.48, Training Accuracy: 0.96\n",
            "Epoch: 685, Validation Loss: 16.48, Validation Accuracy: 0.97\n",
            "Epoch: 685, Testing Loss: 16.48, Testing Accuracy: 0.95\n",
            "Epoch: 686, Training Loss: 16.47, Training Accuracy: 0.96\n",
            "Epoch: 686, Validation Loss: 16.47, Validation Accuracy: 0.97\n",
            "Epoch: 686, Testing Loss: 16.47, Testing Accuracy: 0.95\n",
            "Epoch: 687, Training Loss: 16.46, Training Accuracy: 0.96\n",
            "Epoch: 687, Validation Loss: 16.46, Validation Accuracy: 0.97\n",
            "Epoch: 687, Testing Loss: 16.46, Testing Accuracy: 0.95\n",
            "Epoch: 688, Training Loss: 16.45, Training Accuracy: 0.96\n",
            "Epoch: 688, Validation Loss: 16.45, Validation Accuracy: 0.97\n",
            "Epoch: 688, Testing Loss: 16.45, Testing Accuracy: 0.95\n",
            "Epoch: 689, Training Loss: 16.44, Training Accuracy: 0.96\n",
            "Epoch: 689, Validation Loss: 16.44, Validation Accuracy: 0.97\n",
            "Epoch: 689, Testing Loss: 16.44, Testing Accuracy: 0.95\n",
            "Epoch: 690, Training Loss: 16.43, Training Accuracy: 0.96\n",
            "Epoch: 690, Validation Loss: 16.43, Validation Accuracy: 0.97\n",
            "Epoch: 690, Testing Loss: 16.43, Testing Accuracy: 0.95\n",
            "Epoch: 691, Training Loss: 16.42, Training Accuracy: 0.96\n",
            "Epoch: 691, Validation Loss: 16.42, Validation Accuracy: 0.97\n",
            "Epoch: 691, Testing Loss: 16.42, Testing Accuracy: 0.95\n",
            "Epoch: 692, Training Loss: 16.41, Training Accuracy: 0.96\n",
            "Epoch: 692, Validation Loss: 16.41, Validation Accuracy: 0.97\n",
            "Epoch: 692, Testing Loss: 16.41, Testing Accuracy: 0.95\n",
            "Epoch: 693, Training Loss: 16.40, Training Accuracy: 0.96\n",
            "Epoch: 693, Validation Loss: 16.40, Validation Accuracy: 0.97\n",
            "Epoch: 693, Testing Loss: 16.40, Testing Accuracy: 0.95\n",
            "Epoch: 694, Training Loss: 16.39, Training Accuracy: 0.96\n",
            "Epoch: 694, Validation Loss: 16.39, Validation Accuracy: 0.97\n",
            "Epoch: 694, Testing Loss: 16.39, Testing Accuracy: 0.95\n",
            "Epoch: 695, Training Loss: 16.38, Training Accuracy: 0.96\n",
            "Epoch: 695, Validation Loss: 16.38, Validation Accuracy: 0.97\n",
            "Epoch: 695, Testing Loss: 16.38, Testing Accuracy: 0.95\n",
            "Epoch: 696, Training Loss: 16.37, Training Accuracy: 0.96\n",
            "Epoch: 696, Validation Loss: 16.37, Validation Accuracy: 0.97\n",
            "Epoch: 696, Testing Loss: 16.37, Testing Accuracy: 0.95\n",
            "Epoch: 697, Training Loss: 16.36, Training Accuracy: 0.96\n",
            "Epoch: 697, Validation Loss: 16.36, Validation Accuracy: 0.97\n",
            "Epoch: 697, Testing Loss: 16.36, Testing Accuracy: 0.95\n",
            "Epoch: 698, Training Loss: 16.36, Training Accuracy: 0.96\n",
            "Epoch: 698, Validation Loss: 16.36, Validation Accuracy: 0.97\n",
            "Epoch: 698, Testing Loss: 16.36, Testing Accuracy: 0.95\n",
            "Epoch: 699, Training Loss: 16.35, Training Accuracy: 0.96\n",
            "Epoch: 699, Validation Loss: 16.35, Validation Accuracy: 0.97\n",
            "Epoch: 699, Testing Loss: 16.35, Testing Accuracy: 0.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Eg8ziMq-CH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 700\n",
        "l_r = 0.001\n",
        "b1 = None\n",
        "b2 = None\n",
        "e = 10e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNpIBkmrsnEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracySGD(predictions, labels):\n",
        "    return (np.sum((predictions>=0.5)==labels) / np.shape(predictions)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ3WZ-TNrT_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildGraph(beta1 = None, beta2 = None, epsilon = None, lossType=None, learning_rate = None):\n",
        "    num_batches = int(3500/batch_size)\n",
        "    lbda = 0\n",
        "    graph = tf.Graph()\n",
        "  \n",
        "    with graph.as_default():\n",
        "        W = tf.truncated_normal(shape=(784, 1), mean=0.0, stddev=0.0, dtype=tf.float32)\n",
        "        W = tf.Variable(W)\n",
        "        b = tf.zeros(1)\n",
        "        b = tf.Variable(b)\n",
        "\n",
        "        x = tf.placeholder(tf.float32, shape=(batch_size, 784))\n",
        "        y = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
        "\n",
        "        v_x = tf.placeholder(tf.float32, shape=(len(validData), 784))\n",
        "        v_y = tf.placeholder(tf.int8, shape=(len(validTarget), 1))\n",
        "\n",
        "        t_x = tf.placeholder(tf.float32, shape=(len(testData), 784))\n",
        "        t_y = tf.placeholder(tf.int8, shape=(len(testTarget), 1))\n",
        "\n",
        "        tf.set_random_seed(421)\n",
        "\n",
        "        if lossType == \"MSE\":\n",
        "            z = tf.matmul(x,W) + b\n",
        "            loss = tf.losses.mean_squared_error(y, z)\n",
        "            reg = tf.nn.l2_loss(W)\n",
        "            loss += (lbda/2.0)*reg\n",
        "\n",
        "            v_z = tf.matmul(v_x,W) + b\n",
        "            vloss = tf.losses.mean_squared_error(v_y, v_z)\n",
        "            vloss += (lbda/2.0)*reg\n",
        "\n",
        "            t_z = tf.matmul(t_x,W) + b\n",
        "            tloss = tf.losses.mean_squared_error(t_y, t_z)\n",
        "            tloss += (lbda/2.0)*reg\n",
        "\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate = 0.001, beta1 = 0.95).minimize(loss)\n",
        "\n",
        "        elif lossType == \"CE\":\n",
        "            z = tf.sigmoid(tf.matmul(x,W) + b)\n",
        "            loss = tf.losses.sigmoid_cross_entropy(y, z)\n",
        "            reg = tf.nn.l2_loss(W)\n",
        "            loss += (lbda/2.0)*reg\n",
        "\n",
        "            v_z = tf.sigmoid(tf.matmul(v_x,W) + b)\n",
        "            vloss = tf.losses.sigmoid_cross_entropy(v_y, v_z)\n",
        "            vloss += (lbda/2.0)*reg\n",
        "\n",
        "            t_z = tf.sigmoid(tf.matmul(t_x,W) + b)\n",
        "            tloss = tf.losses.sigmoid_cross_entropy(t_y, t_z)\n",
        "            tloss += (lbda/2.0)*reg\n",
        "\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n",
        "\n",
        "        with tf.Session(graph=graph) as session:\n",
        "            tf.global_variables_initializer().run()\n",
        "            training_loss = []\n",
        "            validation_loss = []\n",
        "            testing_loss = []\n",
        "            training_accuracy = []\n",
        "            validation_accuracy = []\n",
        "            testing_accuracy = []\n",
        "            for epoch in range(epochs):\n",
        "                total_loss = 0\n",
        "                for n in range(num_batches):\n",
        "                    x_batch = trainData[n*batch_size:(n+1)*batch_size,]\n",
        "                    y_batch = trainTarget[n*batch_size:(n+1)*batch_size,]\n",
        "                    _, opt_W, opt_b, train_loss, pred, v_loss, v_pred, t_loss, t_pred = session.run([optimizer, W, b, loss, z, vloss, v_z, tloss, t_z], \n",
        "                                                                                                                      {x: x_batch, \n",
        "                                                                                                                       y: y_batch,\n",
        "                                                                                                                       v_x: validData,\n",
        "                                                                                                                       v_y: validTarget,\n",
        "                                                                                                                         t_x: testData,\n",
        "                                                                                                                       t_y: testTarget})\n",
        "                if (n % 1 == 0):\n",
        "                    training_loss += [train_loss]\n",
        "                    t_accuracy = accuracySGD(pred, y_batch)\n",
        "                    training_accuracy += [t_accuracy]\n",
        "                    validation_loss += [v_loss]\n",
        "                    v_accuracy = accuracySGD(v_pred, validTarget)\n",
        "                    validation_accuracy += [v_accuracy]\n",
        "                    testing_loss += [t_loss]\n",
        "                    t_accuracy = accuracySGD(t_pred, testTarget)\n",
        "                    testing_accuracy += [t_accuracy]\n",
        "\n",
        "                    print('Epoch: {}, Training Loss: {}, Training Accuracy: {}'.format(epoch, train_loss, t_accuracy))\n",
        "                    print('Epoch: {}, Validation Loss: {}, Validation Accuracy: {}'.format(epoch, v_loss, v_accuracy))\n",
        "                    print('Epoch: {}, Testing Loss: {}, Testing Accuracy: {}'.format(epoch, t_loss, t_accuracy))\n",
        "\n",
        "    # Your implementation here\n",
        "    return opt_W, opt_b, (pred>=0.5), trainTarget, train_loss, optimizer, reg, training_loss, training_accuracy, validation_loss, validation_accuracy, testing_loss, testing_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpLH-dkbrgN8",
        "colab_type": "code",
        "outputId": "753660cc-eb00-41bd-b040-34e61310a0f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "opt_W, opt_b, pred, trainTarget, train_loss, optimizer, reg, ce_training_loss, ce_training_accuracy, ce_validation_loss, ce_validation_accuracy, ce_testing_loss, ce_testing_accuracy = buildGraph(lossType = \"CE\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Training Loss: 0.6521342992782593, Training Accuracy: 0.8413793103448276\n",
            "Epoch: 0, Validation Loss: 0.6628973484039307, Validation Accuracy: 0.79\n",
            "Epoch: 0, Testing Loss: 0.6850776672363281, Testing Accuracy: 0.8413793103448276\n",
            "Epoch: 1, Training Loss: 0.6058230996131897, Training Accuracy: 0.9103448275862069\n",
            "Epoch: 1, Validation Loss: 0.6181305050849915, Validation Accuracy: 0.92\n",
            "Epoch: 1, Testing Loss: 0.6346621513366699, Testing Accuracy: 0.9103448275862069\n",
            "Epoch: 2, Training Loss: 0.5755142569541931, Training Accuracy: 0.9448275862068966\n",
            "Epoch: 2, Validation Loss: 0.5884568095207214, Validation Accuracy: 0.98\n",
            "Epoch: 2, Testing Loss: 0.6042874455451965, Testing Accuracy: 0.9448275862068966\n",
            "Epoch: 3, Training Loss: 0.5573850870132446, Training Accuracy: 0.9586206896551724\n",
            "Epoch: 3, Validation Loss: 0.5700535774230957, Validation Accuracy: 0.99\n",
            "Epoch: 3, Testing Loss: 0.5869660377502441, Testing Accuracy: 0.9586206896551724\n",
            "Epoch: 4, Training Loss: 0.5467203855514526, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 4, Validation Loss: 0.5587455630302429, Validation Accuracy: 0.99\n",
            "Epoch: 4, Testing Loss: 0.5767520666122437, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 5, Training Loss: 0.5397803783416748, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 5, Validation Loss: 0.5509548783302307, Validation Accuracy: 0.99\n",
            "Epoch: 5, Testing Loss: 0.5698330402374268, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 6, Training Loss: 0.5350437164306641, Training Accuracy: 0.9586206896551724\n",
            "Epoch: 6, Validation Loss: 0.5453599691390991, Validation Accuracy: 0.99\n",
            "Epoch: 6, Testing Loss: 0.564957857131958, Testing Accuracy: 0.9586206896551724\n",
            "Epoch: 7, Training Loss: 0.5317005515098572, Training Accuracy: 0.9586206896551724\n",
            "Epoch: 7, Validation Loss: 0.5413159132003784, Validation Accuracy: 0.99\n",
            "Epoch: 7, Testing Loss: 0.5615237355232239, Testing Accuracy: 0.9586206896551724\n",
            "Epoch: 8, Training Loss: 0.5291958451271057, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 8, Validation Loss: 0.53830486536026, Validation Accuracy: 0.99\n",
            "Epoch: 8, Testing Loss: 0.5590493083000183, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 9, Training Loss: 0.5272233486175537, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 9, Validation Loss: 0.5359852313995361, Validation Accuracy: 0.99\n",
            "Epoch: 9, Testing Loss: 0.5571973323822021, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 10, Training Loss: 0.5256307721138, Training Accuracy: 0.9586206896551724\n",
            "Epoch: 10, Validation Loss: 0.5341470241546631, Validation Accuracy: 0.98\n",
            "Epoch: 10, Testing Loss: 0.5557481050491333, Testing Accuracy: 0.9586206896551724\n",
            "Epoch: 11, Training Loss: 0.5243187546730042, Training Accuracy: 0.9586206896551724\n",
            "Epoch: 11, Validation Loss: 0.5326349139213562, Validation Accuracy: 0.98\n",
            "Epoch: 11, Testing Loss: 0.5545498132705688, Testing Accuracy: 0.9586206896551724\n",
            "Epoch: 12, Training Loss: 0.523209810256958, Training Accuracy: 0.9586206896551724\n",
            "Epoch: 12, Validation Loss: 0.5313342809677124, Validation Accuracy: 0.98\n",
            "Epoch: 12, Testing Loss: 0.5535041093826294, Testing Accuracy: 0.9586206896551724\n",
            "Epoch: 13, Training Loss: 0.5222488641738892, Training Accuracy: 0.9586206896551724\n",
            "Epoch: 13, Validation Loss: 0.530177891254425, Validation Accuracy: 0.98\n",
            "Epoch: 13, Testing Loss: 0.5525618195533752, Testing Accuracy: 0.9586206896551724\n",
            "Epoch: 14, Training Loss: 0.5214009881019592, Training Accuracy: 0.9586206896551724\n",
            "Epoch: 14, Validation Loss: 0.5291363000869751, Validation Accuracy: 0.98\n",
            "Epoch: 14, Testing Loss: 0.5517066121101379, Testing Accuracy: 0.9586206896551724\n",
            "Epoch: 15, Training Loss: 0.5206426978111267, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 15, Validation Loss: 0.5281972289085388, Validation Accuracy: 0.98\n",
            "Epoch: 15, Testing Loss: 0.5509347319602966, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 16, Training Loss: 0.5199573040008545, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 16, Validation Loss: 0.5273520946502686, Validation Accuracy: 0.98\n",
            "Epoch: 16, Testing Loss: 0.550242006778717, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 17, Training Loss: 0.5193319916725159, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 17, Validation Loss: 0.5265902876853943, Validation Accuracy: 0.98\n",
            "Epoch: 17, Testing Loss: 0.5496206879615784, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 18, Training Loss: 0.5187575221061707, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 18, Validation Loss: 0.5258997678756714, Validation Accuracy: 0.98\n",
            "Epoch: 18, Testing Loss: 0.5490604639053345, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 19, Training Loss: 0.5182265043258667, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 19, Validation Loss: 0.5252684950828552, Validation Accuracy: 0.98\n",
            "Epoch: 19, Testing Loss: 0.5485501289367676, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 20, Training Loss: 0.5177332162857056, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 20, Validation Loss: 0.5246860980987549, Validation Accuracy: 0.98\n",
            "Epoch: 20, Testing Loss: 0.5480806231498718, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 21, Training Loss: 0.5172728300094604, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 21, Validation Loss: 0.5241442322731018, Validation Accuracy: 0.98\n",
            "Epoch: 21, Testing Loss: 0.5476444959640503, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 22, Training Loss: 0.5168415904045105, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 22, Validation Loss: 0.5236374735832214, Validation Accuracy: 0.98\n",
            "Epoch: 22, Testing Loss: 0.5472372770309448, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 23, Training Loss: 0.5164362192153931, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 23, Validation Loss: 0.5231620669364929, Validation Accuracy: 0.98\n",
            "Epoch: 23, Testing Loss: 0.5468559861183167, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 24, Training Loss: 0.5160539746284485, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 24, Validation Loss: 0.5227156281471252, Validation Accuracy: 0.98\n",
            "Epoch: 24, Testing Loss: 0.546498715877533, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 25, Training Loss: 0.5156927704811096, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 25, Validation Loss: 0.5222963094711304, Validation Accuracy: 0.98\n",
            "Epoch: 25, Testing Loss: 0.5461640954017639, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 26, Training Loss: 0.5153506994247437, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 26, Validation Loss: 0.5219022631645203, Validation Accuracy: 0.98\n",
            "Epoch: 26, Testing Loss: 0.5458505153656006, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 27, Training Loss: 0.5150259733200073, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 27, Validation Loss: 0.5215314626693726, Validation Accuracy: 0.98\n",
            "Epoch: 27, Testing Loss: 0.5455564260482788, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 28, Training Loss: 0.514717161655426, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 28, Validation Loss: 0.521182119846344, Validation Accuracy: 0.98\n",
            "Epoch: 28, Testing Loss: 0.5452800989151001, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 29, Training Loss: 0.5144230127334595, Training Accuracy: 0.9655172413793104\n",
            "Epoch: 29, Validation Loss: 0.5208521485328674, Validation Accuracy: 0.98\n",
            "Epoch: 29, Testing Loss: 0.5450199246406555, Testing Accuracy: 0.9655172413793104\n",
            "Epoch: 30, Training Loss: 0.5141422748565674, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 30, Validation Loss: 0.5205397009849548, Validation Accuracy: 0.98\n",
            "Epoch: 30, Testing Loss: 0.5447742938995361, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 31, Training Loss: 0.513873815536499, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 31, Validation Loss: 0.5202432870864868, Validation Accuracy: 0.98\n",
            "Epoch: 31, Testing Loss: 0.5445419549942017, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 32, Training Loss: 0.5136169195175171, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 32, Validation Loss: 0.5199616551399231, Validation Accuracy: 0.98\n",
            "Epoch: 32, Testing Loss: 0.5443217754364014, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 33, Training Loss: 0.5133705735206604, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 33, Validation Loss: 0.5196936726570129, Validation Accuracy: 0.98\n",
            "Epoch: 33, Testing Loss: 0.5441129803657532, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 34, Training Loss: 0.5131341814994812, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 34, Validation Loss: 0.5194382667541504, Validation Accuracy: 0.98\n",
            "Epoch: 34, Testing Loss: 0.5439145565032959, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 35, Training Loss: 0.5129069685935974, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 35, Validation Loss: 0.5191940069198608, Validation Accuracy: 0.98\n",
            "Epoch: 35, Testing Loss: 0.5436860918998718, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 36, Training Loss: 0.512688398361206, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 36, Validation Loss: 0.5189624428749084, Validation Accuracy: 0.98\n",
            "Epoch: 36, Testing Loss: 0.5435465574264526, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 37, Training Loss: 0.5124778747558594, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 37, Validation Loss: 0.5187404751777649, Validation Accuracy: 0.98\n",
            "Epoch: 37, Testing Loss: 0.5433757305145264, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 38, Training Loss: 0.512274980545044, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 38, Validation Loss: 0.518528163433075, Validation Accuracy: 0.98\n",
            "Epoch: 38, Testing Loss: 0.5432128310203552, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 39, Training Loss: 0.5120790600776672, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 39, Validation Loss: 0.5183250308036804, Validation Accuracy: 0.98\n",
            "Epoch: 39, Testing Loss: 0.543057382106781, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 40, Training Loss: 0.5118899345397949, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 40, Validation Loss: 0.5181304216384888, Validation Accuracy: 0.98\n",
            "Epoch: 40, Testing Loss: 0.5429088473320007, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 41, Training Loss: 0.5117071270942688, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 41, Validation Loss: 0.5179437398910522, Validation Accuracy: 0.98\n",
            "Epoch: 41, Testing Loss: 0.5427669286727905, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 42, Training Loss: 0.5115302205085754, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 42, Validation Loss: 0.517764687538147, Validation Accuracy: 0.98\n",
            "Epoch: 42, Testing Loss: 0.5426311492919922, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 43, Training Loss: 0.5113588571548462, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 43, Validation Loss: 0.5175926089286804, Validation Accuracy: 0.98\n",
            "Epoch: 43, Testing Loss: 0.5425010919570923, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 44, Training Loss: 0.5111929178237915, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 44, Validation Loss: 0.5174272656440735, Validation Accuracy: 0.98\n",
            "Epoch: 44, Testing Loss: 0.5423763990402222, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 45, Training Loss: 0.5110319256782532, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 45, Validation Loss: 0.5172682404518127, Validation Accuracy: 0.98\n",
            "Epoch: 45, Testing Loss: 0.542256772518158, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 46, Training Loss: 0.5108758211135864, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 46, Validation Loss: 0.5171151757240295, Validation Accuracy: 0.98\n",
            "Epoch: 46, Testing Loss: 0.5421419143676758, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 47, Training Loss: 0.5107242465019226, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 47, Validation Loss: 0.5169677138328552, Validation Accuracy: 0.98\n",
            "Epoch: 47, Testing Loss: 0.5420315861701965, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 48, Training Loss: 0.5105768442153931, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 48, Validation Loss: 0.5168256759643555, Validation Accuracy: 0.98\n",
            "Epoch: 48, Testing Loss: 0.5419255495071411, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 49, Training Loss: 0.5104336738586426, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 49, Validation Loss: 0.5166886448860168, Validation Accuracy: 0.98\n",
            "Epoch: 49, Testing Loss: 0.5418236255645752, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 50, Training Loss: 0.510294497013092, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 50, Validation Loss: 0.5165565013885498, Validation Accuracy: 0.98\n",
            "Epoch: 50, Testing Loss: 0.5417253971099854, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 51, Training Loss: 0.5101589560508728, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 51, Validation Loss: 0.5164288878440857, Validation Accuracy: 0.98\n",
            "Epoch: 51, Testing Loss: 0.5416308641433716, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 52, Training Loss: 0.5100270509719849, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 52, Validation Loss: 0.516305685043335, Validation Accuracy: 0.98\n",
            "Epoch: 52, Testing Loss: 0.54153972864151, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 53, Training Loss: 0.5098985433578491, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 53, Validation Loss: 0.516186535358429, Validation Accuracy: 0.98\n",
            "Epoch: 53, Testing Loss: 0.5414518713951111, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 54, Training Loss: 0.5097733736038208, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 54, Validation Loss: 0.5160713791847229, Validation Accuracy: 0.98\n",
            "Epoch: 54, Testing Loss: 0.5413669347763062, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 55, Training Loss: 0.509651243686676, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 55, Validation Loss: 0.5159600377082825, Validation Accuracy: 0.98\n",
            "Epoch: 55, Testing Loss: 0.5412850975990295, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 56, Training Loss: 0.50953209400177, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 56, Validation Loss: 0.5158522129058838, Validation Accuracy: 0.98\n",
            "Epoch: 56, Testing Loss: 0.541205883026123, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 57, Training Loss: 0.5094160437583923, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 57, Validation Loss: 0.5157477855682373, Validation Accuracy: 0.98\n",
            "Epoch: 57, Testing Loss: 0.5411293506622314, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 58, Training Loss: 0.5093027353286743, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 58, Validation Loss: 0.5156461000442505, Validation Accuracy: 0.98\n",
            "Epoch: 58, Testing Loss: 0.5410550236701965, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 59, Training Loss: 0.5091920495033264, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 59, Validation Loss: 0.5155486464500427, Validation Accuracy: 0.98\n",
            "Epoch: 59, Testing Loss: 0.5409836769104004, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 60, Training Loss: 0.5090839266777039, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 60, Validation Loss: 0.5154536366462708, Validation Accuracy: 0.98\n",
            "Epoch: 60, Testing Loss: 0.5409143567085266, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 61, Training Loss: 0.5089782476425171, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 61, Validation Loss: 0.5153614282608032, Validation Accuracy: 0.98\n",
            "Epoch: 61, Testing Loss: 0.54084712266922, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 62, Training Loss: 0.5088750720024109, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 62, Validation Loss: 0.5152720808982849, Validation Accuracy: 0.98\n",
            "Epoch: 62, Testing Loss: 0.5407819747924805, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 63, Training Loss: 0.5087742209434509, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 63, Validation Loss: 0.5151851773262024, Validation Accuracy: 0.98\n",
            "Epoch: 63, Testing Loss: 0.5407187342643738, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 64, Training Loss: 0.5086755752563477, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 64, Validation Loss: 0.5151009559631348, Validation Accuracy: 0.98\n",
            "Epoch: 64, Testing Loss: 0.5406574606895447, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 65, Training Loss: 0.5085790753364563, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 65, Validation Loss: 0.5150189995765686, Validation Accuracy: 0.98\n",
            "Epoch: 65, Testing Loss: 0.5405979156494141, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 66, Training Loss: 0.5084847211837769, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 66, Validation Loss: 0.5149394273757935, Validation Accuracy: 0.98\n",
            "Epoch: 66, Testing Loss: 0.5405400991439819, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 67, Training Loss: 0.5083923935890198, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 67, Validation Loss: 0.5148621201515198, Validation Accuracy: 0.98\n",
            "Epoch: 67, Testing Loss: 0.5404838919639587, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 68, Training Loss: 0.5083019733428955, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 68, Validation Loss: 0.5147868394851685, Validation Accuracy: 0.98\n",
            "Epoch: 68, Testing Loss: 0.5404291749000549, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 69, Training Loss: 0.5082135200500488, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 69, Validation Loss: 0.5147136449813843, Validation Accuracy: 0.98\n",
            "Epoch: 69, Testing Loss: 0.5403760075569153, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 70, Training Loss: 0.5081268548965454, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 70, Validation Loss: 0.5146424174308777, Validation Accuracy: 0.98\n",
            "Epoch: 70, Testing Loss: 0.5403242707252502, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 71, Training Loss: 0.5080419778823853, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 71, Validation Loss: 0.5145730376243591, Validation Accuracy: 0.98\n",
            "Epoch: 71, Testing Loss: 0.5402739644050598, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 72, Training Loss: 0.5079589486122131, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 72, Validation Loss: 0.5145055651664734, Validation Accuracy: 0.98\n",
            "Epoch: 72, Testing Loss: 0.5402249097824097, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 73, Training Loss: 0.5078775882720947, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 73, Validation Loss: 0.5144398212432861, Validation Accuracy: 0.98\n",
            "Epoch: 73, Testing Loss: 0.5401771664619446, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 74, Training Loss: 0.5077977776527405, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 74, Validation Loss: 0.5143757462501526, Validation Accuracy: 0.98\n",
            "Epoch: 74, Testing Loss: 0.540130615234375, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 75, Training Loss: 0.5077195763587952, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 75, Validation Loss: 0.5143128037452698, Validation Accuracy: 0.98\n",
            "Epoch: 75, Testing Loss: 0.540084958076477, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 76, Training Loss: 0.507642924785614, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 76, Validation Loss: 0.5142524838447571, Validation Accuracy: 0.98\n",
            "Epoch: 76, Testing Loss: 0.5400409698486328, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 77, Training Loss: 0.5075679421424866, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 77, Validation Loss: 0.5141931772232056, Validation Accuracy: 0.98\n",
            "Epoch: 77, Testing Loss: 0.5399976968765259, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 78, Training Loss: 0.5074942708015442, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 78, Validation Loss: 0.5141353011131287, Validation Accuracy: 0.98\n",
            "Epoch: 78, Testing Loss: 0.5399554967880249, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 79, Training Loss: 0.5074220299720764, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 79, Validation Loss: 0.5140788555145264, Validation Accuracy: 0.98\n",
            "Epoch: 79, Testing Loss: 0.5399142503738403, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 80, Training Loss: 0.5073512196540833, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 80, Validation Loss: 0.5140237808227539, Validation Accuracy: 0.98\n",
            "Epoch: 80, Testing Loss: 0.5398740768432617, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 81, Training Loss: 0.5072817206382751, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 81, Validation Loss: 0.5139700770378113, Validation Accuracy: 0.98\n",
            "Epoch: 81, Testing Loss: 0.5398346781730652, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 82, Training Loss: 0.5072135329246521, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 82, Validation Loss: 0.5139175653457642, Validation Accuracy: 0.98\n",
            "Epoch: 82, Testing Loss: 0.5397962331771851, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 83, Training Loss: 0.5071466565132141, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 83, Validation Loss: 0.5138663649559021, Validation Accuracy: 0.98\n",
            "Epoch: 83, Testing Loss: 0.5397586822509766, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 84, Training Loss: 0.5070809721946716, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 84, Validation Loss: 0.5138050317764282, Validation Accuracy: 0.98\n",
            "Epoch: 84, Testing Loss: 0.5397219061851501, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 85, Training Loss: 0.507016658782959, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 85, Validation Loss: 0.513767421245575, Validation Accuracy: 0.98\n",
            "Epoch: 85, Testing Loss: 0.5396859049797058, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 86, Training Loss: 0.5069533586502075, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 86, Validation Loss: 0.5137197375297546, Validation Accuracy: 0.98\n",
            "Epoch: 86, Testing Loss: 0.5396506190299988, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 87, Training Loss: 0.5068913102149963, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 87, Validation Loss: 0.5136730670928955, Validation Accuracy: 0.98\n",
            "Epoch: 87, Testing Loss: 0.5396161675453186, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 88, Training Loss: 0.5068303346633911, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 88, Validation Loss: 0.5136275291442871, Validation Accuracy: 0.98\n",
            "Epoch: 88, Testing Loss: 0.5395824313163757, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 89, Training Loss: 0.5067704319953918, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 89, Validation Loss: 0.5135830044746399, Validation Accuracy: 0.98\n",
            "Epoch: 89, Testing Loss: 0.5395492911338806, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 90, Training Loss: 0.5067116618156433, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 90, Validation Loss: 0.5135394334793091, Validation Accuracy: 0.98\n",
            "Epoch: 90, Testing Loss: 0.5395169258117676, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 91, Training Loss: 0.506653904914856, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 91, Validation Loss: 0.5134968161582947, Validation Accuracy: 0.98\n",
            "Epoch: 91, Testing Loss: 0.5394850969314575, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 92, Training Loss: 0.5065971612930298, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 92, Validation Loss: 0.5134551525115967, Validation Accuracy: 0.98\n",
            "Epoch: 92, Testing Loss: 0.5394539833068848, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 93, Training Loss: 0.5065413117408752, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 93, Validation Loss: 0.5134144425392151, Validation Accuracy: 0.98\n",
            "Epoch: 93, Testing Loss: 0.539423406124115, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 94, Training Loss: 0.5064865350723267, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 94, Validation Loss: 0.5133745670318604, Validation Accuracy: 0.98\n",
            "Epoch: 94, Testing Loss: 0.5393934845924377, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 95, Training Loss: 0.5064326524734497, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 95, Validation Loss: 0.5133355855941772, Validation Accuracy: 0.98\n",
            "Epoch: 95, Testing Loss: 0.5393640995025635, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 96, Training Loss: 0.5063796639442444, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 96, Validation Loss: 0.513297438621521, Validation Accuracy: 0.98\n",
            "Epoch: 96, Testing Loss: 0.5393352508544922, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 97, Training Loss: 0.5063276290893555, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 97, Validation Loss: 0.5132600665092468, Validation Accuracy: 0.98\n",
            "Epoch: 97, Testing Loss: 0.5393069386482239, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 98, Training Loss: 0.5062764883041382, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 98, Validation Loss: 0.5132235884666443, Validation Accuracy: 0.98\n",
            "Epoch: 98, Testing Loss: 0.5392791032791138, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 99, Training Loss: 0.5062260031700134, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 99, Validation Loss: 0.5131877660751343, Validation Accuracy: 0.98\n",
            "Epoch: 99, Testing Loss: 0.5392518639564514, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 100, Training Loss: 0.5061765313148499, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 100, Validation Loss: 0.5131527781486511, Validation Accuracy: 0.98\n",
            "Epoch: 100, Testing Loss: 0.5392251014709473, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 101, Training Loss: 0.5061277747154236, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 101, Validation Loss: 0.5131185054779053, Validation Accuracy: 0.98\n",
            "Epoch: 101, Testing Loss: 0.5391986966133118, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 102, Training Loss: 0.506079912185669, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 102, Validation Loss: 0.513084888458252, Validation Accuracy: 0.98\n",
            "Epoch: 102, Testing Loss: 0.5391727685928345, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 103, Training Loss: 0.5060327053070068, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 103, Validation Loss: 0.5130519866943359, Validation Accuracy: 0.98\n",
            "Epoch: 103, Testing Loss: 0.5391474366188049, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 104, Training Loss: 0.505986213684082, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 104, Validation Loss: 0.5130197405815125, Validation Accuracy: 0.98\n",
            "Epoch: 104, Testing Loss: 0.5391224026679993, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 105, Training Loss: 0.5059405565261841, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 105, Validation Loss: 0.512988269329071, Validation Accuracy: 0.98\n",
            "Epoch: 105, Testing Loss: 0.5390977263450623, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 106, Training Loss: 0.5058954954147339, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 106, Validation Loss: 0.5129573345184326, Validation Accuracy: 0.98\n",
            "Epoch: 106, Testing Loss: 0.539073646068573, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 107, Training Loss: 0.5058513283729553, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 107, Validation Loss: 0.5129269957542419, Validation Accuracy: 0.98\n",
            "Epoch: 107, Testing Loss: 0.5390498638153076, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 108, Training Loss: 0.5058076977729797, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 108, Validation Loss: 0.5128973126411438, Validation Accuracy: 0.98\n",
            "Epoch: 108, Testing Loss: 0.5390264391899109, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 109, Training Loss: 0.5057646632194519, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 109, Validation Loss: 0.5128681659698486, Validation Accuracy: 0.98\n",
            "Epoch: 109, Testing Loss: 0.5390034914016724, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 110, Training Loss: 0.5057222843170166, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 110, Validation Loss: 0.512839674949646, Validation Accuracy: 0.98\n",
            "Epoch: 110, Testing Loss: 0.5389807820320129, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 111, Training Loss: 0.5056805610656738, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 111, Validation Loss: 0.5128117203712463, Validation Accuracy: 0.98\n",
            "Epoch: 111, Testing Loss: 0.5389585494995117, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 112, Training Loss: 0.5056394934654236, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 112, Validation Loss: 0.5127843618392944, Validation Accuracy: 0.98\n",
            "Epoch: 112, Testing Loss: 0.5389366149902344, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 113, Training Loss: 0.5055990219116211, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 113, Validation Loss: 0.5127574801445007, Validation Accuracy: 0.98\n",
            "Epoch: 113, Testing Loss: 0.5389149785041809, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 114, Training Loss: 0.5055591464042664, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 114, Validation Loss: 0.5127311944961548, Validation Accuracy: 0.98\n",
            "Epoch: 114, Testing Loss: 0.5388936400413513, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 115, Training Loss: 0.5055198073387146, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 115, Validation Loss: 0.5127053260803223, Validation Accuracy: 0.98\n",
            "Epoch: 115, Testing Loss: 0.5388727784156799, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 116, Training Loss: 0.5054810643196106, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 116, Validation Loss: 0.5126800537109375, Validation Accuracy: 0.98\n",
            "Epoch: 116, Testing Loss: 0.5388520956039429, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 117, Training Loss: 0.50544273853302, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 117, Validation Loss: 0.5126551985740662, Validation Accuracy: 0.98\n",
            "Epoch: 117, Testing Loss: 0.5388317704200745, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 118, Training Loss: 0.5054051280021667, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 118, Validation Loss: 0.5126308798789978, Validation Accuracy: 0.98\n",
            "Epoch: 118, Testing Loss: 0.5388117432594299, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 119, Training Loss: 0.5053678154945374, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 119, Validation Loss: 0.5126069784164429, Validation Accuracy: 0.98\n",
            "Epoch: 119, Testing Loss: 0.5387920141220093, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 120, Training Loss: 0.5053312182426453, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 120, Validation Loss: 0.5125835537910461, Validation Accuracy: 0.98\n",
            "Epoch: 120, Testing Loss: 0.5387725830078125, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 121, Training Loss: 0.5052950382232666, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 121, Validation Loss: 0.5125606060028076, Validation Accuracy: 0.98\n",
            "Epoch: 121, Testing Loss: 0.53875333070755, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 122, Training Loss: 0.5052593350410461, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 122, Validation Loss: 0.5125380158424377, Validation Accuracy: 0.98\n",
            "Epoch: 122, Testing Loss: 0.5387344360351562, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 123, Training Loss: 0.5052241086959839, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 123, Validation Loss: 0.5125159025192261, Validation Accuracy: 0.98\n",
            "Epoch: 123, Testing Loss: 0.5387157797813416, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 124, Training Loss: 0.5051893591880798, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 124, Validation Loss: 0.5124942660331726, Validation Accuracy: 0.98\n",
            "Epoch: 124, Testing Loss: 0.5386974215507507, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 125, Training Loss: 0.5051549673080444, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 125, Validation Loss: 0.512472927570343, Validation Accuracy: 0.98\n",
            "Epoch: 125, Testing Loss: 0.5386792421340942, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 126, Training Loss: 0.5051211714744568, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 126, Validation Loss: 0.5124520659446716, Validation Accuracy: 0.98\n",
            "Epoch: 126, Testing Loss: 0.5386613607406616, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 127, Training Loss: 0.505087673664093, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 127, Validation Loss: 0.5124315619468689, Validation Accuracy: 0.98\n",
            "Epoch: 127, Testing Loss: 0.5386436581611633, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 128, Training Loss: 0.5050547122955322, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 128, Validation Loss: 0.5124114751815796, Validation Accuracy: 0.98\n",
            "Epoch: 128, Testing Loss: 0.5386262536048889, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 129, Training Loss: 0.5050219893455505, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 129, Validation Loss: 0.5123917460441589, Validation Accuracy: 0.98\n",
            "Epoch: 129, Testing Loss: 0.5386090874671936, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 130, Training Loss: 0.5049898028373718, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 130, Validation Loss: 0.5123723745346069, Validation Accuracy: 0.98\n",
            "Epoch: 130, Testing Loss: 0.5385921001434326, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 131, Training Loss: 0.504957914352417, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 131, Validation Loss: 0.5123533606529236, Validation Accuracy: 0.98\n",
            "Epoch: 131, Testing Loss: 0.5385753512382507, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 132, Training Loss: 0.5049265027046204, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 132, Validation Loss: 0.5123344659805298, Validation Accuracy: 0.98\n",
            "Epoch: 132, Testing Loss: 0.5385586619377136, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 133, Training Loss: 0.5048953890800476, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 133, Validation Loss: 0.5123164057731628, Validation Accuracy: 0.98\n",
            "Epoch: 133, Testing Loss: 0.5385425090789795, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 134, Training Loss: 0.5048647522926331, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 134, Validation Loss: 0.5122984051704407, Validation Accuracy: 0.98\n",
            "Epoch: 134, Testing Loss: 0.5385263562202454, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 135, Training Loss: 0.5048344135284424, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 135, Validation Loss: 0.5122808218002319, Validation Accuracy: 0.98\n",
            "Epoch: 135, Testing Loss: 0.5385104417800903, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 136, Training Loss: 0.5048044323921204, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 136, Validation Loss: 0.5122634768486023, Validation Accuracy: 0.98\n",
            "Epoch: 136, Testing Loss: 0.5384947061538696, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 137, Training Loss: 0.504774808883667, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 137, Validation Loss: 0.5122465491294861, Validation Accuracy: 0.98\n",
            "Epoch: 137, Testing Loss: 0.538479208946228, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 138, Training Loss: 0.5047454833984375, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 138, Validation Loss: 0.5122298002243042, Validation Accuracy: 0.98\n",
            "Epoch: 138, Testing Loss: 0.538463830947876, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 139, Training Loss: 0.5047165155410767, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 139, Validation Loss: 0.5122134685516357, Validation Accuracy: 0.98\n",
            "Epoch: 139, Testing Loss: 0.538448691368103, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 140, Training Loss: 0.5046877861022949, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 140, Validation Loss: 0.5121973156929016, Validation Accuracy: 0.98\n",
            "Epoch: 140, Testing Loss: 0.5384336113929749, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 141, Training Loss: 0.5046594738960266, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 141, Validation Loss: 0.5121816396713257, Validation Accuracy: 0.98\n",
            "Epoch: 141, Testing Loss: 0.5384187698364258, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 142, Training Loss: 0.5046314597129822, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 142, Validation Loss: 0.5121660828590393, Validation Accuracy: 0.98\n",
            "Epoch: 142, Testing Loss: 0.5384041666984558, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 143, Training Loss: 0.5046036839485168, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 143, Validation Loss: 0.5121509432792664, Validation Accuracy: 0.98\n",
            "Epoch: 143, Testing Loss: 0.5383896231651306, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 144, Training Loss: 0.5045762062072754, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 144, Validation Loss: 0.5121359825134277, Validation Accuracy: 0.98\n",
            "Epoch: 144, Testing Loss: 0.5383753180503845, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 145, Training Loss: 0.5045491456985474, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 145, Validation Loss: 0.5121213793754578, Validation Accuracy: 0.98\n",
            "Epoch: 145, Testing Loss: 0.5383611917495728, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 146, Training Loss: 0.5045222640037537, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 146, Validation Loss: 0.5121069550514221, Validation Accuracy: 0.98\n",
            "Epoch: 146, Testing Loss: 0.5383471250534058, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 147, Training Loss: 0.5044956207275391, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 147, Validation Loss: 0.5120928287506104, Validation Accuracy: 0.98\n",
            "Epoch: 147, Testing Loss: 0.5383333563804626, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 148, Training Loss: 0.5044692754745483, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 148, Validation Loss: 0.5120790004730225, Validation Accuracy: 0.98\n",
            "Epoch: 148, Testing Loss: 0.5383195281028748, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 149, Training Loss: 0.5044432282447815, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 149, Validation Loss: 0.5120653510093689, Validation Accuracy: 0.98\n",
            "Epoch: 149, Testing Loss: 0.5383059978485107, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 150, Training Loss: 0.504417359828949, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 150, Validation Loss: 0.5120519399642944, Validation Accuracy: 0.98\n",
            "Epoch: 150, Testing Loss: 0.5382925271987915, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 151, Training Loss: 0.5043917894363403, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 151, Validation Loss: 0.5120388269424438, Validation Accuracy: 0.98\n",
            "Epoch: 151, Testing Loss: 0.5382792353630066, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 152, Training Loss: 0.5043665170669556, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 152, Validation Loss: 0.5120260119438171, Validation Accuracy: 0.98\n",
            "Epoch: 152, Testing Loss: 0.538266122341156, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 153, Training Loss: 0.5043414235115051, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 153, Validation Loss: 0.5120132565498352, Validation Accuracy: 0.98\n",
            "Epoch: 153, Testing Loss: 0.5382530689239502, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 154, Training Loss: 0.5043166279792786, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 154, Validation Loss: 0.5120008587837219, Validation Accuracy: 0.98\n",
            "Epoch: 154, Testing Loss: 0.5382401347160339, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 155, Training Loss: 0.5042920112609863, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 155, Validation Loss: 0.5119886994361877, Validation Accuracy: 0.98\n",
            "Epoch: 155, Testing Loss: 0.538227379322052, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 156, Training Loss: 0.504267692565918, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 156, Validation Loss: 0.5119766592979431, Validation Accuracy: 0.98\n",
            "Epoch: 156, Testing Loss: 0.5382146239280701, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 157, Training Loss: 0.5042434334754944, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 157, Validation Loss: 0.5119649171829224, Validation Accuracy: 0.98\n",
            "Epoch: 157, Testing Loss: 0.5382021069526672, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 158, Training Loss: 0.5042194724082947, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 158, Validation Loss: 0.5119534134864807, Validation Accuracy: 0.98\n",
            "Epoch: 158, Testing Loss: 0.538189709186554, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 159, Training Loss: 0.5041957497596741, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 159, Validation Loss: 0.5119420289993286, Validation Accuracy: 0.98\n",
            "Epoch: 159, Testing Loss: 0.5381773114204407, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 160, Training Loss: 0.504172146320343, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 160, Validation Loss: 0.5119308829307556, Validation Accuracy: 0.98\n",
            "Epoch: 160, Testing Loss: 0.5381651520729065, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 161, Training Loss: 0.5041488409042358, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 161, Validation Loss: 0.5119199156761169, Validation Accuracy: 0.98\n",
            "Epoch: 161, Testing Loss: 0.5381529927253723, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 162, Training Loss: 0.504125714302063, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 162, Validation Loss: 0.5119092464447021, Validation Accuracy: 0.98\n",
            "Epoch: 162, Testing Loss: 0.5381409525871277, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 163, Training Loss: 0.5041027069091797, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 163, Validation Loss: 0.5118986368179321, Validation Accuracy: 0.98\n",
            "Epoch: 163, Testing Loss: 0.5381290912628174, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 164, Training Loss: 0.5040799379348755, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 164, Validation Loss: 0.5118882656097412, Validation Accuracy: 0.98\n",
            "Epoch: 164, Testing Loss: 0.5381172299385071, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 165, Training Loss: 0.5040573477745056, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 165, Validation Loss: 0.5118780732154846, Validation Accuracy: 0.98\n",
            "Epoch: 165, Testing Loss: 0.5381054878234863, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 166, Training Loss: 0.5040349364280701, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 166, Validation Loss: 0.5118681192398071, Validation Accuracy: 0.98\n",
            "Epoch: 166, Testing Loss: 0.5380938649177551, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 167, Training Loss: 0.5040127635002136, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 167, Validation Loss: 0.5118582844734192, Validation Accuracy: 0.98\n",
            "Epoch: 167, Testing Loss: 0.5380823612213135, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 168, Training Loss: 0.503990650177002, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 168, Validation Loss: 0.5118486285209656, Validation Accuracy: 0.98\n",
            "Epoch: 168, Testing Loss: 0.5380709171295166, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 169, Training Loss: 0.5039687156677246, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 169, Validation Loss: 0.5118390917778015, Validation Accuracy: 0.98\n",
            "Epoch: 169, Testing Loss: 0.5380595326423645, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 170, Training Loss: 0.5039469003677368, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 170, Validation Loss: 0.5118298530578613, Validation Accuracy: 0.98\n",
            "Epoch: 170, Testing Loss: 0.538048267364502, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 171, Training Loss: 0.5039253234863281, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 171, Validation Loss: 0.5118206143379211, Validation Accuracy: 0.98\n",
            "Epoch: 171, Testing Loss: 0.5380370020866394, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 172, Training Loss: 0.5039039254188538, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 172, Validation Loss: 0.5118116736412048, Validation Accuracy: 0.98\n",
            "Epoch: 172, Testing Loss: 0.538025975227356, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 173, Training Loss: 0.5038825869560242, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 173, Validation Loss: 0.5118028521537781, Validation Accuracy: 0.98\n",
            "Epoch: 173, Testing Loss: 0.538014829158783, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 174, Training Loss: 0.5038614869117737, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 174, Validation Loss: 0.5117941498756409, Validation Accuracy: 0.98\n",
            "Epoch: 174, Testing Loss: 0.5380038619041443, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 175, Training Loss: 0.503840446472168, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 175, Validation Loss: 0.511785626411438, Validation Accuracy: 0.98\n",
            "Epoch: 175, Testing Loss: 0.5379928946495056, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 176, Training Loss: 0.5038195252418518, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 176, Validation Loss: 0.5117772817611694, Validation Accuracy: 0.98\n",
            "Epoch: 176, Testing Loss: 0.5379820466041565, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 177, Training Loss: 0.5037988424301147, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 177, Validation Loss: 0.5117690563201904, Validation Accuracy: 0.98\n",
            "Epoch: 177, Testing Loss: 0.5379713177680969, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 178, Training Loss: 0.5037781596183777, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 178, Validation Loss: 0.511760950088501, Validation Accuracy: 0.98\n",
            "Epoch: 178, Testing Loss: 0.5379605889320374, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 179, Training Loss: 0.503757655620575, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 179, Validation Loss: 0.5117529630661011, Validation Accuracy: 0.98\n",
            "Epoch: 179, Testing Loss: 0.5379499197006226, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 180, Training Loss: 0.5037373304367065, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 180, Validation Loss: 0.5117452144622803, Validation Accuracy: 0.98\n",
            "Epoch: 180, Testing Loss: 0.5379393696784973, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 181, Training Loss: 0.5037170648574829, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 181, Validation Loss: 0.5117374658584595, Validation Accuracy: 0.98\n",
            "Epoch: 181, Testing Loss: 0.5379288196563721, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 182, Training Loss: 0.5036969184875488, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 182, Validation Loss: 0.5117299556732178, Validation Accuracy: 0.98\n",
            "Epoch: 182, Testing Loss: 0.5379183292388916, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 183, Training Loss: 0.5036768913269043, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 183, Validation Loss: 0.5117225050926208, Validation Accuracy: 0.98\n",
            "Epoch: 183, Testing Loss: 0.5379080176353455, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 184, Training Loss: 0.5036569833755493, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 184, Validation Loss: 0.511715292930603, Validation Accuracy: 0.98\n",
            "Epoch: 184, Testing Loss: 0.5378975868225098, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 185, Training Loss: 0.5036371350288391, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 185, Validation Loss: 0.5117080807685852, Validation Accuracy: 0.98\n",
            "Epoch: 185, Testing Loss: 0.5378873348236084, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 186, Training Loss: 0.5036174058914185, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 186, Validation Loss: 0.5117008686065674, Validation Accuracy: 0.98\n",
            "Epoch: 186, Testing Loss: 0.5378769636154175, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 187, Training Loss: 0.5035977363586426, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 187, Validation Loss: 0.5116941332817078, Validation Accuracy: 0.98\n",
            "Epoch: 187, Testing Loss: 0.5378668308258057, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 188, Training Loss: 0.503578245639801, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 188, Validation Loss: 0.5116873383522034, Validation Accuracy: 0.98\n",
            "Epoch: 188, Testing Loss: 0.5378566384315491, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 189, Training Loss: 0.5035588145256042, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 189, Validation Loss: 0.5116806626319885, Validation Accuracy: 0.98\n",
            "Epoch: 189, Testing Loss: 0.5378465056419373, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 190, Training Loss: 0.5035394430160522, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 190, Validation Loss: 0.5116741061210632, Validation Accuracy: 0.98\n",
            "Epoch: 190, Testing Loss: 0.5378365516662598, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 191, Training Loss: 0.5035201907157898, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 191, Validation Loss: 0.5116676092147827, Validation Accuracy: 0.98\n",
            "Epoch: 191, Testing Loss: 0.5378264784812927, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 192, Training Loss: 0.5035009980201721, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 192, Validation Loss: 0.5116612315177917, Validation Accuracy: 0.98\n",
            "Epoch: 192, Testing Loss: 0.5378165245056152, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 193, Training Loss: 0.5034819841384888, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 193, Validation Loss: 0.5116550326347351, Validation Accuracy: 0.98\n",
            "Epoch: 193, Testing Loss: 0.5378065705299377, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 194, Training Loss: 0.5034628510475159, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 194, Validation Loss: 0.5116488933563232, Validation Accuracy: 0.98\n",
            "Epoch: 194, Testing Loss: 0.5377967357635498, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 195, Training Loss: 0.5034439563751221, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 195, Validation Loss: 0.5116427540779114, Validation Accuracy: 0.98\n",
            "Epoch: 195, Testing Loss: 0.5377869009971619, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 196, Training Loss: 0.503425121307373, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 196, Validation Loss: 0.5116367936134338, Validation Accuracy: 0.98\n",
            "Epoch: 196, Testing Loss: 0.5377770662307739, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 197, Training Loss: 0.503406286239624, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 197, Validation Loss: 0.5116309523582458, Validation Accuracy: 0.98\n",
            "Epoch: 197, Testing Loss: 0.5377673506736755, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 198, Training Loss: 0.5033875703811646, Training Accuracy: 0.9724137931034482\n",
            "Epoch: 198, Validation Loss: 0.5116251707077026, Validation Accuracy: 0.98\n",
            "Epoch: 198, Testing Loss: 0.5377576351165771, Testing Accuracy: 0.9724137931034482\n",
            "Epoch: 199, Training Loss: 0.5033688545227051, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 199, Validation Loss: 0.511619508266449, Validation Accuracy: 0.98\n",
            "Epoch: 199, Testing Loss: 0.5377479195594788, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 200, Training Loss: 0.5033503174781799, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 200, Validation Loss: 0.5116139650344849, Validation Accuracy: 0.98\n",
            "Epoch: 200, Testing Loss: 0.5377383232116699, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 201, Training Loss: 0.5033317804336548, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 201, Validation Loss: 0.5116084814071655, Validation Accuracy: 0.98\n",
            "Epoch: 201, Testing Loss: 0.5377287268638611, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 202, Training Loss: 0.5033132433891296, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 202, Validation Loss: 0.5116031169891357, Validation Accuracy: 0.98\n",
            "Epoch: 202, Testing Loss: 0.537719190120697, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 203, Training Loss: 0.5032948851585388, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 203, Validation Loss: 0.5115978121757507, Validation Accuracy: 0.98\n",
            "Epoch: 203, Testing Loss: 0.537709653377533, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 204, Training Loss: 0.503276526927948, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 204, Validation Loss: 0.5115926265716553, Validation Accuracy: 0.98\n",
            "Epoch: 204, Testing Loss: 0.5377001762390137, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 205, Training Loss: 0.5032581686973572, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 205, Validation Loss: 0.5115874409675598, Validation Accuracy: 0.98\n",
            "Epoch: 205, Testing Loss: 0.5376907587051392, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 206, Training Loss: 0.5032399892807007, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 206, Validation Loss: 0.5115823745727539, Validation Accuracy: 0.98\n",
            "Epoch: 206, Testing Loss: 0.5376813411712646, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 207, Training Loss: 0.5032217502593994, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 207, Validation Loss: 0.5115773677825928, Validation Accuracy: 0.98\n",
            "Epoch: 207, Testing Loss: 0.5376719236373901, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 208, Training Loss: 0.5032035708427429, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 208, Validation Loss: 0.5115724802017212, Validation Accuracy: 0.98\n",
            "Epoch: 208, Testing Loss: 0.5376625657081604, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 209, Training Loss: 0.5031854510307312, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 209, Validation Loss: 0.5115676522254944, Validation Accuracy: 0.98\n",
            "Epoch: 209, Testing Loss: 0.5376532673835754, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 210, Training Loss: 0.5031673908233643, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 210, Validation Loss: 0.5115628838539124, Validation Accuracy: 0.98\n",
            "Epoch: 210, Testing Loss: 0.53764408826828, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 211, Training Loss: 0.5031493902206421, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 211, Validation Loss: 0.5115582346916199, Validation Accuracy: 0.98\n",
            "Epoch: 211, Testing Loss: 0.5376347899436951, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 212, Training Loss: 0.5031314492225647, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 212, Validation Loss: 0.5115536451339722, Validation Accuracy: 0.98\n",
            "Epoch: 212, Testing Loss: 0.5376256108283997, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 213, Training Loss: 0.5031135082244873, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 213, Validation Loss: 0.5115491151809692, Validation Accuracy: 0.98\n",
            "Epoch: 213, Testing Loss: 0.5376163721084595, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 214, Training Loss: 0.5030956268310547, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 214, Validation Loss: 0.5115446448326111, Validation Accuracy: 0.98\n",
            "Epoch: 214, Testing Loss: 0.5376072525978088, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 215, Training Loss: 0.5030778050422668, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 215, Validation Loss: 0.5115402340888977, Validation Accuracy: 0.98\n",
            "Epoch: 215, Testing Loss: 0.537598192691803, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 216, Training Loss: 0.5030600428581238, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 216, Validation Loss: 0.5115359425544739, Validation Accuracy: 0.98\n",
            "Epoch: 216, Testing Loss: 0.5375891327857971, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 217, Training Loss: 0.5030422806739807, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 217, Validation Loss: 0.51153165102005, Validation Accuracy: 0.98\n",
            "Epoch: 217, Testing Loss: 0.5375800728797913, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 218, Training Loss: 0.5030245780944824, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 218, Validation Loss: 0.5115274786949158, Validation Accuracy: 0.98\n",
            "Epoch: 218, Testing Loss: 0.5375710725784302, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 219, Training Loss: 0.5030068755149841, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 219, Validation Loss: 0.5115233063697815, Validation Accuracy: 0.98\n",
            "Epoch: 219, Testing Loss: 0.5375620722770691, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 220, Training Loss: 0.5029892325401306, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 220, Validation Loss: 0.5115192532539368, Validation Accuracy: 0.98\n",
            "Epoch: 220, Testing Loss: 0.5375531315803528, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 221, Training Loss: 0.5029716491699219, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 221, Validation Loss: 0.511515200138092, Validation Accuracy: 0.98\n",
            "Epoch: 221, Testing Loss: 0.537544310092926, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 222, Training Loss: 0.5029540657997131, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 222, Validation Loss: 0.5115112066268921, Validation Accuracy: 0.98\n",
            "Epoch: 222, Testing Loss: 0.5375354290008545, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 223, Training Loss: 0.502936601638794, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 223, Validation Loss: 0.5115073323249817, Validation Accuracy: 0.98\n",
            "Epoch: 223, Testing Loss: 0.537526547908783, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 224, Training Loss: 0.50291907787323, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 224, Validation Loss: 0.5115034580230713, Validation Accuracy: 0.98\n",
            "Epoch: 224, Testing Loss: 0.5375177264213562, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 225, Training Loss: 0.5029016733169556, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 225, Validation Loss: 0.5114996433258057, Validation Accuracy: 0.98\n",
            "Epoch: 225, Testing Loss: 0.537509024143219, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 226, Training Loss: 0.5028843283653259, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 226, Validation Loss: 0.5114958882331848, Validation Accuracy: 0.98\n",
            "Epoch: 226, Testing Loss: 0.5375002026557922, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 227, Training Loss: 0.5028669834136963, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 227, Validation Loss: 0.5114921927452087, Validation Accuracy: 0.98\n",
            "Epoch: 227, Testing Loss: 0.537491500377655, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 228, Training Loss: 0.5028496980667114, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 228, Validation Loss: 0.5114885568618774, Validation Accuracy: 0.98\n",
            "Epoch: 228, Testing Loss: 0.5374828577041626, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 229, Training Loss: 0.5028323531150818, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 229, Validation Loss: 0.5114848017692566, Validation Accuracy: 0.98\n",
            "Epoch: 229, Testing Loss: 0.5374740958213806, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 230, Training Loss: 0.5028152465820312, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 230, Validation Loss: 0.5114813446998596, Validation Accuracy: 0.98\n",
            "Epoch: 230, Testing Loss: 0.5374655723571777, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 231, Training Loss: 0.5027980208396912, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 231, Validation Loss: 0.5114778280258179, Validation Accuracy: 0.98\n",
            "Epoch: 231, Testing Loss: 0.5374569892883301, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 232, Training Loss: 0.5027809143066406, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 232, Validation Loss: 0.5114743709564209, Validation Accuracy: 0.98\n",
            "Epoch: 232, Testing Loss: 0.5374484062194824, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 233, Training Loss: 0.5027638077735901, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 233, Validation Loss: 0.5114709138870239, Validation Accuracy: 0.98\n",
            "Epoch: 233, Testing Loss: 0.5374399423599243, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 234, Training Loss: 0.5027468204498291, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 234, Validation Loss: 0.5114675760269165, Validation Accuracy: 0.98\n",
            "Epoch: 234, Testing Loss: 0.5374314785003662, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 235, Training Loss: 0.5027298927307129, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 235, Validation Loss: 0.5114641785621643, Validation Accuracy: 0.98\n",
            "Epoch: 235, Testing Loss: 0.5374229550361633, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 236, Training Loss: 0.5027130246162415, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 236, Validation Loss: 0.5114608407020569, Validation Accuracy: 0.98\n",
            "Epoch: 236, Testing Loss: 0.53741455078125, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 237, Training Loss: 0.5026960968971252, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 237, Validation Loss: 0.5114575624465942, Validation Accuracy: 0.98\n",
            "Epoch: 237, Testing Loss: 0.5374061465263367, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 238, Training Loss: 0.5026792883872986, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 238, Validation Loss: 0.5114543437957764, Validation Accuracy: 0.98\n",
            "Epoch: 238, Testing Loss: 0.5373978018760681, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 239, Training Loss: 0.5026625990867615, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 239, Validation Loss: 0.511451005935669, Validation Accuracy: 0.98\n",
            "Epoch: 239, Testing Loss: 0.53738933801651, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 240, Training Loss: 0.5026458501815796, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 240, Validation Loss: 0.5114479064941406, Validation Accuracy: 0.98\n",
            "Epoch: 240, Testing Loss: 0.5373811721801758, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 241, Training Loss: 0.5026292204856873, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 241, Validation Loss: 0.5114447474479675, Validation Accuracy: 0.98\n",
            "Epoch: 241, Testing Loss: 0.537372887134552, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 242, Training Loss: 0.5026126503944397, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 242, Validation Loss: 0.5114415884017944, Validation Accuracy: 0.98\n",
            "Epoch: 242, Testing Loss: 0.537364661693573, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 243, Training Loss: 0.5025960803031921, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 243, Validation Loss: 0.5114385485649109, Validation Accuracy: 0.98\n",
            "Epoch: 243, Testing Loss: 0.537356436252594, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 244, Training Loss: 0.5025796294212341, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 244, Validation Loss: 0.5114354491233826, Validation Accuracy: 0.98\n",
            "Epoch: 244, Testing Loss: 0.537348210811615, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 245, Training Loss: 0.5025632381439209, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 245, Validation Loss: 0.511432409286499, Validation Accuracy: 0.98\n",
            "Epoch: 245, Testing Loss: 0.5373400449752808, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 246, Training Loss: 0.5025469064712524, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 246, Validation Loss: 0.5114294290542603, Validation Accuracy: 0.98\n",
            "Epoch: 246, Testing Loss: 0.5373318195343018, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 247, Training Loss: 0.5025306940078735, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 247, Validation Loss: 0.5114263892173767, Validation Accuracy: 0.98\n",
            "Epoch: 247, Testing Loss: 0.5373237133026123, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 248, Training Loss: 0.5025144815444946, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 248, Validation Loss: 0.5114234089851379, Validation Accuracy: 0.98\n",
            "Epoch: 248, Testing Loss: 0.5373156070709229, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 249, Training Loss: 0.5024983882904053, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 249, Validation Loss: 0.5114204287528992, Validation Accuracy: 0.98\n",
            "Epoch: 249, Testing Loss: 0.5373075604438782, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 250, Training Loss: 0.5024823546409607, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 250, Validation Loss: 0.5114174485206604, Validation Accuracy: 0.98\n",
            "Epoch: 250, Testing Loss: 0.5372995138168335, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 251, Training Loss: 0.5024663209915161, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 251, Validation Loss: 0.5114145874977112, Validation Accuracy: 0.98\n",
            "Epoch: 251, Testing Loss: 0.5372914671897888, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 252, Training Loss: 0.5024503469467163, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 252, Validation Loss: 0.5114116072654724, Validation Accuracy: 0.98\n",
            "Epoch: 252, Testing Loss: 0.5372834205627441, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 253, Training Loss: 0.502434492111206, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 253, Validation Loss: 0.5114087462425232, Validation Accuracy: 0.98\n",
            "Epoch: 253, Testing Loss: 0.537275493144989, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 254, Training Loss: 0.5024187564849854, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 254, Validation Loss: 0.511405885219574, Validation Accuracy: 0.98\n",
            "Epoch: 254, Testing Loss: 0.5372675657272339, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 255, Training Loss: 0.5024030208587646, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 255, Validation Loss: 0.51140296459198, Validation Accuracy: 0.98\n",
            "Epoch: 255, Testing Loss: 0.5372596383094788, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 256, Training Loss: 0.502387523651123, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 256, Validation Loss: 0.5114001631736755, Validation Accuracy: 0.98\n",
            "Epoch: 256, Testing Loss: 0.5372517108917236, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 257, Training Loss: 0.5023719668388367, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 257, Validation Loss: 0.5113973021507263, Validation Accuracy: 0.98\n",
            "Epoch: 257, Testing Loss: 0.5372437834739685, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 258, Training Loss: 0.5023564696311951, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 258, Validation Loss: 0.5113945007324219, Validation Accuracy: 0.98\n",
            "Epoch: 258, Testing Loss: 0.5372359156608582, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 259, Training Loss: 0.502341091632843, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 259, Validation Loss: 0.5113916993141174, Validation Accuracy: 0.98\n",
            "Epoch: 259, Testing Loss: 0.5372281074523926, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 260, Training Loss: 0.5023257732391357, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 260, Validation Loss: 0.5113889575004578, Validation Accuracy: 0.98\n",
            "Epoch: 260, Testing Loss: 0.5372202396392822, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 261, Training Loss: 0.502310574054718, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 261, Validation Loss: 0.5113860964775085, Validation Accuracy: 0.98\n",
            "Epoch: 261, Testing Loss: 0.5372123718261719, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 262, Training Loss: 0.5022954940795898, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 262, Validation Loss: 0.5113833546638489, Validation Accuracy: 0.98\n",
            "Epoch: 262, Testing Loss: 0.5372045636177063, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 263, Training Loss: 0.5022803544998169, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 263, Validation Loss: 0.5113805532455444, Validation Accuracy: 0.98\n",
            "Epoch: 263, Testing Loss: 0.5371968150138855, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 264, Training Loss: 0.5022653341293335, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 264, Validation Loss: 0.5113778114318848, Validation Accuracy: 0.98\n",
            "Epoch: 264, Testing Loss: 0.5371890068054199, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 265, Training Loss: 0.5022504925727844, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 265, Validation Loss: 0.5113750696182251, Validation Accuracy: 0.98\n",
            "Epoch: 265, Testing Loss: 0.5371812582015991, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 266, Training Loss: 0.5022356510162354, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 266, Validation Loss: 0.5113723278045654, Validation Accuracy: 0.98\n",
            "Epoch: 266, Testing Loss: 0.5371734499931335, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 267, Training Loss: 0.5022209286689758, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 267, Validation Loss: 0.5113694667816162, Validation Accuracy: 0.98\n",
            "Epoch: 267, Testing Loss: 0.5371657013893127, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 268, Training Loss: 0.5022062063217163, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 268, Validation Loss: 0.5113667845726013, Validation Accuracy: 0.98\n",
            "Epoch: 268, Testing Loss: 0.5371580719947815, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 269, Training Loss: 0.5021916627883911, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 269, Validation Loss: 0.5113641023635864, Validation Accuracy: 0.98\n",
            "Epoch: 269, Testing Loss: 0.5371503233909607, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 270, Training Loss: 0.5021771788597107, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 270, Validation Loss: 0.5113613605499268, Validation Accuracy: 0.98\n",
            "Epoch: 270, Testing Loss: 0.5371426343917847, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 271, Training Loss: 0.502162754535675, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 271, Validation Loss: 0.5113586187362671, Validation Accuracy: 0.98\n",
            "Epoch: 271, Testing Loss: 0.5371349453926086, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 272, Training Loss: 0.502148449420929, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 272, Validation Loss: 0.511355996131897, Validation Accuracy: 0.98\n",
            "Epoch: 272, Testing Loss: 0.5371272563934326, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 273, Training Loss: 0.5021341443061829, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 273, Validation Loss: 0.5113532543182373, Validation Accuracy: 0.98\n",
            "Epoch: 273, Testing Loss: 0.5371195673942566, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 274, Training Loss: 0.5021198987960815, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 274, Validation Loss: 0.5113505125045776, Validation Accuracy: 0.98\n",
            "Epoch: 274, Testing Loss: 0.5371118783950806, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 275, Training Loss: 0.5021057724952698, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 275, Validation Loss: 0.5113478302955627, Validation Accuracy: 0.98\n",
            "Epoch: 275, Testing Loss: 0.5371043086051941, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 276, Training Loss: 0.5020918250083923, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 276, Validation Loss: 0.5113451480865479, Validation Accuracy: 0.98\n",
            "Epoch: 276, Testing Loss: 0.5370966196060181, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 277, Training Loss: 0.5020778775215149, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 277, Validation Loss: 0.5113424062728882, Validation Accuracy: 0.98\n",
            "Epoch: 277, Testing Loss: 0.5370890498161316, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 278, Training Loss: 0.5020639896392822, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 278, Validation Loss: 0.5113397240638733, Validation Accuracy: 0.98\n",
            "Epoch: 278, Testing Loss: 0.5370813608169556, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 279, Training Loss: 0.5020502209663391, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 279, Validation Loss: 0.5113370418548584, Validation Accuracy: 0.98\n",
            "Epoch: 279, Testing Loss: 0.5370737910270691, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 280, Training Loss: 0.502036452293396, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 280, Validation Loss: 0.5113343596458435, Validation Accuracy: 0.98\n",
            "Epoch: 280, Testing Loss: 0.5370662212371826, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 281, Training Loss: 0.5020228624343872, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 281, Validation Loss: 0.5113317370414734, Validation Accuracy: 0.98\n",
            "Epoch: 281, Testing Loss: 0.5370586514472961, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 282, Training Loss: 0.5020092129707336, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 282, Validation Loss: 0.5113290548324585, Validation Accuracy: 0.98\n",
            "Epoch: 282, Testing Loss: 0.5370510816574097, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 283, Training Loss: 0.5019958019256592, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 283, Validation Loss: 0.5113263726234436, Validation Accuracy: 0.98\n",
            "Epoch: 283, Testing Loss: 0.5370435118675232, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 284, Training Loss: 0.5019823312759399, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 284, Validation Loss: 0.5113236904144287, Validation Accuracy: 0.98\n",
            "Epoch: 284, Testing Loss: 0.5370360016822815, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 285, Training Loss: 0.501969039440155, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 285, Validation Loss: 0.5113210082054138, Validation Accuracy: 0.98\n",
            "Epoch: 285, Testing Loss: 0.537028431892395, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 286, Training Loss: 0.5019558072090149, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 286, Validation Loss: 0.5113183856010437, Validation Accuracy: 0.98\n",
            "Epoch: 286, Testing Loss: 0.5370208621025085, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 287, Training Loss: 0.50194251537323, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 287, Validation Loss: 0.5113157033920288, Validation Accuracy: 0.98\n",
            "Epoch: 287, Testing Loss: 0.5370134115219116, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 288, Training Loss: 0.5019293427467346, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 288, Validation Loss: 0.5113130211830139, Validation Accuracy: 0.98\n",
            "Epoch: 288, Testing Loss: 0.5369978547096252, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 289, Training Loss: 0.5019162893295288, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 289, Validation Loss: 0.5113103985786438, Validation Accuracy: 0.98\n",
            "Epoch: 289, Testing Loss: 0.5369982719421387, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 290, Training Loss: 0.5019032955169678, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 290, Validation Loss: 0.5113077759742737, Validation Accuracy: 0.98\n",
            "Epoch: 290, Testing Loss: 0.536990761756897, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 291, Training Loss: 0.5018903613090515, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 291, Validation Loss: 0.5113051533699036, Validation Accuracy: 0.98\n",
            "Epoch: 291, Testing Loss: 0.5369833111763, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 292, Training Loss: 0.5018775463104248, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 292, Validation Loss: 0.5113025903701782, Validation Accuracy: 0.98\n",
            "Epoch: 292, Testing Loss: 0.5369758605957031, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 293, Training Loss: 0.5018647313117981, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 293, Validation Loss: 0.5112999081611633, Validation Accuracy: 0.98\n",
            "Epoch: 293, Testing Loss: 0.5369683504104614, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 294, Training Loss: 0.5018519759178162, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 294, Validation Loss: 0.5112972855567932, Validation Accuracy: 0.98\n",
            "Epoch: 294, Testing Loss: 0.5369608998298645, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 295, Training Loss: 0.5018393993377686, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 295, Validation Loss: 0.5112947225570679, Validation Accuracy: 0.98\n",
            "Epoch: 295, Testing Loss: 0.5369534492492676, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 296, Training Loss: 0.5018267631530762, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 296, Validation Loss: 0.511292040348053, Validation Accuracy: 0.98\n",
            "Epoch: 296, Testing Loss: 0.5369459390640259, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 297, Training Loss: 0.5018142461776733, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 297, Validation Loss: 0.5112894177436829, Validation Accuracy: 0.98\n",
            "Epoch: 297, Testing Loss: 0.5369386076927185, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 298, Training Loss: 0.5018017292022705, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 298, Validation Loss: 0.5112868547439575, Validation Accuracy: 0.98\n",
            "Epoch: 298, Testing Loss: 0.5369311571121216, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 299, Training Loss: 0.501789391040802, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 299, Validation Loss: 0.5112842321395874, Validation Accuracy: 0.98\n",
            "Epoch: 299, Testing Loss: 0.5369237661361694, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 300, Training Loss: 0.5017770528793335, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 300, Validation Loss: 0.5112817287445068, Validation Accuracy: 0.98\n",
            "Epoch: 300, Testing Loss: 0.5369163155555725, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 301, Training Loss: 0.501764714717865, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 301, Validation Loss: 0.5112790465354919, Validation Accuracy: 0.98\n",
            "Epoch: 301, Testing Loss: 0.5369089245796204, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 302, Training Loss: 0.5017524361610413, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 302, Validation Loss: 0.5112764835357666, Validation Accuracy: 0.98\n",
            "Epoch: 302, Testing Loss: 0.5369015336036682, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 303, Training Loss: 0.5017403364181519, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 303, Validation Loss: 0.511273980140686, Validation Accuracy: 0.98\n",
            "Epoch: 303, Testing Loss: 0.5368942022323608, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 304, Training Loss: 0.5017281770706177, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 304, Validation Loss: 0.5112714171409607, Validation Accuracy: 0.98\n",
            "Epoch: 304, Testing Loss: 0.5368868112564087, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 305, Training Loss: 0.5017161965370178, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 305, Validation Loss: 0.5112687945365906, Validation Accuracy: 0.98\n",
            "Epoch: 305, Testing Loss: 0.5368794202804565, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 306, Training Loss: 0.501704216003418, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 306, Validation Loss: 0.51126629114151, Validation Accuracy: 0.98\n",
            "Epoch: 306, Testing Loss: 0.5368720889091492, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 307, Training Loss: 0.5016922354698181, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 307, Validation Loss: 0.5112637281417847, Validation Accuracy: 0.98\n",
            "Epoch: 307, Testing Loss: 0.5368648171424866, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 308, Training Loss: 0.501680314540863, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 308, Validation Loss: 0.5112612247467041, Validation Accuracy: 0.98\n",
            "Epoch: 308, Testing Loss: 0.5368574261665344, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 309, Training Loss: 0.5016684532165527, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 309, Validation Loss: 0.5112587213516235, Validation Accuracy: 0.98\n",
            "Epoch: 309, Testing Loss: 0.5368502140045166, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 310, Training Loss: 0.501656711101532, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 310, Validation Loss: 0.5112561583518982, Validation Accuracy: 0.98\n",
            "Epoch: 310, Testing Loss: 0.5368428826332092, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 311, Training Loss: 0.501645028591156, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 311, Validation Loss: 0.5112536549568176, Validation Accuracy: 0.98\n",
            "Epoch: 311, Testing Loss: 0.5368355512619019, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 312, Training Loss: 0.5016334056854248, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 312, Validation Loss: 0.5112511515617371, Validation Accuracy: 0.98\n",
            "Epoch: 312, Testing Loss: 0.536828339099884, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 313, Training Loss: 0.5016217231750488, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 313, Validation Loss: 0.5112487077713013, Validation Accuracy: 0.98\n",
            "Epoch: 313, Testing Loss: 0.5368210673332214, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 314, Training Loss: 0.5016101598739624, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 314, Validation Loss: 0.5112462043762207, Validation Accuracy: 0.98\n",
            "Epoch: 314, Testing Loss: 0.5368137955665588, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 315, Training Loss: 0.5015986561775208, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 315, Validation Loss: 0.5112437605857849, Validation Accuracy: 0.98\n",
            "Epoch: 315, Testing Loss: 0.5368066430091858, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 316, Training Loss: 0.5015872120857239, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 316, Validation Loss: 0.5112411975860596, Validation Accuracy: 0.98\n",
            "Epoch: 316, Testing Loss: 0.5367993712425232, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 317, Training Loss: 0.5015758275985718, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 317, Validation Loss: 0.5112388134002686, Validation Accuracy: 0.98\n",
            "Epoch: 317, Testing Loss: 0.5367922186851501, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 318, Training Loss: 0.5015644431114197, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 318, Validation Loss: 0.511236310005188, Validation Accuracy: 0.98\n",
            "Epoch: 318, Testing Loss: 0.5367850661277771, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 319, Training Loss: 0.5015531182289124, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 319, Validation Loss: 0.5112338066101074, Validation Accuracy: 0.98\n",
            "Epoch: 319, Testing Loss: 0.5367765426635742, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 320, Training Loss: 0.5015419125556946, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 320, Validation Loss: 0.5112314820289612, Validation Accuracy: 0.98\n",
            "Epoch: 320, Testing Loss: 0.5367707014083862, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 321, Training Loss: 0.5015307068824768, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 321, Validation Loss: 0.5112290382385254, Validation Accuracy: 0.98\n",
            "Epoch: 321, Testing Loss: 0.536763608455658, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 322, Training Loss: 0.501519501209259, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 322, Validation Loss: 0.5112265944480896, Validation Accuracy: 0.98\n",
            "Epoch: 322, Testing Loss: 0.5367563962936401, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 323, Training Loss: 0.5015084147453308, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 323, Validation Loss: 0.5112241506576538, Validation Accuracy: 0.98\n",
            "Epoch: 323, Testing Loss: 0.5367493033409119, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 324, Training Loss: 0.5014973282814026, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 324, Validation Loss: 0.5112218260765076, Validation Accuracy: 0.98\n",
            "Epoch: 324, Testing Loss: 0.5367422103881836, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 325, Training Loss: 0.5014862418174744, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 325, Validation Loss: 0.5112194418907166, Validation Accuracy: 0.98\n",
            "Epoch: 325, Testing Loss: 0.5367351770401001, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 326, Training Loss: 0.5014753341674805, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 326, Validation Loss: 0.5112170577049255, Validation Accuracy: 0.98\n",
            "Epoch: 326, Testing Loss: 0.536728024482727, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 327, Training Loss: 0.5014643669128418, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 327, Validation Loss: 0.5112147331237793, Validation Accuracy: 0.98\n",
            "Epoch: 327, Testing Loss: 0.5367210507392883, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 328, Training Loss: 0.5014534592628479, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 328, Validation Loss: 0.5112124085426331, Validation Accuracy: 0.98\n",
            "Epoch: 328, Testing Loss: 0.5367140173912048, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 329, Training Loss: 0.5014426112174988, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 329, Validation Loss: 0.5112100839614868, Validation Accuracy: 0.98\n",
            "Epoch: 329, Testing Loss: 0.5367069840431213, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 330, Training Loss: 0.5014318227767944, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 330, Validation Loss: 0.5112077593803406, Validation Accuracy: 0.98\n",
            "Epoch: 330, Testing Loss: 0.5367000102996826, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 331, Training Loss: 0.5014210343360901, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 331, Validation Loss: 0.5112054347991943, Validation Accuracy: 0.98\n",
            "Epoch: 331, Testing Loss: 0.5366930365562439, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 332, Training Loss: 0.5014103651046753, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 332, Validation Loss: 0.5112031102180481, Validation Accuracy: 0.98\n",
            "Epoch: 332, Testing Loss: 0.53668612241745, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 333, Training Loss: 0.5013996362686157, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 333, Validation Loss: 0.5112008452415466, Validation Accuracy: 0.98\n",
            "Epoch: 333, Testing Loss: 0.536679208278656, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 334, Training Loss: 0.5013889670372009, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 334, Validation Loss: 0.5111985206604004, Validation Accuracy: 0.98\n",
            "Epoch: 334, Testing Loss: 0.5366722345352173, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 335, Training Loss: 0.5013784170150757, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 335, Validation Loss: 0.5111962556838989, Validation Accuracy: 0.98\n",
            "Epoch: 335, Testing Loss: 0.5366653203964233, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 336, Training Loss: 0.5013678669929504, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 336, Validation Loss: 0.5111940503120422, Validation Accuracy: 0.98\n",
            "Epoch: 336, Testing Loss: 0.5366584658622742, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 337, Training Loss: 0.5013573169708252, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 337, Validation Loss: 0.511191725730896, Validation Accuracy: 0.98\n",
            "Epoch: 337, Testing Loss: 0.5366515517234802, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 338, Training Loss: 0.5013468861579895, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 338, Validation Loss: 0.5111895203590393, Validation Accuracy: 0.98\n",
            "Epoch: 338, Testing Loss: 0.5366447567939758, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 339, Training Loss: 0.5013364553451538, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 339, Validation Loss: 0.5111873149871826, Validation Accuracy: 0.98\n",
            "Epoch: 339, Testing Loss: 0.5366379022598267, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 340, Training Loss: 0.5013260245323181, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 340, Validation Loss: 0.5111851096153259, Validation Accuracy: 0.98\n",
            "Epoch: 340, Testing Loss: 0.5366311073303223, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 341, Training Loss: 0.5013157725334167, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 341, Validation Loss: 0.5111829042434692, Validation Accuracy: 0.98\n",
            "Epoch: 341, Testing Loss: 0.5366243720054626, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 342, Training Loss: 0.5013054013252258, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 342, Validation Loss: 0.5111806988716125, Validation Accuracy: 0.98\n",
            "Epoch: 342, Testing Loss: 0.5366175174713135, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 343, Training Loss: 0.5012951493263245, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 343, Validation Loss: 0.5111786127090454, Validation Accuracy: 0.98\n",
            "Epoch: 343, Testing Loss: 0.5366108417510986, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 344, Training Loss: 0.5012848377227783, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 344, Validation Loss: 0.5111764073371887, Validation Accuracy: 0.98\n",
            "Epoch: 344, Testing Loss: 0.5366041660308838, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 345, Training Loss: 0.5012747049331665, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 345, Validation Loss: 0.5111742615699768, Validation Accuracy: 0.98\n",
            "Epoch: 345, Testing Loss: 0.5365974307060242, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 346, Training Loss: 0.5012645125389099, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 346, Validation Loss: 0.5111720561981201, Validation Accuracy: 0.98\n",
            "Epoch: 346, Testing Loss: 0.5365906357765198, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 347, Training Loss: 0.5012543797492981, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 347, Validation Loss: 0.5111677646636963, Validation Accuracy: 0.98\n",
            "Epoch: 347, Testing Loss: 0.5365766882896423, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 348, Training Loss: 0.501244306564331, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 348, Validation Loss: 0.5111678838729858, Validation Accuracy: 0.98\n",
            "Epoch: 348, Testing Loss: 0.5365774035453796, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 349, Training Loss: 0.5012341737747192, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 349, Validation Loss: 0.5111657977104187, Validation Accuracy: 0.98\n",
            "Epoch: 349, Testing Loss: 0.5365707874298096, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 350, Training Loss: 0.501224160194397, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 350, Validation Loss: 0.5111637711524963, Validation Accuracy: 0.98\n",
            "Epoch: 350, Testing Loss: 0.5365641713142395, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 351, Training Loss: 0.5012142062187195, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 351, Validation Loss: 0.5111616253852844, Validation Accuracy: 0.98\n",
            "Epoch: 351, Testing Loss: 0.5365576148033142, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 352, Training Loss: 0.5012043118476868, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 352, Validation Loss: 0.5111596584320068, Validation Accuracy: 0.98\n",
            "Epoch: 352, Testing Loss: 0.5365509986877441, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 353, Training Loss: 0.5011942982673645, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 353, Validation Loss: 0.5111575722694397, Validation Accuracy: 0.98\n",
            "Epoch: 353, Testing Loss: 0.5365445613861084, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 354, Training Loss: 0.5011845231056213, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 354, Validation Loss: 0.5111554861068726, Validation Accuracy: 0.98\n",
            "Epoch: 354, Testing Loss: 0.5365380048751831, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 355, Training Loss: 0.5011746287345886, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 355, Validation Loss: 0.5111534595489502, Validation Accuracy: 0.98\n",
            "Epoch: 355, Testing Loss: 0.5365315675735474, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 356, Training Loss: 0.5011647939682007, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 356, Validation Loss: 0.5111514925956726, Validation Accuracy: 0.98\n",
            "Epoch: 356, Testing Loss: 0.5365250706672668, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 357, Training Loss: 0.5011550188064575, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 357, Validation Loss: 0.5111494660377502, Validation Accuracy: 0.98\n",
            "Epoch: 357, Testing Loss: 0.5365185737609863, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 358, Training Loss: 0.5011453032493591, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 358, Validation Loss: 0.5111474990844727, Validation Accuracy: 0.98\n",
            "Epoch: 358, Testing Loss: 0.5365121960639954, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 359, Training Loss: 0.5011355876922607, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 359, Validation Loss: 0.5111455917358398, Validation Accuracy: 0.98\n",
            "Epoch: 359, Testing Loss: 0.5365057587623596, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 360, Training Loss: 0.5011259317398071, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 360, Validation Loss: 0.5111436247825623, Validation Accuracy: 0.98\n",
            "Epoch: 360, Testing Loss: 0.5364994406700134, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 361, Training Loss: 0.5011162161827087, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 361, Validation Loss: 0.5111417174339294, Validation Accuracy: 0.98\n",
            "Epoch: 361, Testing Loss: 0.5364930629730225, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 362, Training Loss: 0.5011065602302551, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 362, Validation Loss: 0.5111397504806519, Validation Accuracy: 0.98\n",
            "Epoch: 362, Testing Loss: 0.5364867448806763, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 363, Training Loss: 0.5010970234870911, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 363, Validation Loss: 0.511137843132019, Validation Accuracy: 0.98\n",
            "Epoch: 363, Testing Loss: 0.5364804267883301, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 364, Training Loss: 0.501087486743927, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 364, Validation Loss: 0.5111359357833862, Validation Accuracy: 0.98\n",
            "Epoch: 364, Testing Loss: 0.5364741683006287, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 365, Training Loss: 0.5010780692100525, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 365, Validation Loss: 0.5111340880393982, Validation Accuracy: 0.98\n",
            "Epoch: 365, Testing Loss: 0.5364678502082825, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 366, Training Loss: 0.5010685324668884, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 366, Validation Loss: 0.5111321806907654, Validation Accuracy: 0.98\n",
            "Epoch: 366, Testing Loss: 0.536461591720581, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 367, Training Loss: 0.5010591149330139, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 367, Validation Loss: 0.5111303329467773, Validation Accuracy: 0.98\n",
            "Epoch: 367, Testing Loss: 0.5364553928375244, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 368, Training Loss: 0.5010496973991394, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 368, Validation Loss: 0.5111284852027893, Validation Accuracy: 0.98\n",
            "Epoch: 368, Testing Loss: 0.5364492535591125, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 369, Training Loss: 0.5010402798652649, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 369, Validation Loss: 0.511126697063446, Validation Accuracy: 0.98\n",
            "Epoch: 369, Testing Loss: 0.5364431142807007, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 370, Training Loss: 0.5010309219360352, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 370, Validation Loss: 0.511124849319458, Validation Accuracy: 0.98\n",
            "Epoch: 370, Testing Loss: 0.536436915397644, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 371, Training Loss: 0.5010216236114502, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 371, Validation Loss: 0.51112300157547, Validation Accuracy: 0.98\n",
            "Epoch: 371, Testing Loss: 0.536430835723877, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 372, Training Loss: 0.5010123252868652, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 372, Validation Loss: 0.5111211538314819, Validation Accuracy: 0.98\n",
            "Epoch: 372, Testing Loss: 0.5364247560501099, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 373, Training Loss: 0.5010030269622803, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 373, Validation Loss: 0.5111194849014282, Validation Accuracy: 0.98\n",
            "Epoch: 373, Testing Loss: 0.536418616771698, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 374, Training Loss: 0.5009937882423401, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 374, Validation Loss: 0.5111177563667297, Validation Accuracy: 0.98\n",
            "Epoch: 374, Testing Loss: 0.5364125967025757, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 375, Training Loss: 0.5009846091270447, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 375, Validation Loss: 0.5111159682273865, Validation Accuracy: 0.98\n",
            "Epoch: 375, Testing Loss: 0.5364065766334534, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 376, Training Loss: 0.5009754300117493, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 376, Validation Loss: 0.5111141800880432, Validation Accuracy: 0.98\n",
            "Epoch: 376, Testing Loss: 0.5364006161689758, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 377, Training Loss: 0.5009663105010986, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 377, Validation Loss: 0.5111124515533447, Validation Accuracy: 0.98\n",
            "Epoch: 377, Testing Loss: 0.5363946557044983, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 378, Training Loss: 0.500957190990448, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 378, Validation Loss: 0.511110782623291, Validation Accuracy: 0.98\n",
            "Epoch: 378, Testing Loss: 0.536388635635376, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 379, Training Loss: 0.5009481310844421, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 379, Validation Loss: 0.5111091136932373, Validation Accuracy: 0.98\n",
            "Epoch: 379, Testing Loss: 0.5363827347755432, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 380, Training Loss: 0.5009390115737915, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 380, Validation Loss: 0.5111074447631836, Validation Accuracy: 0.98\n",
            "Epoch: 380, Testing Loss: 0.5363768339157104, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 381, Training Loss: 0.5009300708770752, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 381, Validation Loss: 0.5111057162284851, Validation Accuracy: 0.98\n",
            "Epoch: 381, Testing Loss: 0.5363709330558777, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 382, Training Loss: 0.5009210705757141, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 382, Validation Loss: 0.5111041069030762, Validation Accuracy: 0.98\n",
            "Epoch: 382, Testing Loss: 0.5363650918006897, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 383, Training Loss: 0.5009120106697083, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 383, Validation Loss: 0.5111024975776672, Validation Accuracy: 0.98\n",
            "Epoch: 383, Testing Loss: 0.5363592505455017, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 384, Training Loss: 0.5009031295776367, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 384, Validation Loss: 0.5111008286476135, Validation Accuracy: 0.98\n",
            "Epoch: 384, Testing Loss: 0.5363534688949585, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 385, Training Loss: 0.5008941888809204, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 385, Validation Loss: 0.5110992193222046, Validation Accuracy: 0.98\n",
            "Epoch: 385, Testing Loss: 0.5363476872444153, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 386, Training Loss: 0.5008853673934937, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 386, Validation Loss: 0.5110976099967957, Validation Accuracy: 0.98\n",
            "Epoch: 386, Testing Loss: 0.5363419651985168, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 387, Training Loss: 0.5008764863014221, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 387, Validation Loss: 0.5110960006713867, Validation Accuracy: 0.98\n",
            "Epoch: 387, Testing Loss: 0.5363362431526184, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 388, Training Loss: 0.5008676648139954, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 388, Validation Loss: 0.5110944509506226, Validation Accuracy: 0.98\n",
            "Epoch: 388, Testing Loss: 0.53633052110672, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 389, Training Loss: 0.5008587837219238, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 389, Validation Loss: 0.5110929012298584, Validation Accuracy: 0.98\n",
            "Epoch: 389, Testing Loss: 0.5363248586654663, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 390, Training Loss: 0.5008500218391418, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 390, Validation Loss: 0.5110913515090942, Validation Accuracy: 0.98\n",
            "Epoch: 390, Testing Loss: 0.5363192558288574, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 391, Training Loss: 0.5008413195610046, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 391, Validation Loss: 0.5110898613929749, Validation Accuracy: 0.98\n",
            "Epoch: 391, Testing Loss: 0.5363135933876038, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 392, Training Loss: 0.5008326172828674, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 392, Validation Loss: 0.5110883116722107, Validation Accuracy: 0.98\n",
            "Epoch: 392, Testing Loss: 0.5363080501556396, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 393, Training Loss: 0.5008239150047302, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 393, Validation Loss: 0.5110868215560913, Validation Accuracy: 0.98\n",
            "Epoch: 393, Testing Loss: 0.5363024473190308, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 394, Training Loss: 0.5008152723312378, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 394, Validation Loss: 0.5110853314399719, Validation Accuracy: 0.98\n",
            "Epoch: 394, Testing Loss: 0.5362968444824219, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 395, Training Loss: 0.5008066892623901, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 395, Validation Loss: 0.5110838413238525, Validation Accuracy: 0.98\n",
            "Epoch: 395, Testing Loss: 0.5362914204597473, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 396, Training Loss: 0.5007979869842529, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 396, Validation Loss: 0.5110824108123779, Validation Accuracy: 0.98\n",
            "Epoch: 396, Testing Loss: 0.5362858772277832, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 397, Training Loss: 0.50078946352005, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 397, Validation Loss: 0.5110809803009033, Validation Accuracy: 0.98\n",
            "Epoch: 397, Testing Loss: 0.5362803936004639, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 398, Training Loss: 0.5007808804512024, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 398, Validation Loss: 0.5110795497894287, Validation Accuracy: 0.98\n",
            "Epoch: 398, Testing Loss: 0.5362688899040222, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 399, Training Loss: 0.5007723569869995, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 399, Validation Loss: 0.5110781192779541, Validation Accuracy: 0.98\n",
            "Epoch: 399, Testing Loss: 0.53626948595047, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 400, Training Loss: 0.5007638931274414, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 400, Validation Loss: 0.5110767483711243, Validation Accuracy: 0.98\n",
            "Epoch: 400, Testing Loss: 0.5362641215324402, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 401, Training Loss: 0.5007553696632385, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 401, Validation Loss: 0.5110753178596497, Validation Accuracy: 0.98\n",
            "Epoch: 401, Testing Loss: 0.5362587571144104, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 402, Training Loss: 0.5007469654083252, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 402, Validation Loss: 0.5110739469528198, Validation Accuracy: 0.98\n",
            "Epoch: 402, Testing Loss: 0.5362533926963806, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 403, Training Loss: 0.5007385015487671, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 403, Validation Loss: 0.51107257604599, Validation Accuracy: 0.98\n",
            "Epoch: 403, Testing Loss: 0.5362481474876404, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 404, Training Loss: 0.5007300972938538, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 404, Validation Loss: 0.5110712647438049, Validation Accuracy: 0.98\n",
            "Epoch: 404, Testing Loss: 0.5362428426742554, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 405, Training Loss: 0.50072181224823, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 405, Validation Loss: 0.5110698938369751, Validation Accuracy: 0.98\n",
            "Epoch: 405, Testing Loss: 0.5362375378608704, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 406, Training Loss: 0.5007134675979614, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 406, Validation Loss: 0.51106858253479, Validation Accuracy: 0.98\n",
            "Epoch: 406, Testing Loss: 0.5362322330474854, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 407, Training Loss: 0.5007051229476929, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 407, Validation Loss: 0.5110673308372498, Validation Accuracy: 0.98\n",
            "Epoch: 407, Testing Loss: 0.5362271070480347, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 408, Training Loss: 0.5006967782974243, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 408, Validation Loss: 0.5110660195350647, Validation Accuracy: 0.98\n",
            "Epoch: 408, Testing Loss: 0.5362219214439392, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 409, Training Loss: 0.5006885528564453, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 409, Validation Loss: 0.5110647082328796, Validation Accuracy: 0.98\n",
            "Epoch: 409, Testing Loss: 0.5362167358398438, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 410, Training Loss: 0.5006803274154663, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 410, Validation Loss: 0.5110635161399841, Validation Accuracy: 0.98\n",
            "Epoch: 410, Testing Loss: 0.5362116098403931, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 411, Training Loss: 0.5006721019744873, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 411, Validation Loss: 0.5110622644424438, Validation Accuracy: 0.98\n",
            "Epoch: 411, Testing Loss: 0.5362064838409424, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 412, Training Loss: 0.5006639361381531, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 412, Validation Loss: 0.5110610723495483, Validation Accuracy: 0.98\n",
            "Epoch: 412, Testing Loss: 0.5362014174461365, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 413, Training Loss: 0.5006557703018188, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 413, Validation Loss: 0.5110598206520081, Validation Accuracy: 0.98\n",
            "Epoch: 413, Testing Loss: 0.5361963510513306, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 414, Training Loss: 0.5006476044654846, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 414, Validation Loss: 0.5110586285591125, Validation Accuracy: 0.98\n",
            "Epoch: 414, Testing Loss: 0.5361913442611694, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 415, Training Loss: 0.5006395578384399, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 415, Validation Loss: 0.5110573768615723, Validation Accuracy: 0.98\n",
            "Epoch: 415, Testing Loss: 0.5361863970756531, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 416, Training Loss: 0.5006314516067505, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 416, Validation Loss: 0.5110562443733215, Validation Accuracy: 0.98\n",
            "Epoch: 416, Testing Loss: 0.5361813902854919, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 417, Training Loss: 0.5006234049797058, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 417, Validation Loss: 0.5110551118850708, Validation Accuracy: 0.98\n",
            "Epoch: 417, Testing Loss: 0.5361765027046204, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 418, Training Loss: 0.5006152987480164, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 418, Validation Loss: 0.5110539793968201, Validation Accuracy: 0.98\n",
            "Epoch: 418, Testing Loss: 0.536171555519104, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 419, Training Loss: 0.5006073713302612, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 419, Validation Loss: 0.5110528469085693, Validation Accuracy: 0.98\n",
            "Epoch: 419, Testing Loss: 0.5361666083335876, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 420, Training Loss: 0.5005993247032166, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 420, Validation Loss: 0.5110517144203186, Validation Accuracy: 0.98\n",
            "Epoch: 420, Testing Loss: 0.5361617803573608, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 421, Training Loss: 0.5005913376808167, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 421, Validation Loss: 0.5110505819320679, Validation Accuracy: 0.98\n",
            "Epoch: 421, Testing Loss: 0.5361568927764893, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 422, Training Loss: 0.5005834698677063, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 422, Validation Loss: 0.5110495090484619, Validation Accuracy: 0.98\n",
            "Epoch: 422, Testing Loss: 0.5361520648002625, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 423, Training Loss: 0.5005755424499512, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 423, Validation Loss: 0.5110484957695007, Validation Accuracy: 0.98\n",
            "Epoch: 423, Testing Loss: 0.5361472964286804, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 424, Training Loss: 0.500567615032196, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 424, Validation Loss: 0.5110474228858948, Validation Accuracy: 0.98\n",
            "Epoch: 424, Testing Loss: 0.5361424684524536, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 425, Training Loss: 0.5005598068237305, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 425, Validation Loss: 0.5110464096069336, Validation Accuracy: 0.98\n",
            "Epoch: 425, Testing Loss: 0.5361377596855164, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 426, Training Loss: 0.5005519986152649, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 426, Validation Loss: 0.5110453367233276, Validation Accuracy: 0.98\n",
            "Epoch: 426, Testing Loss: 0.5361330509185791, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 427, Training Loss: 0.5005441904067993, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 427, Validation Loss: 0.5110443234443665, Validation Accuracy: 0.98\n",
            "Epoch: 427, Testing Loss: 0.5361282825469971, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 428, Training Loss: 0.5005363821983337, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 428, Validation Loss: 0.51104336977005, Validation Accuracy: 0.98\n",
            "Epoch: 428, Testing Loss: 0.5361236333847046, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 429, Training Loss: 0.5005286931991577, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 429, Validation Loss: 0.5110423564910889, Validation Accuracy: 0.98\n",
            "Epoch: 429, Testing Loss: 0.5361190438270569, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 430, Training Loss: 0.5005209445953369, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 430, Validation Loss: 0.5110414028167725, Validation Accuracy: 0.98\n",
            "Epoch: 430, Testing Loss: 0.5361143350601196, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 431, Training Loss: 0.5005131959915161, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 431, Validation Loss: 0.511040449142456, Validation Accuracy: 0.98\n",
            "Epoch: 431, Testing Loss: 0.5361097455024719, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 432, Training Loss: 0.5005055665969849, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 432, Validation Loss: 0.5110394954681396, Validation Accuracy: 0.98\n",
            "Epoch: 432, Testing Loss: 0.536105215549469, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 433, Training Loss: 0.5004978775978088, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 433, Validation Loss: 0.511038601398468, Validation Accuracy: 0.98\n",
            "Epoch: 433, Testing Loss: 0.5361006855964661, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 434, Training Loss: 0.5004902482032776, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 434, Validation Loss: 0.5110376477241516, Validation Accuracy: 0.98\n",
            "Epoch: 434, Testing Loss: 0.5360960960388184, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 435, Training Loss: 0.5004826188087463, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 435, Validation Loss: 0.5110368132591248, Validation Accuracy: 0.98\n",
            "Epoch: 435, Testing Loss: 0.5360915660858154, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 436, Training Loss: 0.5004750490188599, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 436, Validation Loss: 0.5110358595848083, Validation Accuracy: 0.98\n",
            "Epoch: 436, Testing Loss: 0.5360870957374573, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 437, Training Loss: 0.5004674792289734, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 437, Validation Loss: 0.5110350251197815, Validation Accuracy: 0.98\n",
            "Epoch: 437, Testing Loss: 0.5360826253890991, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 438, Training Loss: 0.5004599690437317, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 438, Validation Loss: 0.5110341906547546, Validation Accuracy: 0.98\n",
            "Epoch: 438, Testing Loss: 0.5360782146453857, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 439, Training Loss: 0.5004523992538452, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 439, Validation Loss: 0.5110333561897278, Validation Accuracy: 0.98\n",
            "Epoch: 439, Testing Loss: 0.5360738039016724, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 440, Training Loss: 0.5004448890686035, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 440, Validation Loss: 0.5110325217247009, Validation Accuracy: 0.98\n",
            "Epoch: 440, Testing Loss: 0.5360693335533142, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 441, Training Loss: 0.5004374384880066, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 441, Validation Loss: 0.5110317468643188, Validation Accuracy: 0.98\n",
            "Epoch: 441, Testing Loss: 0.5360649228096008, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 442, Training Loss: 0.5004300475120544, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 442, Validation Loss: 0.5110294818878174, Validation Accuracy: 0.98\n",
            "Epoch: 442, Testing Loss: 0.5360581874847412, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 443, Training Loss: 0.5004225969314575, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 443, Validation Loss: 0.5110301375389099, Validation Accuracy: 0.98\n",
            "Epoch: 443, Testing Loss: 0.5360562801361084, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 444, Training Loss: 0.5004152059555054, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 444, Validation Loss: 0.511027991771698, Validation Accuracy: 0.98\n",
            "Epoch: 444, Testing Loss: 0.5360519289970398, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 445, Training Loss: 0.5004077553749084, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 445, Validation Loss: 0.5110286474227905, Validation Accuracy: 0.98\n",
            "Epoch: 445, Testing Loss: 0.5360475778579712, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 446, Training Loss: 0.5004004836082458, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 446, Validation Loss: 0.5110279321670532, Validation Accuracy: 0.98\n",
            "Epoch: 446, Testing Loss: 0.5360432863235474, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 447, Training Loss: 0.5003930926322937, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 447, Validation Loss: 0.5110271573066711, Validation Accuracy: 0.98\n",
            "Epoch: 447, Testing Loss: 0.5360390543937683, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 448, Training Loss: 0.5003858208656311, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 448, Validation Loss: 0.5110264420509338, Validation Accuracy: 0.98\n",
            "Epoch: 448, Testing Loss: 0.5360347628593445, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 449, Training Loss: 0.5003785490989685, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 449, Validation Loss: 0.5110257863998413, Validation Accuracy: 0.98\n",
            "Epoch: 449, Testing Loss: 0.5360305309295654, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 450, Training Loss: 0.5003712773323059, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 450, Validation Loss: 0.511025071144104, Validation Accuracy: 0.98\n",
            "Epoch: 450, Testing Loss: 0.5360262989997864, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 451, Training Loss: 0.5003640651702881, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 451, Validation Loss: 0.5110244154930115, Validation Accuracy: 0.98\n",
            "Epoch: 451, Testing Loss: 0.5360220670700073, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 452, Training Loss: 0.5003568530082703, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 452, Validation Loss: 0.511023759841919, Validation Accuracy: 0.98\n",
            "Epoch: 452, Testing Loss: 0.5360178351402283, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 453, Training Loss: 0.5003497004508972, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 453, Validation Loss: 0.5110230445861816, Validation Accuracy: 0.98\n",
            "Epoch: 453, Testing Loss: 0.536013662815094, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 454, Training Loss: 0.500342607498169, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 454, Validation Loss: 0.5110224485397339, Validation Accuracy: 0.98\n",
            "Epoch: 454, Testing Loss: 0.5360094904899597, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 455, Training Loss: 0.5003354549407959, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 455, Validation Loss: 0.5110218524932861, Validation Accuracy: 0.98\n",
            "Epoch: 455, Testing Loss: 0.5360052585601807, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 456, Training Loss: 0.5003283619880676, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 456, Validation Loss: 0.5110212564468384, Validation Accuracy: 0.98\n",
            "Epoch: 456, Testing Loss: 0.5360010862350464, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 457, Training Loss: 0.5003212690353394, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 457, Validation Loss: 0.5110206007957458, Validation Accuracy: 0.98\n",
            "Epoch: 457, Testing Loss: 0.5359969735145569, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 458, Training Loss: 0.5003141760826111, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 458, Validation Loss: 0.5110200047492981, Validation Accuracy: 0.98\n",
            "Epoch: 458, Testing Loss: 0.5359928607940674, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 459, Training Loss: 0.5003071427345276, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 459, Validation Loss: 0.5110194683074951, Validation Accuracy: 0.98\n",
            "Epoch: 459, Testing Loss: 0.5359886884689331, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 460, Training Loss: 0.5003001093864441, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 460, Validation Loss: 0.5110188722610474, Validation Accuracy: 0.98\n",
            "Epoch: 460, Testing Loss: 0.5359845161437988, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 461, Training Loss: 0.5002930760383606, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 461, Validation Loss: 0.5110183358192444, Validation Accuracy: 0.98\n",
            "Epoch: 461, Testing Loss: 0.5359803438186646, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 462, Training Loss: 0.5002861022949219, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 462, Validation Loss: 0.5110177993774414, Validation Accuracy: 0.98\n",
            "Epoch: 462, Testing Loss: 0.535976231098175, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 463, Training Loss: 0.5002791881561279, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 463, Validation Loss: 0.5110172629356384, Validation Accuracy: 0.98\n",
            "Epoch: 463, Testing Loss: 0.5359721183776855, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 464, Training Loss: 0.500272274017334, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 464, Validation Loss: 0.5110167264938354, Validation Accuracy: 0.98\n",
            "Epoch: 464, Testing Loss: 0.535968005657196, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 465, Training Loss: 0.50026535987854, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 465, Validation Loss: 0.5110161900520325, Validation Accuracy: 0.98\n",
            "Epoch: 465, Testing Loss: 0.5359638929367065, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 466, Training Loss: 0.5002585053443909, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 466, Validation Loss: 0.5110156536102295, Validation Accuracy: 0.98\n",
            "Epoch: 466, Testing Loss: 0.535959780216217, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 467, Training Loss: 0.5002515912055969, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 467, Validation Loss: 0.5110151767730713, Validation Accuracy: 0.98\n",
            "Epoch: 467, Testing Loss: 0.5359556674957275, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 468, Training Loss: 0.5002447962760925, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 468, Validation Loss: 0.5110146999359131, Validation Accuracy: 0.98\n",
            "Epoch: 468, Testing Loss: 0.5359514951705933, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 469, Training Loss: 0.5002380013465881, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 469, Validation Loss: 0.5110142230987549, Validation Accuracy: 0.98\n",
            "Epoch: 469, Testing Loss: 0.5359473824501038, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 470, Training Loss: 0.5002312064170837, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 470, Validation Loss: 0.5110137462615967, Validation Accuracy: 0.98\n",
            "Epoch: 470, Testing Loss: 0.5359432101249695, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 471, Training Loss: 0.5002244710922241, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 471, Validation Loss: 0.5110132694244385, Validation Accuracy: 0.98\n",
            "Epoch: 471, Testing Loss: 0.5359391570091248, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 472, Training Loss: 0.5002177357673645, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 472, Validation Loss: 0.511012852191925, Validation Accuracy: 0.98\n",
            "Epoch: 472, Testing Loss: 0.5359348654747009, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 473, Training Loss: 0.5002109408378601, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 473, Validation Loss: 0.5110123753547668, Validation Accuracy: 0.98\n",
            "Epoch: 473, Testing Loss: 0.5359308123588562, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 474, Training Loss: 0.5002042651176453, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 474, Validation Loss: 0.5110106468200684, Validation Accuracy: 0.98\n",
            "Epoch: 474, Testing Loss: 0.5359266400337219, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 475, Training Loss: 0.5001976490020752, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 475, Validation Loss: 0.5110114812850952, Validation Accuracy: 0.98\n",
            "Epoch: 475, Testing Loss: 0.5359224677085876, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 476, Training Loss: 0.5001909732818604, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 476, Validation Loss: 0.5110110640525818, Validation Accuracy: 0.98\n",
            "Epoch: 476, Testing Loss: 0.5359183549880981, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 477, Training Loss: 0.5001842975616455, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 477, Validation Loss: 0.5110107064247131, Validation Accuracy: 0.98\n",
            "Epoch: 477, Testing Loss: 0.5359141230583191, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 478, Training Loss: 0.5001776814460754, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 478, Validation Loss: 0.5110102891921997, Validation Accuracy: 0.98\n",
            "Epoch: 478, Testing Loss: 0.5359100103378296, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 479, Training Loss: 0.5001711249351501, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 479, Validation Loss: 0.5110098719596863, Validation Accuracy: 0.98\n",
            "Epoch: 479, Testing Loss: 0.5359057784080505, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 480, Training Loss: 0.5001645088195801, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 480, Validation Loss: 0.5110094547271729, Validation Accuracy: 0.98\n",
            "Epoch: 480, Testing Loss: 0.5359015464782715, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 481, Training Loss: 0.5001579523086548, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 481, Validation Loss: 0.5110090970993042, Validation Accuracy: 0.98\n",
            "Epoch: 481, Testing Loss: 0.5358973145484924, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 482, Training Loss: 0.500151515007019, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 482, Validation Loss: 0.5110087394714355, Validation Accuracy: 0.98\n",
            "Epoch: 482, Testing Loss: 0.5358930826187134, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 483, Training Loss: 0.5001449584960938, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 483, Validation Loss: 0.5110083222389221, Validation Accuracy: 0.98\n",
            "Epoch: 483, Testing Loss: 0.5358889102935791, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 484, Training Loss: 0.500138521194458, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 484, Validation Loss: 0.5110079646110535, Validation Accuracy: 0.98\n",
            "Epoch: 484, Testing Loss: 0.5358846187591553, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 485, Training Loss: 0.5001320242881775, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 485, Validation Loss: 0.5110076069831848, Validation Accuracy: 0.98\n",
            "Epoch: 485, Testing Loss: 0.5358803868293762, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 486, Training Loss: 0.5001255869865417, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 486, Validation Loss: 0.5110072493553162, Validation Accuracy: 0.98\n",
            "Epoch: 486, Testing Loss: 0.5358761548995972, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 487, Training Loss: 0.5001192092895508, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 487, Validation Loss: 0.5110069513320923, Validation Accuracy: 0.98\n",
            "Epoch: 487, Testing Loss: 0.5358718037605286, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 488, Training Loss: 0.500112771987915, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 488, Validation Loss: 0.5110065937042236, Validation Accuracy: 0.98\n",
            "Epoch: 488, Testing Loss: 0.5358675122261047, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 489, Training Loss: 0.5001063942909241, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 489, Validation Loss: 0.5110062956809998, Validation Accuracy: 0.98\n",
            "Epoch: 489, Testing Loss: 0.5358632206916809, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 490, Training Loss: 0.5001000761985779, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 490, Validation Loss: 0.5110059976577759, Validation Accuracy: 0.98\n",
            "Epoch: 490, Testing Loss: 0.5358588695526123, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 491, Training Loss: 0.5000937581062317, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 491, Validation Loss: 0.5110055804252625, Validation Accuracy: 0.98\n",
            "Epoch: 491, Testing Loss: 0.5358545780181885, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 492, Training Loss: 0.5000874996185303, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 492, Validation Loss: 0.5110052824020386, Validation Accuracy: 0.98\n",
            "Epoch: 492, Testing Loss: 0.5358502864837646, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 493, Training Loss: 0.5000811815261841, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 493, Validation Loss: 0.5110049843788147, Validation Accuracy: 0.98\n",
            "Epoch: 493, Testing Loss: 0.5358458757400513, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 494, Training Loss: 0.5000749230384827, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 494, Validation Loss: 0.5110046863555908, Validation Accuracy: 0.98\n",
            "Epoch: 494, Testing Loss: 0.5358415246009827, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 495, Training Loss: 0.500068724155426, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 495, Validation Loss: 0.5110044479370117, Validation Accuracy: 0.98\n",
            "Epoch: 495, Testing Loss: 0.5358372330665588, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 496, Training Loss: 0.5000624060630798, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 496, Validation Loss: 0.5110041499137878, Validation Accuracy: 0.98\n",
            "Epoch: 496, Testing Loss: 0.5358328223228455, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 497, Training Loss: 0.500056266784668, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 497, Validation Loss: 0.511003851890564, Validation Accuracy: 0.98\n",
            "Epoch: 497, Testing Loss: 0.5358284711837769, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 498, Training Loss: 0.5000500679016113, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 498, Validation Loss: 0.5110036134719849, Validation Accuracy: 0.98\n",
            "Epoch: 498, Testing Loss: 0.5358240604400635, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 499, Training Loss: 0.5000439882278442, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 499, Validation Loss: 0.511003315448761, Validation Accuracy: 0.98\n",
            "Epoch: 499, Testing Loss: 0.5358197093009949, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 500, Training Loss: 0.5000378489494324, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 500, Validation Loss: 0.5110030770301819, Validation Accuracy: 0.98\n",
            "Epoch: 500, Testing Loss: 0.5358152985572815, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 501, Training Loss: 0.5000317692756653, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 501, Validation Loss: 0.5110028386116028, Validation Accuracy: 0.98\n",
            "Epoch: 501, Testing Loss: 0.5358109474182129, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 502, Training Loss: 0.5000256896018982, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 502, Validation Loss: 0.5110026001930237, Validation Accuracy: 0.98\n",
            "Epoch: 502, Testing Loss: 0.5358065366744995, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 503, Training Loss: 0.5000195503234863, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 503, Validation Loss: 0.5110023617744446, Validation Accuracy: 0.98\n",
            "Epoch: 503, Testing Loss: 0.5358020663261414, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 504, Training Loss: 0.500013530254364, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 504, Validation Loss: 0.5110021829605103, Validation Accuracy: 0.98\n",
            "Epoch: 504, Testing Loss: 0.5357977747917175, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 505, Training Loss: 0.5000075101852417, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 505, Validation Loss: 0.5110019445419312, Validation Accuracy: 0.98\n",
            "Epoch: 505, Testing Loss: 0.5357933044433594, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 506, Training Loss: 0.5000015497207642, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 506, Validation Loss: 0.5110017657279968, Validation Accuracy: 0.98\n",
            "Epoch: 506, Testing Loss: 0.535788893699646, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 507, Training Loss: 0.49999549984931946, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 507, Validation Loss: 0.5110015273094177, Validation Accuracy: 0.98\n",
            "Epoch: 507, Testing Loss: 0.5357845425605774, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 508, Training Loss: 0.4999895393848419, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 508, Validation Loss: 0.5110014081001282, Validation Accuracy: 0.98\n",
            "Epoch: 508, Testing Loss: 0.5357801914215088, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 509, Training Loss: 0.4999835789203644, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 509, Validation Loss: 0.5110012292861938, Validation Accuracy: 0.98\n",
            "Epoch: 509, Testing Loss: 0.5357757806777954, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 510, Training Loss: 0.4999776780605316, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 510, Validation Loss: 0.5110011100769043, Validation Accuracy: 0.98\n",
            "Epoch: 510, Testing Loss: 0.535771369934082, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 511, Training Loss: 0.49997180700302124, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 511, Validation Loss: 0.5110008716583252, Validation Accuracy: 0.98\n",
            "Epoch: 511, Testing Loss: 0.5357670187950134, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 512, Training Loss: 0.4999659061431885, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 512, Validation Loss: 0.5110008120536804, Validation Accuracy: 0.98\n",
            "Epoch: 512, Testing Loss: 0.5357627272605896, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 513, Training Loss: 0.4999600946903229, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 513, Validation Loss: 0.5110006332397461, Validation Accuracy: 0.98\n",
            "Epoch: 513, Testing Loss: 0.5357583165168762, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 514, Training Loss: 0.4999542534351349, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 514, Validation Loss: 0.5110005736351013, Validation Accuracy: 0.98\n",
            "Epoch: 514, Testing Loss: 0.5357539653778076, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 515, Training Loss: 0.4999484121799469, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 515, Validation Loss: 0.5110004544258118, Validation Accuracy: 0.98\n",
            "Epoch: 515, Testing Loss: 0.5357496738433838, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 516, Training Loss: 0.49994271993637085, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 516, Validation Loss: 0.511000394821167, Validation Accuracy: 0.98\n",
            "Epoch: 516, Testing Loss: 0.5357453227043152, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 517, Training Loss: 0.49993690848350525, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 517, Validation Loss: 0.5110003352165222, Validation Accuracy: 0.98\n",
            "Epoch: 517, Testing Loss: 0.5357410311698914, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 518, Training Loss: 0.4999312162399292, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 518, Validation Loss: 0.5110002756118774, Validation Accuracy: 0.98\n",
            "Epoch: 518, Testing Loss: 0.5357367396354675, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 519, Training Loss: 0.49992549419403076, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 519, Validation Loss: 0.5110002160072327, Validation Accuracy: 0.98\n",
            "Epoch: 519, Testing Loss: 0.5357325077056885, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 520, Training Loss: 0.4999197721481323, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 520, Validation Loss: 0.5110001564025879, Validation Accuracy: 0.98\n",
            "Epoch: 520, Testing Loss: 0.5357282161712646, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 521, Training Loss: 0.4999140799045563, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 521, Validation Loss: 0.5110001564025879, Validation Accuracy: 0.98\n",
            "Epoch: 521, Testing Loss: 0.5357239842414856, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 522, Training Loss: 0.499908447265625, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 522, Validation Loss: 0.5110001564025879, Validation Accuracy: 0.98\n",
            "Epoch: 522, Testing Loss: 0.5357198119163513, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 523, Training Loss: 0.49990278482437134, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 523, Validation Loss: 0.5110001564025879, Validation Accuracy: 0.98\n",
            "Epoch: 523, Testing Loss: 0.535715639591217, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 524, Training Loss: 0.49989715218544006, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 524, Validation Loss: 0.5110002160072327, Validation Accuracy: 0.98\n",
            "Epoch: 524, Testing Loss: 0.5357114672660828, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 525, Training Loss: 0.49989160895347595, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 525, Validation Loss: 0.5110002756118774, Validation Accuracy: 0.98\n",
            "Epoch: 525, Testing Loss: 0.5357073545455933, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 526, Training Loss: 0.49988609552383423, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 526, Validation Loss: 0.5110003352165222, Validation Accuracy: 0.98\n",
            "Epoch: 526, Testing Loss: 0.535703182220459, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 527, Training Loss: 0.4998805522918701, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 527, Validation Loss: 0.511000394821167, Validation Accuracy: 0.98\n",
            "Epoch: 527, Testing Loss: 0.535699188709259, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 528, Training Loss: 0.499875009059906, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 528, Validation Loss: 0.5110004544258118, Validation Accuracy: 0.98\n",
            "Epoch: 528, Testing Loss: 0.5356950759887695, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 529, Training Loss: 0.49986955523490906, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 529, Validation Loss: 0.5110006928443909, Validation Accuracy: 0.98\n",
            "Epoch: 529, Testing Loss: 0.5356879234313965, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 530, Training Loss: 0.49986401200294495, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 530, Validation Loss: 0.5110008120536804, Validation Accuracy: 0.98\n",
            "Epoch: 530, Testing Loss: 0.5356870889663696, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 531, Training Loss: 0.499858558177948, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 531, Validation Loss: 0.5110008716583252, Validation Accuracy: 0.98\n",
            "Epoch: 531, Testing Loss: 0.5356831550598145, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 532, Training Loss: 0.4998532235622406, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 532, Validation Loss: 0.5110011100769043, Validation Accuracy: 0.98\n",
            "Epoch: 532, Testing Loss: 0.5356792211532593, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 533, Training Loss: 0.49984779953956604, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 533, Validation Loss: 0.5110013484954834, Validation Accuracy: 0.98\n",
            "Epoch: 533, Testing Loss: 0.5356752872467041, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 534, Training Loss: 0.49984240531921387, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 534, Validation Loss: 0.5110015273094177, Validation Accuracy: 0.98\n",
            "Epoch: 534, Testing Loss: 0.5356714129447937, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 535, Training Loss: 0.4998370409011841, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 535, Validation Loss: 0.5110017657279968, Validation Accuracy: 0.98\n",
            "Epoch: 535, Testing Loss: 0.5356676578521729, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 536, Training Loss: 0.4998317062854767, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 536, Validation Loss: 0.5110020637512207, Validation Accuracy: 0.98\n",
            "Epoch: 536, Testing Loss: 0.535663902759552, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 537, Training Loss: 0.49982643127441406, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 537, Validation Loss: 0.5110023021697998, Validation Accuracy: 0.98\n",
            "Epoch: 537, Testing Loss: 0.5356600880622864, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 538, Training Loss: 0.49982115626335144, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 538, Validation Loss: 0.5110026597976685, Validation Accuracy: 0.98\n",
            "Epoch: 538, Testing Loss: 0.5356563329696655, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 539, Training Loss: 0.4998158812522888, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 539, Validation Loss: 0.5110029578208923, Validation Accuracy: 0.98\n",
            "Epoch: 539, Testing Loss: 0.5356526374816895, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 540, Training Loss: 0.49981069564819336, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 540, Validation Loss: 0.511003315448761, Validation Accuracy: 0.98\n",
            "Epoch: 540, Testing Loss: 0.5356489419937134, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 541, Training Loss: 0.49980539083480835, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 541, Validation Loss: 0.5110036730766296, Validation Accuracy: 0.98\n",
            "Epoch: 541, Testing Loss: 0.5356453657150269, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 542, Training Loss: 0.4998002350330353, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 542, Validation Loss: 0.5110040903091431, Validation Accuracy: 0.98\n",
            "Epoch: 542, Testing Loss: 0.5356418490409851, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 543, Training Loss: 0.4997951090335846, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 543, Validation Loss: 0.5110045075416565, Validation Accuracy: 0.98\n",
            "Epoch: 543, Testing Loss: 0.5356383323669434, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 544, Training Loss: 0.49978989362716675, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 544, Validation Loss: 0.5110049843788147, Validation Accuracy: 0.98\n",
            "Epoch: 544, Testing Loss: 0.5356348752975464, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 545, Training Loss: 0.4997848570346832, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 545, Validation Loss: 0.5110054612159729, Validation Accuracy: 0.98\n",
            "Epoch: 545, Testing Loss: 0.5356313586235046, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 546, Training Loss: 0.49977970123291016, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 546, Validation Loss: 0.5110059976577759, Validation Accuracy: 0.98\n",
            "Epoch: 546, Testing Loss: 0.5356280207633972, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 547, Training Loss: 0.499774694442749, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 547, Validation Loss: 0.5110064744949341, Validation Accuracy: 0.98\n",
            "Epoch: 547, Testing Loss: 0.5356245636940002, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 548, Training Loss: 0.4997696280479431, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 548, Validation Loss: 0.5110070705413818, Validation Accuracy: 0.98\n",
            "Epoch: 548, Testing Loss: 0.5356212854385376, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 549, Training Loss: 0.4997645318508148, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 549, Validation Loss: 0.51100754737854, Validation Accuracy: 0.98\n",
            "Epoch: 549, Testing Loss: 0.5356179475784302, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 550, Training Loss: 0.49975961446762085, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 550, Validation Loss: 0.5110082030296326, Validation Accuracy: 0.98\n",
            "Epoch: 550, Testing Loss: 0.5356147289276123, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 551, Training Loss: 0.4997546374797821, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 551, Validation Loss: 0.5110087990760803, Validation Accuracy: 0.98\n",
            "Epoch: 551, Testing Loss: 0.5356115102767944, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 552, Training Loss: 0.49974966049194336, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 552, Validation Loss: 0.5110093951225281, Validation Accuracy: 0.98\n",
            "Epoch: 552, Testing Loss: 0.5356083512306213, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 553, Training Loss: 0.4997447729110718, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 553, Validation Loss: 0.5110100507736206, Validation Accuracy: 0.98\n",
            "Epoch: 553, Testing Loss: 0.5356051921844482, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 554, Training Loss: 0.4997398555278778, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 554, Validation Loss: 0.5110107064247131, Validation Accuracy: 0.98\n",
            "Epoch: 554, Testing Loss: 0.5356020331382751, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 555, Training Loss: 0.49973493814468384, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 555, Validation Loss: 0.5110113620758057, Validation Accuracy: 0.98\n",
            "Epoch: 555, Testing Loss: 0.5355989933013916, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 556, Training Loss: 0.49973008036613464, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 556, Validation Loss: 0.511012077331543, Validation Accuracy: 0.98\n",
            "Epoch: 556, Testing Loss: 0.5355958938598633, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 557, Training Loss: 0.49972525238990784, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 557, Validation Loss: 0.5110127925872803, Validation Accuracy: 0.98\n",
            "Epoch: 557, Testing Loss: 0.5355929732322693, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 558, Training Loss: 0.4997204542160034, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 558, Validation Loss: 0.5110135674476624, Validation Accuracy: 0.98\n",
            "Epoch: 558, Testing Loss: 0.5355875492095947, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 559, Training Loss: 0.4997156262397766, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 559, Validation Loss: 0.5110142230987549, Validation Accuracy: 0.98\n",
            "Epoch: 559, Testing Loss: 0.5355870723724365, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 560, Training Loss: 0.4997108578681946, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 560, Validation Loss: 0.5110150575637817, Validation Accuracy: 0.98\n",
            "Epoch: 560, Testing Loss: 0.5355841517448425, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 561, Training Loss: 0.4997061491012573, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 561, Validation Loss: 0.5110157132148743, Validation Accuracy: 0.98\n",
            "Epoch: 561, Testing Loss: 0.5355812311172485, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 562, Training Loss: 0.49970144033432007, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 562, Validation Loss: 0.5110165476799011, Validation Accuracy: 0.98\n",
            "Epoch: 562, Testing Loss: 0.5355783104896545, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 563, Training Loss: 0.4996967017650604, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 563, Validation Loss: 0.5110173225402832, Validation Accuracy: 0.98\n",
            "Epoch: 563, Testing Loss: 0.5355755686759949, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 564, Training Loss: 0.49969199299812317, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 564, Validation Loss: 0.5110180377960205, Validation Accuracy: 0.98\n",
            "Epoch: 564, Testing Loss: 0.5355727076530457, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 565, Training Loss: 0.4996873140335083, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 565, Validation Loss: 0.5110188722610474, Validation Accuracy: 0.98\n",
            "Epoch: 565, Testing Loss: 0.5355699062347412, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 566, Training Loss: 0.4996826648712158, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 566, Validation Loss: 0.5110195279121399, Validation Accuracy: 0.98\n",
            "Epoch: 566, Testing Loss: 0.5355671644210815, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 567, Training Loss: 0.4996780455112457, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 567, Validation Loss: 0.511020302772522, Validation Accuracy: 0.98\n",
            "Epoch: 567, Testing Loss: 0.5355644226074219, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 568, Training Loss: 0.49967333674430847, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 568, Validation Loss: 0.511021077632904, Validation Accuracy: 0.98\n",
            "Epoch: 568, Testing Loss: 0.5355616807937622, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 569, Training Loss: 0.49966874718666077, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 569, Validation Loss: 0.5110218524932861, Validation Accuracy: 0.98\n",
            "Epoch: 569, Testing Loss: 0.5355589389801025, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 570, Training Loss: 0.49966421723365784, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 570, Validation Loss: 0.5110225081443787, Validation Accuracy: 0.98\n",
            "Epoch: 570, Testing Loss: 0.5355561971664429, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 571, Training Loss: 0.49965962767601013, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 571, Validation Loss: 0.511023223400116, Validation Accuracy: 0.98\n",
            "Epoch: 571, Testing Loss: 0.535553514957428, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 572, Training Loss: 0.49965500831604004, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 572, Validation Loss: 0.5110238790512085, Validation Accuracy: 0.98\n",
            "Epoch: 572, Testing Loss: 0.5355508327484131, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 573, Training Loss: 0.49965038895606995, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 573, Validation Loss: 0.5110245943069458, Validation Accuracy: 0.98\n",
            "Epoch: 573, Testing Loss: 0.5355460047721863, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 574, Training Loss: 0.499645859003067, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 574, Validation Loss: 0.5110251903533936, Validation Accuracy: 0.98\n",
            "Epoch: 574, Testing Loss: 0.5355454683303833, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 575, Training Loss: 0.49964138865470886, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 575, Validation Loss: 0.5110257863998413, Validation Accuracy: 0.98\n",
            "Epoch: 575, Testing Loss: 0.5355427861213684, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 576, Training Loss: 0.4996368885040283, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 576, Validation Loss: 0.5110263824462891, Validation Accuracy: 0.98\n",
            "Epoch: 576, Testing Loss: 0.5355400443077087, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 577, Training Loss: 0.4996322989463806, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 577, Validation Loss: 0.511026918888092, Validation Accuracy: 0.98\n",
            "Epoch: 577, Testing Loss: 0.5355373620986938, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 578, Training Loss: 0.49962782859802246, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 578, Validation Loss: 0.5110273957252502, Validation Accuracy: 0.98\n",
            "Epoch: 578, Testing Loss: 0.5355347394943237, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 579, Training Loss: 0.4996233284473419, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 579, Validation Loss: 0.5110278725624084, Validation Accuracy: 0.98\n",
            "Epoch: 579, Testing Loss: 0.5355320572853088, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 580, Training Loss: 0.499618798494339, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 580, Validation Loss: 0.5110283493995667, Validation Accuracy: 0.98\n",
            "Epoch: 580, Testing Loss: 0.5355293154716492, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 581, Training Loss: 0.49961429834365845, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 581, Validation Loss: 0.5110287666320801, Validation Accuracy: 0.98\n",
            "Epoch: 581, Testing Loss: 0.5355266332626343, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 582, Training Loss: 0.4996098577976227, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 582, Validation Loss: 0.5110291242599487, Validation Accuracy: 0.98\n",
            "Epoch: 582, Testing Loss: 0.5355239510536194, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 583, Training Loss: 0.49960535764694214, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 583, Validation Loss: 0.5110294818878174, Validation Accuracy: 0.98\n",
            "Epoch: 583, Testing Loss: 0.5355212092399597, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 584, Training Loss: 0.49960094690322876, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 584, Validation Loss: 0.5110297799110413, Validation Accuracy: 0.98\n",
            "Epoch: 584, Testing Loss: 0.5355185866355896, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 585, Training Loss: 0.499596506357193, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 585, Validation Loss: 0.5110300183296204, Validation Accuracy: 0.98\n",
            "Epoch: 585, Testing Loss: 0.5355157852172852, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 586, Training Loss: 0.4995920658111572, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 586, Validation Loss: 0.5110302567481995, Validation Accuracy: 0.98\n",
            "Epoch: 586, Testing Loss: 0.5355131030082703, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 587, Training Loss: 0.4995875358581543, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 587, Validation Loss: 0.511030375957489, Validation Accuracy: 0.98\n",
            "Epoch: 587, Testing Loss: 0.5355103611946106, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 588, Training Loss: 0.4995831251144409, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 588, Validation Loss: 0.5110305547714233, Validation Accuracy: 0.98\n",
            "Epoch: 588, Testing Loss: 0.5355076193809509, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 589, Training Loss: 0.49957871437072754, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 589, Validation Loss: 0.5110306739807129, Validation Accuracy: 0.98\n",
            "Epoch: 589, Testing Loss: 0.5355048775672913, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 590, Training Loss: 0.49957433342933655, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 590, Validation Loss: 0.5110307335853577, Validation Accuracy: 0.98\n",
            "Epoch: 590, Testing Loss: 0.5355020761489868, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 591, Training Loss: 0.49956992268562317, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 591, Validation Loss: 0.5110307931900024, Validation Accuracy: 0.98\n",
            "Epoch: 591, Testing Loss: 0.5354993343353271, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 592, Training Loss: 0.4995655119419098, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 592, Validation Loss: 0.5110307335853577, Validation Accuracy: 0.98\n",
            "Epoch: 592, Testing Loss: 0.5354965329170227, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 593, Training Loss: 0.499561071395874, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 593, Validation Loss: 0.5110306739807129, Validation Accuracy: 0.98\n",
            "Epoch: 593, Testing Loss: 0.5354937314987183, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 594, Training Loss: 0.4995567202568054, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 594, Validation Loss: 0.5110306143760681, Validation Accuracy: 0.98\n",
            "Epoch: 594, Testing Loss: 0.5354909896850586, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 595, Training Loss: 0.49955230951309204, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 595, Validation Loss: 0.5110304951667786, Validation Accuracy: 0.98\n",
            "Epoch: 595, Testing Loss: 0.5354881286621094, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 596, Training Loss: 0.49954789876937866, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 596, Validation Loss: 0.511030375957489, Validation Accuracy: 0.98\n",
            "Epoch: 596, Testing Loss: 0.5354854464530945, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 597, Training Loss: 0.49954357743263245, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 597, Validation Loss: 0.5110301375389099, Validation Accuracy: 0.98\n",
            "Epoch: 597, Testing Loss: 0.5354825854301453, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 598, Training Loss: 0.49953919649124146, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 598, Validation Loss: 0.5110300183296204, Validation Accuracy: 0.98\n",
            "Epoch: 598, Testing Loss: 0.535479724407196, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 599, Training Loss: 0.4995347857475281, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 599, Validation Loss: 0.5110297203063965, Validation Accuracy: 0.98\n",
            "Epoch: 599, Testing Loss: 0.5354769825935364, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 600, Training Loss: 0.49953046441078186, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 600, Validation Loss: 0.5110294818878174, Validation Accuracy: 0.98\n",
            "Epoch: 600, Testing Loss: 0.5354741811752319, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 601, Training Loss: 0.4995260536670685, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 601, Validation Loss: 0.5110291838645935, Validation Accuracy: 0.98\n",
            "Epoch: 601, Testing Loss: 0.5354713201522827, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 602, Training Loss: 0.49952173233032227, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 602, Validation Loss: 0.5110288858413696, Validation Accuracy: 0.98\n",
            "Epoch: 602, Testing Loss: 0.5354684591293335, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 603, Training Loss: 0.49951744079589844, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 603, Validation Loss: 0.511028528213501, Validation Accuracy: 0.98\n",
            "Epoch: 603, Testing Loss: 0.5354657173156738, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 604, Training Loss: 0.4995131194591522, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 604, Validation Loss: 0.5110281705856323, Validation Accuracy: 0.98\n",
            "Epoch: 604, Testing Loss: 0.5354627966880798, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 605, Training Loss: 0.49950870871543884, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 605, Validation Loss: 0.5110278129577637, Validation Accuracy: 0.98\n",
            "Epoch: 605, Testing Loss: 0.5354600548744202, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 606, Training Loss: 0.4995043873786926, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 606, Validation Loss: 0.5110273957252502, Validation Accuracy: 0.98\n",
            "Epoch: 606, Testing Loss: 0.5354571342468262, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 607, Training Loss: 0.4995000660419464, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 607, Validation Loss: 0.5110269784927368, Validation Accuracy: 0.98\n",
            "Epoch: 607, Testing Loss: 0.5354543924331665, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 608, Training Loss: 0.49949580430984497, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 608, Validation Loss: 0.5110265016555786, Validation Accuracy: 0.98\n",
            "Epoch: 608, Testing Loss: 0.5354515314102173, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 609, Training Loss: 0.49949154257774353, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 609, Validation Loss: 0.5110260248184204, Validation Accuracy: 0.98\n",
            "Epoch: 609, Testing Loss: 0.5354487299919128, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 610, Training Loss: 0.4994872212409973, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 610, Validation Loss: 0.5110255479812622, Validation Accuracy: 0.98\n",
            "Epoch: 610, Testing Loss: 0.5354459285736084, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 611, Training Loss: 0.49948298931121826, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 611, Validation Loss: 0.511025071144104, Validation Accuracy: 0.98\n",
            "Epoch: 611, Testing Loss: 0.535443127155304, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 612, Training Loss: 0.49947866797447205, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 612, Validation Loss: 0.511024534702301, Validation Accuracy: 0.98\n",
            "Epoch: 612, Testing Loss: 0.5354402661323547, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 613, Training Loss: 0.499474436044693, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 613, Validation Loss: 0.511023998260498, Validation Accuracy: 0.98\n",
            "Epoch: 613, Testing Loss: 0.5354374647140503, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 614, Training Loss: 0.4994702637195587, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 614, Validation Loss: 0.5110234618186951, Validation Accuracy: 0.98\n",
            "Epoch: 614, Testing Loss: 0.5354347229003906, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 615, Training Loss: 0.4994659721851349, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 615, Validation Loss: 0.5110229253768921, Validation Accuracy: 0.98\n",
            "Epoch: 615, Testing Loss: 0.535431981086731, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 616, Training Loss: 0.49946171045303345, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 616, Validation Loss: 0.5110223889350891, Validation Accuracy: 0.98\n",
            "Epoch: 616, Testing Loss: 0.5354291200637817, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 617, Training Loss: 0.4994574785232544, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 617, Validation Loss: 0.5110217928886414, Validation Accuracy: 0.98\n",
            "Epoch: 617, Testing Loss: 0.5354263782501221, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 618, Training Loss: 0.4994533956050873, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 618, Validation Loss: 0.5110212564468384, Validation Accuracy: 0.98\n",
            "Epoch: 618, Testing Loss: 0.5354236364364624, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 619, Training Loss: 0.4994491636753082, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 619, Validation Loss: 0.5110206007957458, Validation Accuracy: 0.98\n",
            "Epoch: 619, Testing Loss: 0.535420835018158, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 620, Training Loss: 0.49944496154785156, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 620, Validation Loss: 0.5110200643539429, Validation Accuracy: 0.98\n",
            "Epoch: 620, Testing Loss: 0.5354181528091431, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 621, Training Loss: 0.4994407892227173, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 621, Validation Loss: 0.5110194683074951, Validation Accuracy: 0.98\n",
            "Epoch: 621, Testing Loss: 0.5354154109954834, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 622, Training Loss: 0.4994365870952606, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 622, Validation Loss: 0.5110188126564026, Validation Accuracy: 0.98\n",
            "Epoch: 622, Testing Loss: 0.5354127287864685, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 623, Training Loss: 0.4994325339794159, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 623, Validation Loss: 0.5110181570053101, Validation Accuracy: 0.98\n",
            "Epoch: 623, Testing Loss: 0.5354100465774536, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 624, Training Loss: 0.49942833185195923, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 624, Validation Loss: 0.5110175609588623, Validation Accuracy: 0.98\n",
            "Epoch: 624, Testing Loss: 0.5354073643684387, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 625, Training Loss: 0.4994242787361145, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 625, Validation Loss: 0.5110169649124146, Validation Accuracy: 0.98\n",
            "Epoch: 625, Testing Loss: 0.5354046821594238, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 626, Training Loss: 0.49942007660865784, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 626, Validation Loss: 0.511016309261322, Validation Accuracy: 0.98\n",
            "Epoch: 626, Testing Loss: 0.5354019403457642, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 627, Training Loss: 0.4994160234928131, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 627, Validation Loss: 0.5110156536102295, Validation Accuracy: 0.98\n",
            "Epoch: 627, Testing Loss: 0.535399317741394, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 628, Training Loss: 0.4994118809700012, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 628, Validation Loss: 0.5110150575637817, Validation Accuracy: 0.98\n",
            "Epoch: 628, Testing Loss: 0.5353966951370239, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 629, Training Loss: 0.4994078278541565, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 629, Validation Loss: 0.5110143423080444, Validation Accuracy: 0.98\n",
            "Epoch: 629, Testing Loss: 0.5353940725326538, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 630, Training Loss: 0.49940377473831177, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 630, Validation Loss: 0.5110137462615967, Validation Accuracy: 0.98\n",
            "Epoch: 630, Testing Loss: 0.5353915095329285, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 631, Training Loss: 0.49939966201782227, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 631, Validation Loss: 0.5110130906105042, Validation Accuracy: 0.98\n",
            "Epoch: 631, Testing Loss: 0.5353889465332031, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 632, Training Loss: 0.49939560890197754, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 632, Validation Loss: 0.5110124349594116, Validation Accuracy: 0.98\n",
            "Epoch: 632, Testing Loss: 0.5353862643241882, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 633, Training Loss: 0.4993916451931, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 633, Validation Loss: 0.5110117793083191, Validation Accuracy: 0.98\n",
            "Epoch: 633, Testing Loss: 0.5353837609291077, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 634, Training Loss: 0.49938756227493286, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 634, Validation Loss: 0.5110110640525818, Validation Accuracy: 0.98\n",
            "Epoch: 634, Testing Loss: 0.5353812575340271, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 635, Training Loss: 0.4993835985660553, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 635, Validation Loss: 0.5110104084014893, Validation Accuracy: 0.98\n",
            "Epoch: 635, Testing Loss: 0.535378634929657, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 636, Training Loss: 0.49937963485717773, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 636, Validation Loss: 0.5110097527503967, Validation Accuracy: 0.98\n",
            "Epoch: 636, Testing Loss: 0.5353761911392212, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 637, Training Loss: 0.4993756115436554, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 637, Validation Loss: 0.5110090970993042, Validation Accuracy: 0.98\n",
            "Epoch: 637, Testing Loss: 0.5353736877441406, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 638, Training Loss: 0.49937164783477783, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 638, Validation Loss: 0.5110084414482117, Validation Accuracy: 0.98\n",
            "Epoch: 638, Testing Loss: 0.5353711843490601, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 639, Training Loss: 0.49936768412590027, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 639, Validation Loss: 0.5110077857971191, Validation Accuracy: 0.98\n",
            "Epoch: 639, Testing Loss: 0.5353687405586243, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 640, Training Loss: 0.4993637204170227, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 640, Validation Loss: 0.5110071301460266, Validation Accuracy: 0.98\n",
            "Epoch: 640, Testing Loss: 0.5353662967681885, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 641, Training Loss: 0.49935978651046753, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 641, Validation Loss: 0.5110064148902893, Validation Accuracy: 0.98\n",
            "Epoch: 641, Testing Loss: 0.5353638529777527, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 642, Training Loss: 0.49935582280158997, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 642, Validation Loss: 0.5110057592391968, Validation Accuracy: 0.98\n",
            "Epoch: 642, Testing Loss: 0.5353614687919617, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 643, Training Loss: 0.4993518888950348, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 643, Validation Loss: 0.5110051035881042, Validation Accuracy: 0.98\n",
            "Epoch: 643, Testing Loss: 0.5353590250015259, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 644, Training Loss: 0.4993480443954468, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 644, Validation Loss: 0.5110044479370117, Validation Accuracy: 0.98\n",
            "Epoch: 644, Testing Loss: 0.5353566408157349, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 645, Training Loss: 0.499344140291214, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 645, Validation Loss: 0.5110037326812744, Validation Accuracy: 0.98\n",
            "Epoch: 645, Testing Loss: 0.5353543162345886, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 646, Training Loss: 0.4993402063846588, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 646, Validation Loss: 0.5110030770301819, Validation Accuracy: 0.98\n",
            "Epoch: 646, Testing Loss: 0.5353519320487976, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 647, Training Loss: 0.4993364214897156, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 647, Validation Loss: 0.5110023617744446, Validation Accuracy: 0.98\n",
            "Epoch: 647, Testing Loss: 0.5353495478630066, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 648, Training Loss: 0.499332457780838, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 648, Validation Loss: 0.5110017657279968, Validation Accuracy: 0.98\n",
            "Epoch: 648, Testing Loss: 0.5353472232818604, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 649, Training Loss: 0.4993286430835724, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 649, Validation Loss: 0.5110011100769043, Validation Accuracy: 0.98\n",
            "Epoch: 649, Testing Loss: 0.5353449583053589, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 650, Training Loss: 0.499324768781662, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 650, Validation Loss: 0.511000394821167, Validation Accuracy: 0.98\n",
            "Epoch: 650, Testing Loss: 0.5353426933288574, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 651, Training Loss: 0.49932095408439636, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 651, Validation Loss: 0.5109997391700745, Validation Accuracy: 0.98\n",
            "Epoch: 651, Testing Loss: 0.535340428352356, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 652, Training Loss: 0.4993171989917755, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 652, Validation Loss: 0.5109990239143372, Validation Accuracy: 0.98\n",
            "Epoch: 652, Testing Loss: 0.5353381633758545, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 653, Training Loss: 0.4993133544921875, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 653, Validation Loss: 0.5109983682632446, Validation Accuracy: 0.98\n",
            "Epoch: 653, Testing Loss: 0.535335898399353, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 654, Training Loss: 0.49930956959724426, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 654, Validation Loss: 0.5109977126121521, Validation Accuracy: 0.98\n",
            "Epoch: 654, Testing Loss: 0.5353336930274963, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 655, Training Loss: 0.49930575489997864, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 655, Validation Loss: 0.5109970569610596, Validation Accuracy: 0.98\n",
            "Epoch: 655, Testing Loss: 0.5353314876556396, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 656, Training Loss: 0.4993019700050354, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 656, Validation Loss: 0.5109963417053223, Validation Accuracy: 0.98\n",
            "Epoch: 656, Testing Loss: 0.535329282283783, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 657, Training Loss: 0.4992982745170593, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 657, Validation Loss: 0.5109956860542297, Validation Accuracy: 0.98\n",
            "Epoch: 657, Testing Loss: 0.5353270173072815, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 658, Training Loss: 0.4992945194244385, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 658, Validation Loss: 0.5109950304031372, Validation Accuracy: 0.98\n",
            "Epoch: 658, Testing Loss: 0.5353248715400696, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 659, Training Loss: 0.49929073452949524, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 659, Validation Loss: 0.5109943747520447, Validation Accuracy: 0.98\n",
            "Epoch: 659, Testing Loss: 0.5353227257728577, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 660, Training Loss: 0.4992870092391968, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 660, Validation Loss: 0.5109937191009521, Validation Accuracy: 0.98\n",
            "Epoch: 660, Testing Loss: 0.5353206396102905, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 661, Training Loss: 0.49928322434425354, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 661, Validation Loss: 0.5109930038452148, Validation Accuracy: 0.98\n",
            "Epoch: 661, Testing Loss: 0.5353184342384338, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 662, Training Loss: 0.49927961826324463, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 662, Validation Loss: 0.5109922885894775, Validation Accuracy: 0.98\n",
            "Epoch: 662, Testing Loss: 0.5353163480758667, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 663, Training Loss: 0.4992758631706238, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 663, Validation Loss: 0.5109899044036865, Validation Accuracy: 0.98\n",
            "Epoch: 663, Testing Loss: 0.5353143215179443, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 664, Training Loss: 0.4992721676826477, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 664, Validation Loss: 0.5109909772872925, Validation Accuracy: 0.98\n",
            "Epoch: 664, Testing Loss: 0.5353121757507324, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 665, Training Loss: 0.499268501996994, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 665, Validation Loss: 0.5109903216362, Validation Accuracy: 0.98\n",
            "Epoch: 665, Testing Loss: 0.5353100895881653, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 666, Training Loss: 0.49926477670669556, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 666, Validation Loss: 0.5109896659851074, Validation Accuracy: 0.98\n",
            "Epoch: 666, Testing Loss: 0.5353081226348877, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 667, Training Loss: 0.49926117062568665, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 667, Validation Loss: 0.5109890103340149, Validation Accuracy: 0.98\n",
            "Epoch: 667, Testing Loss: 0.5353059768676758, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 668, Training Loss: 0.49925750494003296, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 668, Validation Loss: 0.5109882950782776, Validation Accuracy: 0.98\n",
            "Epoch: 668, Testing Loss: 0.5353040099143982, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 669, Training Loss: 0.4992538392543793, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 669, Validation Loss: 0.5109876394271851, Validation Accuracy: 0.98\n",
            "Epoch: 669, Testing Loss: 0.5353019833564758, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 670, Training Loss: 0.4992501735687256, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 670, Validation Loss: 0.5109869837760925, Validation Accuracy: 0.98\n",
            "Epoch: 670, Testing Loss: 0.5353000164031982, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 671, Training Loss: 0.4992465674877167, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 671, Validation Loss: 0.510986328125, Validation Accuracy: 0.98\n",
            "Epoch: 671, Testing Loss: 0.5352979898452759, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 672, Training Loss: 0.4992429316043854, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 672, Validation Loss: 0.5109856724739075, Validation Accuracy: 0.98\n",
            "Epoch: 672, Testing Loss: 0.5352960228919983, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 673, Training Loss: 0.49923932552337646, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 673, Validation Loss: 0.5109850168228149, Validation Accuracy: 0.98\n",
            "Epoch: 673, Testing Loss: 0.5352939963340759, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 674, Training Loss: 0.49923574924468994, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 674, Validation Loss: 0.5109843015670776, Validation Accuracy: 0.98\n",
            "Epoch: 674, Testing Loss: 0.5352920889854431, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 675, Training Loss: 0.4992321729660034, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 675, Validation Loss: 0.5109836459159851, Validation Accuracy: 0.98\n",
            "Epoch: 675, Testing Loss: 0.5352902412414551, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 676, Training Loss: 0.4992285966873169, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 676, Validation Loss: 0.5109829902648926, Validation Accuracy: 0.98\n",
            "Epoch: 676, Testing Loss: 0.5352882146835327, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 677, Training Loss: 0.499224990606308, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 677, Validation Loss: 0.5109823346138, Validation Accuracy: 0.98\n",
            "Epoch: 677, Testing Loss: 0.5352863073348999, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 678, Training Loss: 0.49922141432762146, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 678, Validation Loss: 0.5109816193580627, Validation Accuracy: 0.98\n",
            "Epoch: 678, Testing Loss: 0.5352843999862671, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 679, Training Loss: 0.4992178678512573, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 679, Validation Loss: 0.5109809637069702, Validation Accuracy: 0.98\n",
            "Epoch: 679, Testing Loss: 0.535282552242279, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 680, Training Loss: 0.4992142915725708, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 680, Validation Loss: 0.5109803080558777, Validation Accuracy: 0.98\n",
            "Epoch: 680, Testing Loss: 0.5352806448936462, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 681, Training Loss: 0.49921077489852905, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 681, Validation Loss: 0.5109796524047852, Validation Accuracy: 0.98\n",
            "Epoch: 681, Testing Loss: 0.5352787971496582, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 682, Training Loss: 0.4992072284221649, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 682, Validation Loss: 0.5109789371490479, Validation Accuracy: 0.98\n",
            "Epoch: 682, Testing Loss: 0.5352769494056702, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 683, Training Loss: 0.49920371174812317, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 683, Validation Loss: 0.5109782814979553, Validation Accuracy: 0.98\n",
            "Epoch: 683, Testing Loss: 0.5352751016616821, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 684, Training Loss: 0.49920013546943665, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 684, Validation Loss: 0.510977566242218, Validation Accuracy: 0.98\n",
            "Epoch: 684, Testing Loss: 0.5352732539176941, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 685, Training Loss: 0.4991966485977173, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 685, Validation Loss: 0.5109769701957703, Validation Accuracy: 0.98\n",
            "Epoch: 685, Testing Loss: 0.535271406173706, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 686, Training Loss: 0.49919310212135315, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 686, Validation Loss: 0.5109761953353882, Validation Accuracy: 0.98\n",
            "Epoch: 686, Testing Loss: 0.5352696180343628, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 687, Training Loss: 0.49918967485427856, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 687, Validation Loss: 0.5109755992889404, Validation Accuracy: 0.98\n",
            "Epoch: 687, Testing Loss: 0.5352677702903748, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 688, Training Loss: 0.4991861879825592, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 688, Validation Loss: 0.5109749436378479, Validation Accuracy: 0.98\n",
            "Epoch: 688, Testing Loss: 0.5352659821510315, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 689, Training Loss: 0.49918270111083984, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 689, Validation Loss: 0.5109742283821106, Validation Accuracy: 0.98\n",
            "Epoch: 689, Testing Loss: 0.535264253616333, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 690, Training Loss: 0.4991792142391205, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 690, Validation Loss: 0.5109735727310181, Validation Accuracy: 0.98\n",
            "Epoch: 690, Testing Loss: 0.5352624654769897, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 691, Training Loss: 0.4991757571697235, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 691, Validation Loss: 0.5109729170799255, Validation Accuracy: 0.98\n",
            "Epoch: 691, Testing Loss: 0.5352607369422913, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 692, Training Loss: 0.49917227029800415, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 692, Validation Loss: 0.5109722018241882, Validation Accuracy: 0.98\n",
            "Epoch: 692, Testing Loss: 0.5352590680122375, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 693, Training Loss: 0.49916884303092957, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 693, Validation Loss: 0.5109715461730957, Validation Accuracy: 0.98\n",
            "Epoch: 693, Testing Loss: 0.5352572798728943, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 694, Training Loss: 0.4991653859615326, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 694, Validation Loss: 0.5109708905220032, Validation Accuracy: 0.98\n",
            "Epoch: 694, Testing Loss: 0.5352545380592346, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 695, Training Loss: 0.4991619884967804, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 695, Validation Loss: 0.5109701752662659, Validation Accuracy: 0.98\n",
            "Epoch: 695, Testing Loss: 0.5352538228034973, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 696, Training Loss: 0.4991585314273834, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 696, Validation Loss: 0.5109695196151733, Validation Accuracy: 0.98\n",
            "Epoch: 696, Testing Loss: 0.5352520942687988, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 697, Training Loss: 0.49915510416030884, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 697, Validation Loss: 0.510968804359436, Validation Accuracy: 0.98\n",
            "Epoch: 697, Testing Loss: 0.5352504849433899, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 698, Training Loss: 0.49915164709091187, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 698, Validation Loss: 0.5109681487083435, Validation Accuracy: 0.98\n",
            "Epoch: 698, Testing Loss: 0.5352487564086914, Testing Accuracy: 0.9793103448275862\n",
            "Epoch: 699, Training Loss: 0.49914830923080444, Training Accuracy: 0.9793103448275862\n",
            "Epoch: 699, Validation Loss: 0.5109674334526062, Validation Accuracy: 0.98\n",
            "Epoch: 699, Testing Loss: 0.5352471470832825, Testing Accuracy: 0.9793103448275862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq4c8uLQsikT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotting_loss(epoch, training_error, validation_error, testing_error, title):\n",
        "    epoch_idx = np.arange(0, epoch)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.plot(epoch_idx,training_error)\n",
        "    plt.plot(epoch_idx,validation_error)\n",
        "    plt.plot(epoch_idx,testing_error)\n",
        "    plt.xlabel('Number of Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['Training Loss', 'Validation Loss', 'Testing Loss'])\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTGhzykVs1Jn",
        "colab_type": "code",
        "outputId": "d9ec2a1a-d355-4508-eb9b-c627e302426c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "plotting_loss(epochs, training_error[0], validation_error[0], testing_error[0],\"CE Loss of BGD w/ Reg of 0.5 and lr of 0.001\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAJcCAYAAAAo6aqNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWdGZfQroAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-TH6xDls7Ke",
        "colab_type": "code",
        "outputId": "d364c654-adee-440b-837f-7536e1cfd811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "plotting_loss(epochs, ce_training_loss, ce_validation_loss, ce_testing_loss,\"CE Loss of SGD w/ lr of 0.001 and batch of 700\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAJcCAYAAABAGii1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIBTcRERERBqEgpuIiIhIg/gfG+7/AcmQ+4cAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87O0XShvuAZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotting_accuracy_compare(epoch, training_accuracy, validation_accuracy, testing_accuracy, title):\n",
        "    epoch_idx = np.arange(0, epoch)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.plot(epoch_idx,training_accuracy)\n",
        "    plt.plot(epoch_idx,validation_accuracy)\n",
        "    plt.plot(epoch_idx,testing_accuracy)\n",
        "    plt.plot(epoch_idx,ce_training_accuracy)\n",
        "    plt.plot(epoch_idx,ce_validation_accuracy)\n",
        "    plt.plot(epoch_idx,ce_testing_accuracy)\n",
        "    plt.xlabel('Number of Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Training Accuracy BGD', 'Validation Accuracy BGD', 'Testing Accuracy BGD','Training Accuracy SGD', 'Validation Accuracy SGD', 'Testing Accuracy SGD'])\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qss4tINTuGvY",
        "colab_type": "code",
        "outputId": "0876d580-f3bd-472f-b49e-d24255ccb724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "plotting_accuracy(epochs, training_accuracy[0], validation_accuracy[0], testing_accuracy[0], \"Accuracy Comparison between SGD and BGD\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAJcCAYAAACxEXM4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAxKzcaDuOR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
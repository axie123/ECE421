# -*- coding: utf-8 -*-
"""ECE421A2P1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14UfRMzTtDy6Kpdx_X3sEYTWT56TOLWFz
"""

import numpy as np
import matplotlib.pyplot as plt

def loadData():
    with np.load("notMNIST.npz") as data:
        Data, Target = data["images"], data["labels"]
        np.random.seed(521)
        randIndx = np.arange(len(Data))
        np.random.shuffle(randIndx)
        Data = Data[randIndx] / 255.0
        Target = Target[randIndx]
        trainData, trainTarget = Data[:10000], Target[:10000]
        validData, validTarget = Data[10000:16000], Target[10000:16000]
        testData, testTarget = Data[16000:], Target[16000:]
    return trainData, validData, testData, trainTarget, validTarget, testTarget

def convertOneHot(trainTarget, validTarget, testTarget):
    newtrain = np.zeros((trainTarget.shape[0], 10))
    newvalid = np.zeros((validTarget.shape[0], 10))
    newtest = np.zeros((testTarget.shape[0], 10))
    for item in range(0, trainTarget.shape[0]):
        newtrain[item][trainTarget[item]] = 1
    for item in range(0, validTarget.shape[0]):
        newvalid[item][validTarget[item]] = 1
    for item in range(0, testTarget.shape[0]):
        newtest[item][testTarget[item]] = 1
    return newtrain, newvalid, newtest

def shuffle(trainData, trainTarget):
    np.random.seed(421)
    randIndx = np.arange(len(trainData))
    target = trainTarget
    np.random.shuffle(randIndx)
    data, target = trainData[randIndx], target[randIndx]
    return data, target

def ReLU_FN(n):
    return n if n > 0 else 0

def ReLU(x):
    f = np.vectorize(ReLU_FN)
    return f(x)

def softmax(x):
    return np.exp(x)/np.exp(x).sum(axis=1, keepdims=True)

def compute(W,x,b):
    return np.add(np.dot(np.transpose(x),W),b)

def averageCE(target,prediction):
    lP = np.log(prediction+1e-20)
    return -1*np.mean(target*lP)

def gradCE(y,p):
    return p - y

def dL_dWo(y,p,h):
    return np.dot(np.transpose(h),gradCE(y,p))

def dL_dBo(y,p):
    return np.dot(np.ones((1,len(y))),gradCE(y,p))

def dL_dWh(y,p,x,h,w_o):
    h[h > 0] = 1
    h[h < 0] = 0
    return np.dot(np.transpose(x),np.multiply(h,np.dot(gradCE(y,p),np.transpose(w_o))))

def dL_dBh(y,p,h,w_o):
    h[h > 0] = 1
    h[h < 0] = 0
    return np.dot(np.ones((1,len(y))),np.multiply(h,np.dot(gradCE(y,p),np.transpose(w_o))))

def learning(trainData, y, w_o, v_w_o, w_h, v_w_h, epochs, \
        gamma, lr, b_o, b_h, validData, newvalid, testData, newtest):

    ######INITIALIZATION######

    #Initialize accuracy accumulators
    acc_train = []
    acc_valid = []
    acc_test = []
    
    #Initialize loss accumulators
    loss_train = []
    loss_valid = []
    loss_test = []

    #Initialize bias velocities
    v_b_o = b_o
    #v_b_o = np.full(b_o.shape[0],0.00001)
    v_b_h = b_h
    #v_b_h = np.full(b_h.shape[0],0.00001)

    #####TRAINING#####

    for epoch in range(epochs):

        ###FORWARD PROPAGATION###

        #Training
        h_in_train = np.add(np.dot(trainData, w_h), b_h)
        h_train = ReLU(h_in_train)
        p_train = softmax(np.add(np.dot(h_train, w_o), b_o))
        loss_train.append(averageCE(y, p_train))
        pm_train = np.argmax(p_train, axis = 1)
        ym_train = np.argmax(y, axis = 1)
        eq_train = np.equal(pm_train, ym_train)
        acc_train.append(np.sum((eq_train==True))/(trainData.shape[0]))

        #Validation
        h_valid = ReLU(np.add(np.dot(validData, w_h), b_h))
        p_valid = softmax(np.add(np.dot(h_valid, w_o), b_o))
        loss_valid.append(averageCE(newvalid, p_valid))
        pm_valid = np.argmax(p_valid, axis = 1)
        ym_valid = np.argmax(newvalid, axis = 1)
        eq_valid = np.equal(pm_valid, ym_valid)
        acc_valid.append(np.sum((eq_valid==True))/(validData.shape[0]))

        #Testing
        h_test = ReLU(np.add(np.dot(testData, w_h), b_h))
        p_test = softmax(np.add(np.dot(h_test, w_o), b_o))
        loss_test.append(averageCE(newtest, p_test))
        pm_test = np.argmax(p_test, axis = 1)
        ym_test = np.argmax(newtest, axis = 1)
        eq_test = np.equal(pm_test, ym_test)
        acc_test.append(np.sum((eq_test==True))/(testData.shape[0]))

        print("Epoch:", epoch)
        print("Training Accuracy:", acc_train[epoch])
        print("Training Loss:", loss_train[epoch])
        ###BACK PROPAGATION###

        #Calculate Gradients
        d_w_o = dL_dWo(y, p_train, h_train)
        d_b_o = dL_dBo(y, p_train)
        d_w_h = dL_dWh(y, p_train, trainData, h_in_train, w_o)
        d_b_h = dL_dBh(y, p_train, h_in_train, w_o)

        #Update output layer
        v_w_o = gamma*v_w_o + lr*d_w_o
        w_o = w_o - v_w_o
        v_b_o = gamma*v_b_o + lr*d_b_o
        b_o = b_o - v_b_o

        #Update hidden layer
        v_w_h = gamma*v_w_h + lr*d_w_h
        w_h = w_h - v_w_h
        v_b_h = gamma*v_b_h + lr*d_b_h
        b_h = b_h - v_b_h

    #####TRAINING COMPLETE#####

    return w_o, b_o, w_h, b_h, acc_train, acc_valid, acc_test, loss_train, loss_valid, loss_test

def plot_curves(epochs,acc_train,loss_train,acc_valid,loss_valid,acc_test,loss_test):
    epoch_idx = np.arange(0, epochs)
    plt.plot(epoch_idx, loss_train)
    plt.plot(epoch_idx, loss_valid)
    plt.plot(epoch_idx, loss_test)
    plt.xlabel('Number of Epochs')
    plt.ylabel('Loss')
    plt.legend(['Training Loss', 'Validation Loss', 'Testing Loss'])
    plt.title("Training, Validation, and Testing Loss")
    plt.show()

    epoch_idx = np.arange(0, epochs)
    plt.plot(epoch_idx, acc_train)
    plt.plot(epoch_idx, acc_valid)
    plt.plot(epoch_idx, acc_test)
    plt.xlabel('Number of Epochs')
    plt.ylabel('Accuracy')
    plt.legend(['Training Accuracy', 'Validation Accuracy', 'Testing Accuracy'])
    plt.title("Training, Validation, and Testing Accuracy")
    plt.show()

trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()
trainData = trainData.reshape((trainData.shape[0], -1))
validData = validData.reshape((validData.shape[0], -1))
testData = testData.reshape((testData.shape[0], -1))

hidden_units = 2000
epochs = 200
gamma = 0.99
epsilon = 1e-5


newtrain, newvalid, newtest = convertOneHot(trainTarget, validTarget, testTarget)

#Initializing weights (Xavier Initialization) and their velocities
w_o = np.random.normal(0, np.sqrt(1.0/(hidden_units+10)), (hidden_units,10))
v_w_o = np.full((hidden_units, 10), 1e-5,dtype=np.float64)

w_h = np.random.normal(0, np.sqrt(1.0/(trainData.shape[0]+hidden_units)), (trainData.shape[1],hidden_units))
v_w_h = np.full((trainData.shape[1],hidden_units), 1e-5,dtype=np.float64)

b_o = np.zeros((1, 10),dtype=np.float64)
b_h = np.zeros((1, hidden_units),dtype=np.float64)

weight_o, bias_o, weight_h, bias_h, acc_train, acc_valid, acc_test, loss_train, \
        loss_valid, loss_test = learning(trainData, newtrain, w_o, v_w_o, w_h, v_w_h, epochs, \
        gamma, epsilon, b_o, b_h, validData, newvalid, testData, newtest)
plot_curves(epochs,acc_train,loss_train,acc_valid,loss_valid,acc_test,loss_test)

